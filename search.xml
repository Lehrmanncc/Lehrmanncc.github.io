<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Love guarantee</title>
    <url>/2020/11/10/Love/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="81d3703d014df931126c662f658ff20c78b1043611a1f6633d8496bcb33906a5">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <tags>
        <tag>爱情故事</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenCV 基础操作</title>
    <url>/2020/10/14/opencv/</url>
    <content><![CDATA[<h2 id="图像上的算术运算"><a href="#图像上的算术运算" class="headerlink" title="图像上的算术运算"></a>图像上的算术运算</h2><h3 id="图像融合"><a href="#图像融合" class="headerlink" title="图像融合"></a>图像融合</h3><ul>
<li><p><strong>图像融合即也是图像加法，但它是对每个图像乘以相应的权重，使其具有融合或透明的感觉。根据以下等式进行运算。</strong></p>
<script type="math/tex; mode=display">
G(x)=(1-\alpha)f_1(x)+\alpha f_2(x),\quad \alpha\in[0,1]</script></li>
<li><p><strong>f(x)可作为输入的图像，则上式可变为：</strong></p>
<script type="math/tex; mode=display">
dst=\beta\cdot img_1+\alpha\cdot img_2+\gamma\\
\beta+\alpha=1</script></li>
<li><p><strong>在OpenCV中进行图像融合的函数为cv.addWeighted(img1,$\beta$,img2,$\alpha,\gamma$),其有四个参数,img1与img2分别为输入的图像，$\beta,\alpha$分别为各自图像融合时所占的权重，一般情况下$\gamma$为0。</strong></p>
<p>以下为测试代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img1=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\save.png&#x27;</span>)</span><br><span class="line">img2=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\imori.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dst1=cv.addWeighted(img1,<span class="number">0.8</span>,img2,<span class="number">0.2</span>,<span class="number">0</span>)</span><br><span class="line">dst2=cv.addWeighted(img1,<span class="number">0.2</span>,img2,<span class="number">0.8</span>,<span class="number">0</span>)</span><br><span class="line">images=[dst1,dst2]</span><br><span class="line">titles=[<span class="string">&#x27;dst1&#x27;</span>,<span class="string">&#x27;dst2&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    images[i]=cv.cvtColor(images[i],cv.COLOR_BGR2RGB)</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(images[i],<span class="string">&#x27;jet&#x27;</span>)</span><br><span class="line">    plt.title(titles[i],fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>代码解读：</p>
</li>
<li><p>在这里要注意到，使用Matplotlib显示图片时，要先将图片转换为RGB形式，在OpenCV中，图像是以BGR通道存储的，直接显示会出现问题。</p>
</li>
<li><p>上述测试代码里的img1为img2的灰度图，可以发现当$\beta$取0.8，$\alpha$取0.2时，img1融合时占比较高，当$\beta$取0.2，$\alpha$取0.8时，二者的对比结果如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116131929.png" alt="4" style="zoom:33%;"></p>
<p>可以发现前者更加偏灰度，后者更加偏向原图。</p>
</li>
</ul>
<h3 id="按位运算"><a href="#按位运算" class="headerlink" title="按位运算"></a>按位运算</h3><ul>
<li><p>有时候，我们不想将图像相加或者融合，例如，下面这张图片：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132446.jpg" alt="8" style="zoom:50%;"></p>
<p>它就是由两张图片——（西电LOGO和人物图）进行“掩盖”合成的。</p>
</li>
<li><p><strong>按位操作分为以下四种：AND，OR，NOT，XOR，一一介绍它们的运算：</strong></p>
<p><strong>1. AND：即“与”运算，（1&amp;1=1，1&amp;0=0，0&amp;1=0，0&amp;0=0），其具体意义可理解为，只有两值都大于0，其为真。而在像素中，其取值在0~255之间，0为黑色，255为白色，而这里大于0的像素值都可以看作1。在两个都大于0的像素值之间进行AND运算，其结果取较小的像素值！</strong></p>
<p><strong>2. NOT：即“非”运算，（~1=0，~0=1），其具体意义可理解为，取像素的相反值，则当图像为二值图时，原图的黑色变为白色，白色变为黑色。</strong></p>
<p><strong>3. OR：即“与”运算，（1|1=1，1|0=1，0|1=1，0|0=0），其具体意义可理解为，只有当两值都为0，其为假。而对于两像素值，其先进行二进制转换，后对其进行运算，例如3|5：00000011|00000101=00000111，因此3|5=7。</strong></p>
<p><strong>4. XOR：即“异或”运算，（1^1=0，1^0=1，0^1=1，0^0=0），其具体意义可理解为，只有当两值不相同时，其为真。对于两两像素值，先进行二进制转换，后对其进行运算，例如3^5：00000011^00000101=00000110，因此3^5=6。</strong></p>
</li>
<li><p><strong>OpenCV中，进行按位运算的有以下四个函数：cv.bitwise_not，cv.bitwise_and，cv.bitwise_or，cv.bitwise_xor，这四个函数即对应上面的运算规则。下面将以下两张图片合成为上图：</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132002.jpg" alt="6" style="zoom: 33%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116130801.jpg" alt="7" style="zoom:25%;"></p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img1=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\xidian1.jpg&#x27;</span>)</span><br><span class="line">img2=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\girl.jpg&#x27;</span>)</span><br><span class="line">row,cols,channels=img1.shape</span><br><span class="line">roi=img2[<span class="number">0</span>:row,<span class="number">0</span>:cols]</span><br><span class="line"></span><br><span class="line">img1_gray=cv.cvtColor(img1,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret,dst1=cv.threshold(img1_gray,<span class="number">200</span>,<span class="number">255</span>,cv.THRESH_BINARY_INV)</span><br><span class="line">dst1_inv=cv.bitwise_not(dst1)</span><br><span class="line"></span><br><span class="line">img2_bg=cv.bitwise_and(roi,roi,mask=dst1_inv)</span><br><span class="line">img1_bg=cv.bitwise_and(img1,img1,mask=dst1)</span><br><span class="line"></span><br><span class="line">dst=cv.add(img1_bg,img2_bg)</span><br><span class="line">img2[<span class="number">0</span>:row,<span class="number">0</span>:cols]=dst</span><br><span class="line"></span><br><span class="line">cv.imshow(<span class="string">&#x27;images&#x27;</span>,img2)</span><br><span class="line"></span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>代码解读:</p>
<ul>
<li><p><strong>首先由shape得到img1（LOGO）图像的大小，即行数和列数。由此，可以在img2图像上，通过numpy直接划分出与img1图像一样大小的 roi图像（为原图的左上角）。</strong></p>
</li>
<li><p><strong>通过threshold函数可将img1图像二值化，而在这之前需要将img1图像转换为灰度图，才能将其传入。使用 THRESH_BINARY_INV方法将其二值化，二值化图像如下：</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132043.png" alt="8" style="zoom: 33%;"></p>
<p><strong>将得到的二值化图像（dst1）利用“非”运算，得到的图像如下：</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132101.png" alt="9" style="zoom:33%;"></p>
<p><strong>可以发现，进行“非”运算后，对二值图来说，其实即进行了颜色反转。</strong></p>
</li>
</ul>
<hr>
</li>
</ul>
<h2 id="阈值分割"><a href="#阈值分割" class="headerlink" title="阈值分割"></a>阈值分割</h2><h3 id="固定阈值分割"><a href="#固定阈值分割" class="headerlink" title="固定阈值分割"></a>固定阈值分割</h3><ul>
<li><p><strong>阈值分割简单来说，即大于阈值的变成一种值，小于阈值的为另一种值</strong>   </p>
</li>
<li><p>在python的cv2库中实现固定阈值分割的为<strong>cv2.threshold()函数</strong>。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\imori.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">ret, th1 = cv.threshold(img, <span class="number">127</span>, <span class="number">255</span>, cv.THRESH_BINARY)</span><br><span class="line">ret,th2=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY_INV)</span><br><span class="line">ret,th3=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO)</span><br><span class="line">ret,th4=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO_INV)</span><br><span class="line">ret,th5=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TRUNC)</span><br><span class="line"></span><br><span class="line">titles=[<span class="string">&#x27;Org&#x27;</span>,<span class="string">&#x27;Binary&#x27;</span>,<span class="string">&#x27;Binary_inv&#x27;</span>,<span class="string">&#x27;Tozero&#x27;</span>,<span class="string">&#x27;Tozero_inv&#x27;</span>,<span class="string">&#x27;Trunc&#x27;</span>]</span><br><span class="line">images=[img,th1,th2,th3,th4,th5]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(images[i],<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.title(titles[i],fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  运行结果如图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132138.png" alt="1" style="zoom: 50%;"></p>
<ul>
<li><p><strong>cv2.threshold()函数由4个参数组成，img为传入函数的图像，127为设置的阈值大小(threshold)，255为阈值设定方式里的阈值最大值(maxval)，THRESH_BINARY为阈值设定的方式。</strong>      </p>
</li>
<li><p><strong>ret代表当前的阈值。</strong></p>
</li>
<li><p><strong>matplotlib.pyplot中subplot(2,3,i+1)即为将窗口分为2行3列，i+1表示当前的第i+1个子图，imshow()则是对图像进行处理，它不会让图片进行显示，图像显示需要show()</strong></p>
</li>
<li><p><strong>阈值设定有5种方法，分别为:</strong></p>
<p><strong>1.THRESH_BINARY:</strong></p>
</li>
</ul>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
maxval &\ if\ src(x,y)>thresh\\
0 &\ otherwise
\end{cases}</script><p>当原像素值大于阈值，原像素值变为maxval，除此之外为0。</p>
<p>​        <strong>2.THRESH_BINARY_INV:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
0 &\ if \ src(x,y)>thresh\\
maxval &\ otherwise
\end{cases}</script><p>该阈值方法与上述阈值方法相反，从图像也可以观察得到。</p>
<p>​        <strong>3.THRESH_TOZERO:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
src(x,y) &\ if\ src(x,y)>thresh\\
0 &\ oherwise
\end{cases}</script><p>​        当原像素值大于阈值时，原像素值保持不变，除此之外，原像素值变为0。</p>
<p>​        <strong>4.THRESH_TOZERO_INV:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
0 &\ if \ src(x,y)>thresh\\
src(x,y) &\ otherwise
\end{cases}</script><p>​        该阈值方法与上述阈值方法相反。</p>
<p>​        <strong>5.THRESH_TRUNC:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
threshold &\ if \ src(x,y)>thresh\\
src(x,y) &\ otherwise
\end{cases}</script><p>​        放原像素值大于阈值，则原像素值变为阈值，除此之外，原像素值保持不变。</p>
]]></content>
      <tags>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine learning</title>
    <url>/2020/10/23/ML/</url>
    <content><![CDATA[<h1 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a><strong>Regression</strong></h1><h2 id="Linear-regression-线性回归"><a href="#Linear-regression-线性回归" class="headerlink" title="Linear regression(线性回归)"></a>Linear regression(线性回归)</h2><h3 id="问题的导入：预测神奇宝贝进化后的战斗力（CP）值"><a href="#问题的导入：预测神奇宝贝进化后的战斗力（CP）值" class="headerlink" title="问题的导入：预测神奇宝贝进化后的战斗力（CP）值"></a>问题的导入：预测神奇宝贝进化后的战斗力（CP）值</h3><ul>
<li><p><strong>输入：进化前神奇宝贝A的CP值，种类，血量（HP），重量（Weight)，高度（Height）</strong></p>
</li>
<li><p><strong>输出：进化后神奇宝贝A的CP值</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233907.png" alt="1" style="zoom:67%;"></p>
</li>
</ul>
<h3 id="机器学习的主要步骤："><a href="#机器学习的主要步骤：" class="headerlink" title="机器学习的主要步骤："></a>机器学习的主要步骤：</h3><p><strong>step 1：Model（确定模型）</strong></p>
<p><strong>step 2：Goodless of function（确定损失函数）</strong></p>
<p><strong>step 3：Best function（找到最好的函数）</strong></p>
<h3 id="S-Model"><a href="#S-Model" class="headerlink" title="$\S $Model"></a>$\S $Model</h3><p>在这里，根据常理推测，神奇宝贝进化后的CP值和进化前的CP值有较大的关系，故我们可以假设进化前的CP值与进化后的CP值呈线性关系。</p>
<p>设神奇宝贝进化前的CP值为$x_{cp}$，进化后的CP值为$y$，则可建立以下线性关系：</p>
<script type="math/tex; mode=display">
y=b+wx_{cp} \tag 1</script><p>在这里我们仅考虑了神奇宝贝进化前的CP值这一特征，如果神奇宝贝影响进化后的CP值的不止这一特征，若有n个特征，那么我们可将$x_{cp}$推广到$x_i$，于是便可得到<strong>Linear model</strong>（线性模型）：</p>
<script type="math/tex; mode=display">
\begin{cases}
y=b+\sum_i^nw_ix_i\\
x_i:feature\ (特征)\\
w_i:weight \ (权重)\\
b:bias \ (偏差) \\
\end{cases} \tag 2</script><p>在这个模型里，一般而言，我们会有<strong>training data</strong>，假设现在我们有10组训练数据，即：</p>
<script type="math/tex; mode=display">
(x^1,\widehat{y}^1)\\
(x^2,\widehat{y}^2)\\
\vdots\\
(x^{10},\widehat{y}^{10}) \tag 3</script><p>对于以上数据来说，它们是已知的，其上标代表了它们的序号，而现在，我们可以将以上数据带入简化模型（式1）中，可得：</p>
<script type="math/tex; mode=display">
\widehat{y}^1=b+wx_{cp}^1\\
\widehat{y}^2=b+wx_{cp}^2\\
\vdots\\
\widehat{y}^{10}=b+wx_{cp}^{10}\\  \tag 4</script><p>对于式4，即就是我们训练数据带入模型后所得，而其只有两个未知数，即$b,w$，而现在我们需要做的就是<strong>找到最适合的$b,w$参数值</strong>。现在就需要损失函数发挥作用了。</p>
<h3 id="S-Goodless-of-function"><a href="#S-Goodless-of-function" class="headerlink" title="$\S $Goodless of  function"></a>$\S $Goodless of  function</h3><ul>
<li><p><strong>损失函数的作用</strong></p>
<script type="math/tex; mode=display">
Loss\ function\ L:
\begin{cases}
input:&\ a\ function\\
output:&\ how\ bad\ it\ is
\end{cases}</script><p>这里要注意评价函数的输入，即为函数$f$，而其输出是它的好坏，但由上式可知，式（4）其<strong>未知量只有待求参数$b,w$，</strong>那么函数评价函数$L(f)$即为：</p>
<script type="math/tex; mode=display">
L(f)=L(w,b) \tag 5</script><p>那么现在，<strong>对于输出所评价函数的好坏即可转化为评价参数$w,b$的好坏。</strong></p>
</li>
<li><p><strong>常见的损失函数</strong></p>
<p>现在我们已有真实数据，那么如何反映其参数的好坏呢，我们可以<strong>通过真实的CP值与预测CP值的差进行衡量</strong>，如下所示：</p>
<script type="math/tex; mode=display">
L(f)=\sum_{i=1}^{10}(\widehat{y}^i-f(x_{cp}^i))^2\Longrightarrow\ L(w,b)=\sum_{i=1}^{10}(\widehat{y}^i-(b+w\cdot x_{cp}^i))^2 \tag 6</script><p>当真实值与误差值较小时，那么此时的$w,b$可认为是最好的，则我们的目标即为：</p>
<script type="math/tex; mode=display">
min\quad L(w,b)</script><p>于是我们现在就需要对其进行求解。</p>
</li>
</ul>
<h3 id="S-Best-function-——-Gradient-Descent（梯度下降法）"><a href="#S-Best-function-——-Gradient-Descent（梯度下降法）" class="headerlink" title="$\S $Best  function ——-Gradient  Descent（梯度下降法）"></a>$\S $Best  function ——-Gradient  Descent（梯度下降法）</h3><p>我们需要通过$Loss\ Function\Longrightarrow”the \ best\ function”$，即达到我们的目标，令：</p>
<script type="math/tex; mode=display">
w*,b*=arg\ \ min_{w,b}\ \ L(w,b)=arg\ \ min_{w,b}\ \ \sum_{i=1}^{10}(\widehat{y}^i-(b+w\cdot x_{cp}^i))^2 \tag 7</script><p>对于以上，我们可以采用穷举法，求得$L(w,b)$的最小值，但下面介绍更加高效的算法。</p>
<ul>
<li><p>梯度下降法</p>
<p>若现在损失函数$L(w,b)$只考虑其$w$单变量情况，有以下函数图像：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233452.png" alt="2" style="zoom:67%;"></p>
<p>当我们现在<strong>随机</strong>在函数上选择一点$w^0$，</p>
<p>1.如果函数在该点的斜率为负，即导数$\frac{\mathrm{d}L}{\mathrm{d}w}&lt;0$，那么此时函数应该为<strong>递减的</strong>，于是可以考虑增大$w^0$。</p>
<p>2.如果函数在该点的斜率为正，即导数$\frac{\mathrm{d}L}{\mathrm{d}w}&gt;0$，那么此时函数应该为<strong>递增的</strong>，于是可以考虑减少$w^0$。</p>
<p>增大多少或减少多少？</p>
<p>定义每次移动的步长为：</p>
<script type="math/tex; mode=display">
step=\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0 \tag 8</script><p>在式（8）中，$\eta$称为”learning rate（学习率）”，<strong>我们可以通过控制$\eta$的大小，从而增加step，以此可以提高求解效率，但若$\eta$过大，则会导致步长过大，那么有可能会错过最优解，故我们需要选取合适的$\eta$。而$\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0$，其微分值越大$\rightarrow$曲线越陡峭$\rightarrow$移动步长越大$\rightarrow$提高求解效率。</strong></p>
<p>以上图为例，其算法如下：</p>
<script type="math/tex; mode=display">
w^0-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0\Longrightarrow w^1\\
w^1-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^1\Longrightarrow w^2\\
\vdots \ \ 更新循环\\
w^{t-1}-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{t-1}\Longrightarrow w^t</script><p>在上式中，因为$\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{0}&lt;0$，故需给其添加负号。对点不断更新循环，直到$(\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{t})=0$停止，此时$w=w^t$即所求参数的最优解。</p>
<p><strong>但从上图我们可以发现$w^t$其实并不为global optimal solution(全局最优解)，其为local optimal solution(局部最优解)，不过由于我们研究的为线性回归，所以不存在局部最优解。故在线性回归中，梯度下降法是有效的。</strong></p>
<p>现在，我们可以将单变量$w$推广到多变量$w,b$，函数如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233543.png" alt="3" style="zoom:67%;"></p>
<p>随机选取点$(w^0,b^0)$，计算函数在该点的偏导$\frac{\partial L}{\partial w}|w=w^0,b=b^0,\frac{\partial L}{\partial b}|w=w^0,b=b^0$。</p>
<p>那么函数在该点的梯度即为：</p>
<script type="math/tex; mode=display">
grad\ L(w^0,b^0)=\nabla L(w^0,b^0)=(\frac{\partial L}{\partial w}|w^0,b^0)\overrightarrow{i}+(\frac{\partial L}{\partial b}|w^0,b^0)\overrightarrow{j}\tag 9</script><p>其中$\nabla$为向量微分算子。</p>
<p><strong>而对于梯度向量，其方向为曲线在等值线上的法线方向（高数中有对其的证明），其通过不断的增加步长以更新点，从而达到最优解，如上图所示。</strong></p>
<p>与单变量相同，多变量算法与其相同：</p>
<script type="math/tex; mode=display">
\begin{cases}
w^0-\eta\frac{\partial L}{\partial w}|w=w^0,b=b^0\Longrightarrow w^1\\
b^0-\eta\frac{\partial L}{\partial b}|w=w^0,b=b^0\Longrightarrow b^1
\end{cases}\\
\begin{cases}
w^1-\eta\frac{\partial L}{\partial w}|w=w^1,b=b^1\Longrightarrow w^2\\
b^1-\eta\frac{\partial L}{\partial b}|w=w^1,b=b^1\Longrightarrow b^2
\end{cases}\\
\vdots\ \ 更新循环\\
\begin{cases}
w^t-\eta\frac{\partial L}{\partial w}|w=w^t,b=b^t\Longrightarrow w^t\\
b^t-\eta\frac{\partial L}{\partial b}|w=w^t,b=b^t\Longrightarrow b^t
\end{cases}\\</script><p>最后直到$\nabla L(w^t,b^t)=0$停止，此时<strong>$w^t,b^t$即为所求得的最优解，由于在线性回归中，所以不同担心会由于初始值的选取，而影响达到全局最优解。</strong></p>
</li>
</ul>
<h3 id="S-Result"><a href="#S-Result" class="headerlink" title="$\S $Result"></a>$\S $Result</h3><p>如果现在通过十组训练数据，得到的结果如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233626.png" alt="4" style="zoom:67%;"></p>
<p>如图所示，其通过梯度下降法，所求得的最优参数$b,w$分别为-188.4，2.7，其中，损失值可用$\frac{1}{10}\sum_{n=1}^{10}\mathrm{e}^n$表示，<strong>$e^n$即为第n组真实值与预测值的差值。</strong></p>
<p>现在将model改用不同函数，观察其结果变化情况，如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233707.png" alt="5" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233746.png" alt="6" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233820.png" alt="7" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233844.png" alt="8" style="zoom:67%;"></p>
<p>可以发现，<strong>随着Model改用函数的阶数的升高，训练集损失值越来越小，但可以发现测试集损失值却并不是越来越少，相反，当阶数大于4后，其测试集损失值增大较多，而这种现象叫做”Overfitting”(过拟合)。</strong></p>
<p>那么我们如何才能防止过拟合的发生呢？</p>
<ul>
<li><p><strong>Regularization(正则化)</strong></p>
<p><strong>由于过拟合是model过度拟合其在训练集的数据，则其会造成曲线波动较大，而如果能让曲线变得smooth平滑，便可防止过拟合。</strong></p>
<p><strong>于是考虑，增加一个与参数$w$（斜率）相关的值，如果loss function 越小的话，那么对应这个值也会越小，即$w$也会越小，于是便会使函数更加平滑。</strong></p>
<p>于是可在线性回归模型中，将loss function变为：</p>
<script type="math/tex; mode=display">
L=\sum_{i=1}^{10}(\widehat{y}^i-(b+\sum_jw_jx_j))^2+\lambda\sum_j(w_j)^2 \tag{10}</script><p>而在式（10）中，$\lambda\sum_j(w_j)^2$称为正则化项，$\lambda$称为正则化参数，它的作用就是平衡loss function这两项，若$\lambda$取得非常大，即对参数$w_j$惩罚的非常大，那么相当于$w_j$趋于0，即相当于原线性回归模型中删除了这些项，那么函数就变成了一条$y=b$的水平直线。</p>
<p>那么为什么减小$w_j$能够使其函数不宜发生overfitting呢？</p>
<p>若现在令$\varDelta x_j$为输入时的干扰项，那么输出所形成的误差即为：</p>
<script type="math/tex; mode=display">
y=b+\sum_jw_j(x_j+\varDelta x_j) \Longrightarrow \varDelta y=w_j\varDelta x_j</script><p>由此我们可以发现，当$w_j$较小时，产生的误差$\varDelta y$也更小，则更加光滑的函数会受更少的影响。</p>
<p>但观察正则化项，我们会发现为什么没有$b$呢？</p>
<p><strong>其实，正则化调整的是函数的平滑程度，但我们调整b对函数的平滑程度不会造成影响，它只能让函数上下移动。</strong></p>
</li>
</ul>
<h1 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a><strong>Classification</strong></h1><p>什么是分类呢?例如在现实生活中，当我们看见一张关于猫与狗的照片时，我们可以很容易区分，照片里的属于猫还是狗，那么如果现在有一些不同类别的大量数据，显然由人进行分类是不现实的，那么我们希望可以借助计算机来帮我们进行数据的分类……</p>
<p>那么我们如何解决分类问题呢？例如下图：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233933.png" alt="9" style="zoom:67%;"></p>
<p>希望能<strong>建立一个函数，输入一个神奇宝贝，就能返回其属性（类别）。</strong></p>
<p>首先我们需要将神奇宝贝数字化，即通过它的特征（Hp，Attack，Defense……）来描述这只神奇宝贝。首先将分类问题简化，只考虑其只有两类。</p>
<h2 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h2><p>我们的目标仍是寻找一个函数，输入x输出其类别，即为：</p>
<ul>
<li><p>Function (Model):</p>
<script type="math/tex; mode=display">
x \rightarrow \ f(x)=\begin{cases}
g(x)>0 \quad &output=class\ 1\\
else &output=class\ 2
\end{cases}</script><p>输入x，则当函数值&gt;0，其属于class 1，否则其属于class 2。而这个函数我们可以利用概率模型进行描述，即<script type="math/tex">P(C_1|x)>0.5</script></p>
</li>
<li><p>Loss Function:</p>
<p>Loss Function的目的是用来评价函数的好坏，那么我们想用模型分类的结果，与正确的模型结果进行对比，统计错误的次数。</p>
<script type="math/tex; mode=display">
L(f)=\sum_n\delta(f(x^n)\neq\widehat{y}^n)</script></li>
<li><p>Find the best function:</p>
<p>Example:perception(感知机算法)、svm(支持向量机)</p>
</li>
</ul>
<h2 id="贝叶斯概率模型"><a href="#贝叶斯概率模型" class="headerlink" title="贝叶斯概率模型"></a>贝叶斯概率模型</h2><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233955.png" alt="10" style="zoom:67%;"></p>
<p>在Box 1中我们可以计算得到抽出蓝色球的概率为<script type="math/tex">\frac{4}{5}</script>，绿色球的概率为<script type="math/tex">\frac{1}{5}</script>，若已知我们抽取Box 1的概率为<script type="math/tex">\frac{2}{3}</script>，那么我们就可以利用贝叶斯公式计算得到，已知抽取的小球为蓝色，则从Box 1抽出的机率为多少，即<script type="math/tex">P(B_1|Blue)</script>。</p>
<p>而现在我们可以将这些球看作是一个个数据，从而可以利用贝叶斯公式生成概率模型。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234009.png" alt="11" style="zoom:67%;"></p>
<ul>
<li>先验概率：<script type="math/tex">P(C)</script>，即每种类别发生的概率。</li>
<li>类条件概率：<script type="math/tex">P(x|C)</script>，在某种类别的条件下，某事发生的概率。</li>
<li>后验概率：<script type="math/tex">P(C|x)</script>，某事发生了，它属于某种类别的概率。</li>
</ul>
<p>如果我们可以计算得到<script type="math/tex">P(C_1)</script>，<script type="math/tex">P(C_2)</script>，与<script type="math/tex">P(x|C_1)</script>，<script type="math/tex">P(x|C_2)</script>，那么我们可以通过贝叶斯公式计算得到<script type="math/tex">P(C_1|x)</script>，<script type="math/tex">P(C_2|x)</script>，如果<script type="math/tex">P(C_1|x)>0.5</script>，那么可以认为<script type="math/tex">x</script>是属于Class 1 ，相反可认为<script type="math/tex">x</script>是属于Class 2的。</p>
<p>那么我们的目标就是计算<script type="math/tex">P(C_1)</script>，<script type="math/tex">P(C_2)</script>，与<script type="math/tex">P(x|C_1)</script>，<script type="math/tex">P(x|C_2)</script>。</p>
<ul>
<li><p><strong>先验概率的计算</strong></p>
<p>假设现在两个类别，Class 1：Water(水系)，Class 2：Normal(其他系)。</p>
<p><strong>水系有79只神奇宝贝，其他系有61只神奇宝贝，总样本共有140只神奇宝贝。</strong></p>
<script type="math/tex; mode=display">
\implies P(C_1)=\frac{79}{140}=0.56,\ P(C_2)=\frac{61}{140}=0.44</script><p>即通过所得样本中各自种类的占比得到先验概率，其计算较为简单。</p>
</li>
<li><p><strong>类条件概率的计算</strong></p>
<p>假设水系精灵中有6杰尼龟(水系)，那么我们可以认为在水系精灵中，杰尼龟的概率是<script type="math/tex">P(x|C_1)=\frac{6}{79}=0.076</script>吗？</p>
<p>事实上，这是不对的，<strong>因为该水系精灵样本较少，所以我们无法直接仅由样本中所占比例来推得类条件概率。假设该水系样本精灵中没有水箭龟，但水箭鬼确实是水系精灵，那么我们能说明水箭龟在水系精灵中的概率为0吗？这显然是有问题的。</strong></p>
<p>那么我们如何计算<script type="math/tex">P(x|C)</script>呢？我们利用<strong>极大似然估计法</strong>。</p>
</li>
</ul>
<h3 id="S-极大似然估计"><a href="#S-极大似然估计" class="headerlink" title="$\S$ 极大似然估计"></a>$\S$ 极大似然估计</h3><p>对于<script type="math/tex">P(x|C)</script>的计算，我们很难去寻找到随机变量<script type="math/tex">x</script>所遵循的概率密度函数。<strong>那么如果我们假设已知随机变量<script type="math/tex">x</script>所遵循的概率密度函数，转而计算所假设概率密度函数中的参数值，如果找到最佳的参数值，那么我们就能用该概率密度函数代替类条件概率的密度函数了。</strong></p>
<p>若有样本集<script type="math/tex">X=\{x^1,x^2,x^3,...,x^n\}</script>，每个样本集中的样本都是所谓独立同分布的随机变量。</p>
<p><strong>似然函数定义为：似然函数<script type="math/tex">L(\theta|x)</script>是给定样本x时，关于参数θ的函数，其在数值上等于给定参数<script type="math/tex">\theta</script>后变量X的概率</strong>：</p>
<script type="math/tex; mode=display">
L(\theta|x)=P(X=x|\theta)=P(x^1|\theta)P(x^2|\theta)\dots P(x^n|\theta)</script><p>在当<script type="math/tex">x_i</script>为离散型随机变量时，<script type="math/tex">f(x,\theta)</script>为概率密度函数，则<script type="math/tex">P(x|\theta)=f(x,\theta)</script>。</p>
<script type="math/tex; mode=display">
L(\theta|x)=f(x^1|\theta)f(x^2|\theta)\dots f(x^n|\theta)</script><p><strong>由于<script type="math/tex">X=\{x^1,x^2,x^3,...,x^n\}</script>即为已知数据，那么似然函数即为<script type="math/tex">L(\theta)</script>关于参数<script type="math/tex">\theta</script>的函数，而该似然函数的值越大，说明该参数的估计越佳，那么我们的目标即转变为找到最大的似然函数值，即为</strong></p>
<script type="math/tex; mode=display">
\theta^*=arg \ \ max\ L(\theta)</script><h3 id="S-类条件概率计算"><a href="#S-类条件概率计算" class="headerlink" title="$\S$类条件概率计算"></a>$\S$类条件概率计算</h3><p>回到类条件概率的计算，我们设水系精灵的样本集<script type="math/tex">X=\{x^1,x^2,x^3,...,x^{79}\}</script>，<strong>若每个<script type="math/tex">x^i</script>考虑n个特征</strong>，即<script type="math/tex">x^i=[x_1,x_2,\dots,x_n]</script>，服从<script type="math/tex">Gaussian\ distribution</script>(高斯分布) ，则有：</p>
<script type="math/tex; mode=display">
f_{\mu,B}(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|B|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)B^{-1}(x-\mu)^T}</script><p><strong>现在每个样本<script type="math/tex">x^i</script>仅考虑2个特征</strong>，即<script type="math/tex">x^i=[x_1,x_2]</script>，那么<script type="math/tex">x_i\thicksim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)</script>，即属于二维高斯分布，其概率密度函数为：</p>
<script type="math/tex; mode=display">
f_{\mu,B}(x)=\frac{1}{2\pi|B|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)B^{-1}(x-\mu)^T}</script><p><strong>其中<script type="math/tex">B</script>为协方差矩阵，<script type="math/tex">\mu</script>为均值向量。</strong></p>
<p>根据极大似然估计法，<script type="math/tex">x^i</script>均为独立同分布，且<script type="math/tex">x^i</script>均为离散型数据，则有<script type="math/tex">P(x^i|C)=f_{\mu,B}(x^i)</script>，即：</p>
<script type="math/tex; mode=display">
L(\mu,B)=P(x^1|C_1)P(x^2|C_1)\dots P(x^{79}|C_1)=f_{\mu,B}(x^1)f_{\mu,B}(x^2)\dots f_{\mu,B}(x^{79})=\prod_{i=1}^{79}f_{\mu,B}(x^i)</script><p><strong>那么似然函数<script type="math/tex">L(\mu,B)</script>就是关于参数<script type="math/tex">\mu,B</script>的函数。则最佳参数值即为：</strong></p>
<script type="math/tex; mode=display">
\mu^*,B^*=arg \ \ max_{\mu,B}\ L(\mu,B)</script><p>通过计算，求得最佳参数即为：</p>
<script type="math/tex; mode=display">
\mu^*=\frac{1}{79}\sum_{i=1}^{79}x^i\ \ , \ \ B^*=\frac{1}{79}\sum_{i=1}^{79}(x^i-\mu^*)(x^i-\mu^*)^T</script><p>带入已知数据集，可以分别计算得到两个类别的最佳参数值：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234034.png" alt="12" style="zoom:67%;"></p>
<h3 id="S-Result-1"><a href="#S-Result-1" class="headerlink" title="$\S$Result"></a>$\S$Result</h3><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234048.png" alt="13" style="zoom:67%;"></p>
<p><strong>观察可以发现，在Testing data上仅有47%的正确率。于是将<script type="math/tex">x^i</script>考虑的特征由2升为7，即<script type="math/tex">x^i</script>变成7维向量，但测试正确率仍只有54%。</strong></p>
<h3 id="S-模型改进"><a href="#S-模型改进" class="headerlink" title="$\S$模型改进"></a>$\S$模型改进</h3><p><strong>在最初的模型里，我们是利用似然函数，分别对Class 1 与Class 2进行参数估计，从而会产生两类参数，现在，我们将这两类公用同一个协方差矩阵<script type="math/tex">B</script>，而Class 1 与Class 2也共同用一个似然函数</strong>，即：</p>
<script type="math/tex; mode=display">
L(\mu_1,\mu_2,B)=f_{\mu_1,B}(x^1)f_{\mu_1,B}(x^2)\dots f_{\mu_1,B}(x^{79})f_{\mu_2,B}(x^{80})\dots f_{\mu_2,B}(x^{140})</script><p><strong>由此计算得到的<script type="math/tex">\mu_1，\mu_2</script>与最初计算结果相同，而<script type="math/tex">B=\frac{79}{140}B_1+\frac{61}{140}B_2</script>。</strong></p>
<p>而现在得到的结果如下所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234104.png" alt="14" style="zoom:67%;"></p>
<p>可以发现，当公用协方差矩阵<script type="math/tex">B</script>后，正确率可以提高，那么为什么会出现这种结果呢？<strong>因为当两个Class公用相同的协方差矩阵<script type="math/tex">B</script>后，可以减少参数，从而来防止模型发生Overfitting。</strong></p>
<h3 id="S-Naive-Bayes-Classifier"><a href="#S-Naive-Bayes-Classifier" class="headerlink" title="$\S$Naive Bayes Classifier"></a>$\S$Naive Bayes Classifier</h3><p>在上文中，我们的特征只有两个，如果特征变成多个应该如何处理呢？<script type="math/tex">x=[x_1,x_2,x_3,\dots,x_k ]</script>，<strong>如果各特征是在相互独立的情况下</strong>，那么有如下关系：</p>
<script type="math/tex; mode=display">
P(x|C_1)=P(x_1|C_1)P(x_2|C_1)\dots P(x_k|C_1)</script><p><strong>而这样做的好处是：<script type="math/tex">P(x_i|C_1)</script>遵循一维高斯分布，相较于二维高斯分布计算量大大降低，而这种方法叫做Naive Bayes Classifier(朴素贝叶斯分类器)。</strong></p>
<h3 id="S-概率密度函数选择"><a href="#S-概率密度函数选择" class="headerlink" title="$\S$概率密度函数选择"></a>$\S$概率密度函数选择</h3><p>在前文中，我们选择了随机变量服从高斯分布，事实上我们可以选择其他分布函数，这是比较随意的，但当随机变量呈现相关性质时，有一些特定的分布效果会更加好。</p>
<ul>
<li>当样本数据x取实数值时，采用高斯分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DN(\mu_{jc},\sigma_{jc})</script></li>
<li>当每种特征<script type="math/tex">x_j\in\{0,1\}</script>时，采用伯努利分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DBer(\mu_{jc},\sigma_{jc})</script></li>
<li>当每种特征取值<script type="math/tex">x_j\in\{1,2,3,\dots,k\}</script>时，采用分类分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DCat(\mu_{jc},\sigma_{jc})</script></li>
</ul>
<h2 id="Logistic-Regression-逻辑回归"><a href="#Logistic-Regression-逻辑回归" class="headerlink" title="Logistic Regression(逻辑回归)"></a>Logistic Regression(逻辑回归)</h2><p>在上节中，我们利用了贝叶斯公式，现在对贝叶斯公式进行一些变换：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\frac{1}{1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}</script><p>令<script type="math/tex">z=\ln\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}</script>，则有：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\frac{1}{1+\mathrm{e}^{-z}}=\sigma(z)</script><p>而<script type="math/tex">\sigma(z)</script>称为sigmoid function，其函数图像如下所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201120170942.png" style="zoom:67%;"></p>
<p>可以发现，<strong>sigmoid函数的值域在<script type="math/tex">(0,1)</script>之间，而它通常用于隐层神经元输出，可作为激活函数。</strong></p>
<h3 id="手推公式（可跳过）"><a href="#手推公式（可跳过）" class="headerlink" title="手推公式（可跳过）"></a>手推公式（可跳过）</h3><p>前文中，令<script type="math/tex">z=\ln\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}</script>，对其进行展开：</p>
<script type="math/tex; mode=display">
z=\ln\frac{P(x|C_1)}{P(x|C_2)}+\ln\frac{P(C_1)}{P(C_2)}\implies \ln\frac{P(x|C_1)}{P(x|C_2)}+\ln\frac{\frac{N_1}{N_1+N_2}}{\frac{N_2}{N_1+N_2}}</script><script type="math/tex; mode=display">
P(x|c_1)=\frac{1}{(2\pi)^{\frac{n}{2}}|B^1|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T}</script><script type="math/tex; mode=display">
P(x|c_2)=\frac{1}{(2\pi)^{\frac{n}{2}}|B^2|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T}</script><script type="math/tex; mode=display">
\begin{align*}
\ln\frac{\frac{1}{(2\pi)^{\frac{n}{2}}|B^1|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T}}{\frac{1}{(2\pi)^{\frac{n}{2}}|B^2|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T}}
&=\ln\frac{|B^2|^\frac{1}{2}}{|B^1|^\frac{1}{2}}\exp\{-\frac{1}{2}[(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T-(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T]\}\\
&=\ln\frac{|B^2|^\frac{1}{2}}{|B^1|^\frac{1}{2}}-\frac{1}{2}[(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T-(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T]
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}
(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T
&=(x-\mu^1)(B^1)^{-1}(x^T-(\mu^1)^T)\\
&=x(B^1)^{-1}x^T-x(B^1)^{-1}(\mu^1)^T-\mu^1(B^1)x^T+\mu^1(B^1)(\mu^1)^{T}\quad B^1\mbox{为对称矩阵}\\
&=x(B^1)^{-1}x^T-2\mu^1(B^1)x^T+\mu^1(B^1)(\mu^1)^{T}
\end{align*}</script><script type="math/tex; mode=display">
z=\ln\frac{|B^2|^\frac{1}{2}}{|B^1|^\frac{1}{2}}-\frac{1}{2}x(B^1)^{-1}x^T+\mu^1(B^1)^{-1}x^T-\frac{1}{2}\mu^1(B^1)^{-1}(\mu^1)^{T}+\frac{1}{2}x(B^2)^{-1}x^T-\mu^2(B^2)^{-1}x^T+\frac{1}{2}\mu^2(B^2)^{-1}(\mu^2)^{T}+\ln\frac{N_1}{N_2}</script><p>若假设<script type="math/tex">B^1=B^2=B</script>，则有：</p>
<script type="math/tex; mode=display">
\begin{align*}
z&=(\mu^1-\mu^2)B^{-1}x^T-\frac{1}{2}\mu^1B^{-1}(\mu^1)^{T}+\frac{1}{2}\mu^2B^{-1}(\mu^2)^{T}+\ln\frac{N_1}{N_2}\\
&=wx^T+b
\end{align*}</script><p>综上所述：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(w\cdot x+b)</script><h3 id="Function-set-函数集"><a href="#Function-set-函数集" class="headerlink" title="Function set(函数集)"></a>Function set(函数集)</h3><p>由以上公式推导可以得到:</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(z),\ z=w\cdot x+b=\sum_iw_ix_i+b</script><p>可以发现，<script type="math/tex">z</script>可用<script type="math/tex">\sum_iw_ix_i+b</script>进行线性表示。其中<script type="math/tex">w_i</script>是每个<script type="math/tex">x_i</script>的权重，<script type="math/tex">b</script>即为偏差，在这里<script type="math/tex">w\cdot x+b</script>即为决策边界，而这也是上文中为什么<strong>当两个class公用相同的协方差矩阵后，其决策边界变为直线的缘故</strong>。如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201121132947.png" style="zoom:67%;"></p>
<p><strong>这种函数集的分类问题叫做logistic regression(逻辑回归)。</strong></p>
<h3 id="Goodness-of-a-Function"><a href="#Goodness-of-a-Function" class="headerlink" title="Goodness of a Function"></a>Goodness of a Function</h3><p>假设现在有以下的training data:</p>
<script type="math/tex; mode=display">
\begin{align*}
data:&\ x^1 \ \ x^2 \ \ x^3 \dots x^n\\
class:&\ C_1 \ C_1 \ C_2 \dots C_1
\end{align*}</script><p>令<script type="math/tex">f_{w,b}(x)=P(C_1|x)</script>，则由极大似然估计，可定义以下损失函数：</p>
<script type="math/tex; mode=display">
L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\dots f_{w,b}(x^n)</script><p>而目标即为找到最大的<script type="math/tex">L(w,b)</script>，此时该函数的参数即为最佳参数值。</p>
<script type="math/tex; mode=display">
w^*,b^*=arg\ \ max \ L(w,b)</script><p>变形可得：</p>
<script type="math/tex; mode=display">
w^*,b^*=arg\ \ min \ -\ln L(w,b)</script><p>这样做的好处是，可以<strong>利用梯度下降算法求最小值，并且对<script type="math/tex">L(w,b)</script>取对数，可将乘积形式化为求和形式，易于后续计算。</strong></p>
<script type="math/tex; mode=display">
-\ln L(w,b)=-\ln f_{w,b}(x^1)-\ln f_{w,b}(x^2)-\ln[1- f_{w,b}(x^3)]\dots-\ln f_{w,b}(x^n)</script><p><strong>对class1，class2进行符号转换：</strong></p>
<script type="math/tex; mode=display">
class1:\widehat{y}^1=1,\widehat{y}^2=1\ \ class2:\widehat{y}^3=0\ \cdots</script><p>则：</p>
<script type="math/tex; mode=display">
-\ln f_{w,b}(x^1)\implies -[\widehat{y}^1\ln f(x^1)+(1-\widehat{y}^1)\ln (1-f(x^1))]</script><p>这样每个<script type="math/tex">-\ln f_{w,b}(x^i)</script>都可以统一成上式:</p>
<script type="math/tex; mode=display">
-\ln L(w,b)=\sum_i\color{orange}-[\widehat{y}^i\ln f(x^i)+(1-\widehat{y}^i)\ln (1-f(x^i))]</script><p>而橙色部分其实就是<strong>两个伯努利分布的交叉熵（Cross entropy)。</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201121154624.png" style="zoom:67%;"></p>
<p>如图所示，假设有两个分布：<script type="math/tex">p,q</script>如蓝色框所示，那么交叉熵的计算方式即$H(p,q)=-\sum_xp(x)ln(q(x))$。交叉熵代表的含义就是<strong>这两个分布有多接近</strong>。当两个分布相同时，计算的交叉熵就是熵。</p>
<h3 id="Find-the-best-function"><a href="#Find-the-best-function" class="headerlink" title="Find the best function"></a>Find the best function</h3><p>梯度下降求最小，计算步骤如图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201123003816.png" style="zoom:67%;"></p>
<p>对$\ln L(w,b)$求$w_i$的偏微分，只需要计算出$\frac{\ln f_{w,b}(x^n)}{\partial w_i}$与$\frac{\ln (1-f_{w,b}(x^n))}{\partial w_i}$。$f_{w,b}(x)$即为sigmoid 函数，$z=\sum_nw_ix_i+b$。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201123005251.png" style="zoom:67%;"></p>
<p>计算可得到$\frac{\ln (1-f_{w,b}(x^n))}{\partial w_i}$，将两个偏微分计算带入可得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
-\frac{\ln L(w,b)}{\partial w_i}&=\sum_n-[\widehat{y}^n(1-f_{w,b}(x^n_i))x^n_i+(1-\widehat{y}^n)f_{w,b}(x^n_i)x^n_i]\\
&=\sum_n-{\color{purple}(\widehat{y}^n-f_{w,b}(x^n))}x^n_i
\end{align*}</script><p>紫色部分其实直观的表示了真实数据值与函数值之间的差距大小。</p>
<h3 id="逻辑回归与线性回归的比较"><a href="#逻辑回归与线性回归的比较" class="headerlink" title="逻辑回归与线性回归的比较"></a>逻辑回归与线性回归的比较</h3><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201123011427.png" style="zoom:67%;"></p>
<p>可以发现，<strong>逻辑回归其实是在线性回归的基础上，对原线性模型再加上一层sigmoid 函数，从而将函数的输出介于(0,1)之间。而线性回归其输出可为任意值。</strong></p>
<p>在Logistic regression中，<strong>target的值为0或1</strong>，而在Linear regression中，<strong>target的值可为任意值</strong>。</p>
<h3 id="Discriminative-V-S-Generative-判别模型VS生成模型"><a href="#Discriminative-V-S-Generative-判别模型VS生成模型" class="headerlink" title="Discriminative V.S. Generative(判别模型VS生成模型)"></a>Discriminative V.S. Generative(判别模型VS生成模型)</h3><p>在前文中，其实已经介绍了两种不同的方法进行分类。</p>
<ol>
<li>通过假设概率密度函数求解类条件概率，从而通过贝叶斯公式分类，而这种分类方法就叫做生成模型(Generative model)。</li>
<li>通过利用梯度下降算法求解$w,b$。于是可带入逻辑回归模型进行分类，而这种分类方法就叫做判别模型(Discriminative model)。</li>
</ol>
<p>而对于这两种模型，<strong>它们的函数集都是相同的</strong>：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(wx+b)</script><p>对于判别模型，仅需要利用梯度下降算法，直接求解出参数$w,b$。</p>
<p>对于生成模型，需要通过假设概率分布求解出$\mu_1,\mu_2,B^{-1}$，而在<strong>手推公式</strong>这一节中，有如下代换：</p>
<script type="math/tex; mode=display">
\begin{align*}
w&=(\mu^1-\mu^2)B^{-1}\\
b&=-\frac{1}{2}\mu^1B^{-1}(\mu^1)^{T}+\frac{1}{2}\mu^2B^{-1}(\mu^2)^{T}+\ln\frac{N_1}{N_2}
\end{align*}</script><p>那么这两个不同模型得到的$w,b$相同吗？</p>
<p><strong>其实是不同的</strong>，为什么呢？因为在生成模型中，它是<strong>有假设</strong>的，例如它<strong>假设data满足高斯分布或伯努利分布等等</strong>，但在逻辑回归中，<strong>并没有这些假设</strong>，所以这导致了虽然它们的函数相同却最后计算得到的参数不同。</p>
<p>有些时候生成模型可能会出现一些问题。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201125003150.png" style="zoom:67%;"></p>
<p>例如，在这个例子中，共有13组数据，其中1组为class1，另外12组全部为class2。每组数据均有两个特征。现在当给出一个testing data，问其属于哪个类别。</p>
<p>对于人来说，第一反应它应该就是class1的。那么现在我们看看朴素贝叶斯分类器(naive bayes)是什么结果。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201125003002.png" style="zoom:67%;"></p>
<p>可以发现最后计算结果$P(C_1|x)&lt;0.5$，那么即朴素贝叶斯分类器认为当前tesing data属于class2，那么为什么会造成这个结果呢？<strong>朴素贝叶斯分类器是有假设这种情况存在的（机器脑补这种可能性）。所以结果和人类直观判断的结果不太一样。</strong></p>
<p>对于这两种模型，它们各自有各自的优势：</p>
<ol>
<li>当data数量较少时，<strong>生成模型受数据影响与判别模型相比，影响较小</strong>，因为它有<strong>自己的假设</strong>，甚至无视一些data。而当data较大时，判别模型的优势就较为明显了，因为当数据越多，<strong>它的误差就可以越小</strong>，从而模型越精确。</li>
<li><strong>当遇到一些data有问题时，而生成模型有做假设，有时候就可以把data中有问题的部分忽略掉。</strong></li>
</ol>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
