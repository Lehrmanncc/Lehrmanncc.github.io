<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>MySQL(Linux)</title>
    <url>/2021/04/05/MySQL(Linux)/</url>
    <content><![CDATA[<h1 id="MySQL安装"><a href="#MySQL安装" class="headerlink" title="MySQL安装"></a>MySQL安装</h1><p>通过apt安装MySQL服务</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 更新源</span></span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="comment"># 安装</span></span><br><span class="line">sudo apt-get install mysql-server</span><br></pre></td></tr></table></figure>
<p>安装完成后，进行初始化配置</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo mysql_secure_installation</span><br></pre></td></tr></table></figure>
<p>配置项如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#1</span></span><br><span class="line">VALIDATE PASSWORD PLUGIN can be used to test passwords...</span><br><span class="line">Press y|Y for Yes, any other key for No: N (选择N ,不会进行密码的强校验)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2</span></span><br><span class="line">Please <span class="keyword">set</span> the <span class="keyword">password</span> <span class="keyword">for</span> root here...</span><br><span class="line"><span class="keyword">New</span> <span class="keyword">password</span>: (输入密码)</span><br><span class="line">Re-enter <span class="keyword">new</span> <span class="keyword">password</span>: (重复输入)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3</span></span><br><span class="line"><span class="keyword">By</span> <span class="keyword">default</span>, a MySQL installation has an anonymous <span class="keyword">user</span>,</span><br><span class="line">allowing anyone <span class="keyword">to</span> <span class="keyword">log</span> <span class="keyword">into</span> MySQL <span class="keyword">without</span> <span class="keyword">having</span> <span class="keyword">to</span> have</span><br><span class="line">a <span class="keyword">user</span> <span class="keyword">account</span> created <span class="keyword">for</span> them...</span><br><span class="line">Remove anonymous <span class="keyword">users</span>? (Press y|Y <span class="keyword">for</span> Yes, <span class="keyword">any</span> other <span class="keyword">key</span> <span class="keyword">for</span> <span class="keyword">No</span>) : N (选择N，不删除匿名用户)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4</span></span><br><span class="line">Normally, root should <span class="keyword">only</span> be allowed <span class="keyword">to</span> <span class="keyword">connect</span> <span class="keyword">from</span></span><br><span class="line"><span class="string">&#x27;localhost&#x27;</span>. This ensures that someone cannot guess <span class="keyword">at</span></span><br><span class="line">the root <span class="keyword">password</span> <span class="keyword">from</span> the network...</span><br><span class="line"><span class="keyword">Disallow</span> root login remotely? (Press y|Y <span class="keyword">for</span> Yes, <span class="keyword">any</span> other <span class="keyword">key</span> <span class="keyword">for</span> <span class="keyword">No</span>) : N (选择N，允许root远程连接)</span><br><span class="line"></span><br><span class="line"><span class="comment">#5</span></span><br><span class="line"><span class="keyword">By</span> <span class="keyword">default</span>, MySQL comes <span class="keyword">with</span> a <span class="keyword">database</span> named <span class="string">&#x27;test&#x27;</span> that</span><br><span class="line">anyone can access...</span><br><span class="line">Remove <span class="keyword">test</span> <span class="keyword">database</span> <span class="keyword">and</span> <span class="keyword">access</span> <span class="keyword">to</span> it? (Press y|Y <span class="keyword">for</span> Yes, <span class="keyword">any</span> other <span class="keyword">key</span> <span class="keyword">for</span> <span class="keyword">No</span>) : N (选择N，不删除<span class="keyword">test</span>数据库)</span><br><span class="line"></span><br><span class="line"><span class="comment">#6</span></span><br><span class="line">Reloading the privilege <span class="keyword">tables</span> will ensure that <span class="keyword">all</span> changes</span><br><span class="line">made so <span class="keyword">far</span> will take effect immediately.</span><br><span class="line">Reload privilege <span class="keyword">tables</span> <span class="keyword">now</span>? (Press y|Y <span class="keyword">for</span> Yes, <span class="keyword">any</span> other <span class="keyword">key</span> <span class="keyword">for</span> <span class="keyword">No</span>) : Y (选择Y，修改权限立即生效)</span><br></pre></td></tr></table></figure>
<h1 id="Mycil安装"><a href="#Mycil安装" class="headerlink" title="Mycil安装"></a>Mycil安装</h1><p>传统的MySQL安装后是没有语法补全的，这当然会令我们比较痛苦，效率也比较低。而Mycil是其MySQL的命令行界面，其具有语法提示和语法高亮。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install mycil</span><br></pre></td></tr></table></figure>
<p>这样便安装好了mycil。</p>
<h1 id="MySQL服务"><a href="#MySQL服务" class="headerlink" title="MySQL服务"></a>MySQL服务</h1><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>Linux下要进入mysql服务器首先要启动mysql服务，其命令如下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo service mysql start</span><br></pre></td></tr></table></figure>
<p>相应的，若要关闭或重启mysql服务，则命令如下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo service mysql stop</span><br><span class="line">sudo service mysql restart</span><br></pre></td></tr></table></figure>
<p>这样后，Linux便启动mysql的服务了，之后我们便可以登陆进mysql</p>
<h2 id="登陆"><a href="#登陆" class="headerlink" title="登陆"></a>登陆</h2><p>在最新版本的mysql8.0中，其为了安全性，默认要进行用户验证，必须使用sudo才能进行登陆</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo mysql -u root -p</span><br></pre></td></tr></table></figure>
<p>之后便是输入root用户的密码，然后便可进入mysql数据库了。</p>
<p>mycil与mysql登陆基本一致，不过它是将root密码一次性输入，例如</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo mycil -u root -p 123456</span><br></pre></td></tr></table></figure>
<p>这里的123456即为root密码，这样就可以登陆进mycil了。</p>
<h1 id="数据库操作"><a href="#数据库操作" class="headerlink" title="数据库操作"></a>数据库操作</h1><h2 id="查看数据库"><a href="#查看数据库" class="headerlink" title="查看数据库"></a>查看数据库</h2><p>进入mysql后，我们可以先查看mysql的版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select version();</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210410234424.png" style="zoom:67%;"></p>
<p>可以发现mysql的版本是8.0.23</p>
<p>当进入mysql数据库后，可以使用show语句进行查看所有数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">show databases;</span><br></pre></td></tr></table></figure>
<p>不能忘记分号 “;”</p>
<h2 id="创建新数据库"><a href="#创建新数据库" class="headerlink" title="创建新数据库"></a>创建新数据库</h2><p>新建一个数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create database 数据库名称;</span><br></pre></td></tr></table></figure>
<p>相应的删除操作即为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">drop database 数据库名称;</span><br></pre></td></tr></table></figure>
<h2 id="数据表操作"><a href="#数据表操作" class="headerlink" title="数据表操作"></a>数据表操作</h2><p>对于创建的数据库，如果我们要使用它，便有以下语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">use 数据库名称;</span><br></pre></td></tr></table></figure>
<p>在使用这个数据库后，即进入了该数据库，而数据库是由表构成的，于是我们可以新建一个表：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table stuinfo(id int,name varchar(10),loaction char(20));</span><br></pre></td></tr></table></figure>
<p>注意：这里的varchar与char不同，char是固定长度，而varchar为可变长度。故char(20)是固定长度20的字符类型，而varchar(10)则是可变最大长度为10的字符类型。</p>
<p>现在我们可以使用命令，观察刚才创建表的结构</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">desc stuinfo;</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210410213513.png" style="zoom:67%;"></p>
<h3 id="增"><a href="#增" class="headerlink" title="增"></a>增</h3><p>现在这个表还是空的，那我们要为它插入一些值，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">insert into stuinfo values (1,&#39;xiaoming&#39;,&#39;shanghai&#39;);</span><br><span class="line">insert into stuinfo values (2,&#39;lihan&#39;,&#39;xian&#39;);</span><br><span class="line">insert into stuinfo values (3,&#39;wanggang&#39;,&#39;beijing&#39;);</span><br></pre></td></tr></table></figure>
<p>现在我们可以查看整个表，命令如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from stuinfo;</span><br></pre></td></tr></table></figure>
<p>其中 * 代表所有列，而表名后不加where，则代表所有行，因此取出所有行，所有列。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210410214441.png" style="zoom:67%;"></p>
<p><strong>但我们需要知道，这条语句是暴力查询，在平时我们应尽量避免使用，当我们要查特定某些列或行时，应指明列名，避免查询整张表，从而减少系统的负担。</strong></p>
<h3 id="改"><a href="#改" class="headerlink" title="改"></a>改</h3><p>当我们要替换其中的一个值时，要怎么办呢？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">update stuinfo set name&#x3D;&#39;liliang&#39; where id&#x3D;2;</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210410215805.png" style="zoom:67%;"></p>
<p>可以发现，原来’lihan’已经被替换成了’liliang’</p>
<h3 id="删"><a href="#删" class="headerlink" title="删"></a>删</h3><p>删除就是指删除一整行，我们可以使用如下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">delete from stuinfo where id&#x3D;3;</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210410220406.png" style="zoom:67%;"></p>
<p>可以发现，第三行已经删除了。</p>
<p>那如果我们想删除整个表，使用什么命令比较简洁呢？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">drop table stuinfo;</span><br></pre></td></tr></table></figure>
<p>便可以直接删除整个stuinfo表。</p>
<h3 id="查"><a href="#查" class="headerlink" title="查"></a>查</h3><p>如果我们要特定查看某一行的数值是否存在，则可以使用select语句实现</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from student_1 where name&#x3D;&#39;ppp&#39;</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210411105320.png" style="zoom:67%;"></p>
<p>这样name=’ppp’的这一行就能被特定搜索出来了。</p>
<p>当然我们也可以指定特定的某些列，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select age,location from student_1 where name&#x3D;&#39;王明&#39;</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210411110257.png" style="zoom:67%;"></p>
<h2 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h2><p>单行注释</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#注释文字</span><br><span class="line">-- 注释，注意--后有空格</span><br></pre></td></tr></table></figure>
<p>多行注释</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;* 注释文字 *&#x2F;</span><br></pre></td></tr></table></figure>
<h2 id="保存文件"><a href="#保存文件" class="headerlink" title="保存文件"></a>保存文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tee ~&#x2F;sql&#x2F;</span><br></pre></td></tr></table></figure>
<p>将sql文件保存在sql文件夹下</p>
<h1 id="列类型"><a href="#列类型" class="headerlink" title="列类型"></a>列类型</h1><h2 id="数值型"><a href="#数值型" class="headerlink" title="数值型"></a>数值型</h2><h3 id="整型"><a href="#整型" class="headerlink" title="整型"></a>整型</h3><ul>
<li><p>tinyint</p>
<p>占据空间：1字节(八位)，存储范围：(有符号)：-128~127，(无符号)：0~255。</p>
</li>
<li><p>smallint</p>
<p>占据空间：2字节(16位)，存储范围：-32768~32767，0~65535</p>
</li>
<li><p>mediumint</p>
<p>占据空间：3字节(24位)</p>
</li>
<li><p>int</p>
<p>占据空间：4字节(32位)</p>
</li>
<li><p>bigint</p>
<p>占据空间：8字节(64位)</p>
<p>$n$字节，其共有$8n$位，则存储范围为：</p>
<script type="math/tex; mode=display">
\mbox{无符号：}0\sim2^{8n}-1\\
\mbox{有符号：}-2^{8n-1}\sim2^{8n-1}-1</script></li>
</ul>
<p>但在申明类型时，例如int型，我们该如何选择它是无符号还是有符号的呢？</p>
<p>首先创建一个表，其中age采用tinyint 型</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210411202853.png" style="zoom:67%;"></p>
<p>向表里插入一个age值为128的人</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210411203009.png" style="zoom:67%;"></p>
<p>可以发现，其报错了，128超过了tinyint的范围，则证明tinyint不加特殊说明时，默认是有符号的，其范围为-128~127。</p>
<p>为表再加一列，其命令为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter table 表名 add 列明 参数</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210411204922.png" style="zoom:67%;"></p>
<p>上述即为创建了一个名为score的列，其tinyint为unsigned，即无符号；不允许为空，默认值为0。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210411205526.png" style="zoom:67%;"></p>
<p>可以发现，在定义了tinyint 为unsigned后，其范围即变为0~255。</p>
<p>但如果现在要求我们给班级的每个人添加上学号应该怎么办？学号一般是有固定长度的，例如：000001，000029等，这应该怎么实现呢？</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210412203140.png" style="zoom:67%;"></p>
<p>在这里我们使用了(6)与zerofill两个参数，可以发现num的默认值变为000000了。同时可以发现num的类型多出来了unsigned（<strong>由于零填充不可能为负值，故zerofill同时也是unsigned类型</strong>）现在插入几个值试试。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210412203524.png" style="zoom:67%;"></p>
<p>于是我们可以总结一下int型的参数：</p>
<script type="math/tex; mode=display">
\begin{cases}
\mbox{unsinged：      int型值无符号}\\
\mbox{(M)，zerofill：两者搭配使用，M为补零长度，单独使用使没有意义的}
\end{cases}</script><h3 id="浮点型"><a href="#浮点型" class="headerlink" title="浮点型"></a>浮点型</h3><p><strong>float(M,D)</strong>与<strong>decimal(M,D)</strong></p>
<p>其中M表示“精度”——代表总位数，而D代表“标度”——小数点右边的位数。</p>
<p>同时浮点型也可以在其后面定义unsigned，如下所示定义</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210413142003.png" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210413142107.png" style="zoom:67%;"></p>
<p>而float的整数部分能存10\^38，小数部分能存10\^-38。</p>
<p>如果M&lt;24，则其占4个字节，否则占8个字节。</p>
<p><strong>另一种存储方式叫定点decimal，是把整数部分和小数部分分开存储的，其比float精确。</strong></p>
<p>我们通过实验可以发现两者差异，首先创建一个表</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210413160657.png" style="zoom:67%;"></p>
<p>后向表中插入值，可以发现</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210413162910.png" style="zoom:67%;"></p>
<p>acc1中的值与真实值不相符，为1234567.38，而真实值为1234567.36。其中acc1是float型的，acc2是decimal型的。故可以发现float的精度有时会有损精度</p>
<h3 id="字符型"><a href="#字符型" class="headerlink" title="字符型"></a>字符型</h3><ul>
<li><p><strong>char</strong>：</p>
<p>定长，例如char(4)即创建4个长度的字符型，不管内容如何其长度都为4。这会导致浪费空间。<strong>创建n个长度，如果内容不够nn个长度，则会用空格补齐至n个长度。</strong></p>
<p>但char类型由于其每个是固定长度的，所以其<strong>查找较快</strong>，因为可通过固定长度直接计算得到文件指针位置。</p>
</li>
<li><p><strong>varchar</strong>：</p>
<p>变长，其不用空格补齐。<strong>例如varchar(10)，如果现在存储字符串’小明’，则它的存储长度并不是2，在’小明’的前面会有前缀，来记录内容的长度，在这个例子里该前缀的填入的内容即为2</strong>。</p>
</li>
</ul>
<p>则以上可以发现char的利用率可能得到100%，但varchar的利用率一定&lt;100%。</p>
<p><strong>注意char(M)，varchar(M)限制的是字符，不是字节。</strong></p>
<ul>
<li><p>text：</p>
<p>文本类型，可以存比较大的文本，但搜索较慢。因此，如果不是特别大的内容，建议用char、varchar来代替。</p>
<p>注意，<strong>声明text列时，不必给它默认值。</strong></p>
</li>
</ul>
<h2 id="时期时间类型"><a href="#时期时间类型" class="headerlink" title="时期时间类型"></a>时期时间类型</h2><p>可能我们会有这样的一个疑问？既然已经有了char型与varchar型，那么用它们进行存储时期与时间不就完事了吗？为什么还要用专门的时期时间类型。<strong>从存储空间上来说，时期时间类型对存储的时期与时间进行了优化，其存储空间会小于用char与varchar存储的空间。</strong></p>
<h3 id="date型"><a href="#date型" class="headerlink" title="date型"></a>date型</h3><p>创建一个具有date列类型的表，一般例如一个人的生日会用到时期。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210414000446.png" style="zoom:67%;"></p>
<p>插入数值，可以得到</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210414000645.png" style="zoom:67%;"></p>
<p><strong>date型的的标准格式为：YYYY-MM-DD，而其范围为1000-01-01~9999-12-31。</strong></p>
<h3 id="time"><a href="#time" class="headerlink" title="time"></a>time</h3><p>time类型与date类型格式较相同，不过其表示的为<strong>HH-MM-SS，即时-分-秒</strong>。</p>
<h3 id="datetime"><a href="#datetime" class="headerlink" title="datetime"></a>datetime</h3><p>datetime类型是date与time的结合体，就是年-月-日-时-分-秒。</p>
<h3 id="timestamp"><a href="#timestamp" class="headerlink" title="timestamp"></a>timestamp</h3><p>timestamp在取默认值时，比较特殊，其可以记录当前的时间。例如新增一列sign，其type：timestamp。其defalut：<strong>CURRENT_TIMESTAMP</strong>，其为<strong>系统常量，时时刻刻指向当前时间</strong>。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210415225223.png" style="zoom:67%;"></p>
<p>插入值，可得：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210415225323.png" style="zoom:67%;"></p>
<p>其默认值即为当前的时间。</p>
<h1 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h1><p>在实际开发中，我们需要创建表时，需要首先分析需求，并进行优化。</p>
<p>一般来说，优化原则为：<strong>经常使用并查看的信息，可以放在一张表中，并采取定长，可以极大优化搜索效率；不常用的信息，可以放在另外一张表中，可采取变长，可以节省空间。</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>列名称</th>
<th>列类型</th>
<th>默认值</th>
<th>是否主键</th>
</tr>
</thead>
<tbody>
<tr>
<td>Id</td>
<td>int unsigned</td>
<td></td>
<td>PRL</td>
</tr>
<tr>
<td>Username</td>
<td>char(20)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Gender</td>
<td>char(1)/tinyint</td>
<td></td>
<td></td>
</tr>
<tr>
<td>weight</td>
<td>tinyint unsigned</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Birth</td>
<td>date</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Salary</td>
<td>decimal(8,2)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>lastlogin</td>
<td>int unsigned</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>我们根据上表建立数据表，</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210415235404.png" style="zoom:67%;"></p>
]]></content>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Love record</title>
    <url>/2020/11/10/Love/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="e33c091047010d6935b3f264258306604fdd4b3a48b8391216b6112f40eac053">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <tags>
        <tag>Love record</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch实现回归</title>
    <url>/2021/01/06/Regression/</url>
    <content><![CDATA[<h1 id="实现回归"><a href="#实现回归" class="headerlink" title="实现回归"></a>实现回归</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>首先是一些基础包的导入</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment">#sns.set()可以设置绘图格式</span></span><br><span class="line">sns.set()</span><br><span class="line">sns.set_context(<span class="string">&quot;notebook&quot;</span>, rc=&#123;<span class="string">&quot;font.size&quot;</span>:<span class="number">16</span>,</span><br><span class="line"><span class="string">&quot;axes.titlesize&quot;</span>:<span class="number">20</span>,</span><br><span class="line"><span class="string">&quot;axes.labelsize&quot;</span>:<span class="number">18</span>&#125;)</span><br><span class="line">CB91_Blue = <span class="string">&#x27;#2CBDFE&#x27;</span></span><br><span class="line">CB91_Green = <span class="string">&#x27;#47DBCD&#x27;</span></span><br><span class="line">CB91_Pink = <span class="string">&#x27;#F3A0F2&#x27;</span></span><br><span class="line">CB91_Purple = <span class="string">&#x27;#9D2EC5&#x27;</span></span><br><span class="line">CB91_Violet = <span class="string">&#x27;#661D98&#x27;</span></span><br><span class="line">CB91_Amber = <span class="string">&#x27;#F5B14C&#x27;</span></span><br><span class="line">color_list = [CB91_Blue, CB91_Pink, CB91_Green, CB91_Amber, CB91_Purple, CB91_Violet]</span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.prop_cycle&#x27;</span>] = plt.cycler(color=color_list)</span><br></pre></td></tr></table></figure>
<p>产生随机数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=np.arange(<span class="number">20</span>)</span><br><span class="line">y=np.array([<span class="number">5</span>*x[i]+random.randint(<span class="number">1</span>,<span class="number">20</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x))])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x)</span><br><span class="line">print(y)</span><br><span class="line"><span class="comment">#输出如下</span></span><br><span class="line">[ <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> <span class="number">16</span> <span class="number">17</span> <span class="number">18</span> <span class="number">19</span>]</span><br><span class="line">[  <span class="number">2</span>  <span class="number">20</span>  <span class="number">24</span>  <span class="number">24</span>  <span class="number">30</span>  <span class="number">43</span>  <span class="number">40</span>  <span class="number">55</span>  <span class="number">45</span>  <span class="number">48</span>  <span class="number">66</span>  <span class="number">73</span>  <span class="number">80</span>  <span class="number">76</span>  <span class="number">75</span>  <span class="number">79</span>  <span class="number">88</span> <span class="number">104</span> <span class="number">100</span> <span class="number">107</span>]</span><br></pre></td></tr></table></figure>
<p>作图如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(x,y)</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106211206.png" alt="img"></p>
<p>训练数据，首先要转换为Tensor类型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_train = t.from_numpy(x).float() </span><br><span class="line">y_train = t.from_numpy(y).float()</span><br><span class="line"></span><br><span class="line">print(x_train)</span><br><span class="line">print(y_train)</span><br><span class="line"><span class="comment">#打印输出后的结果</span></span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>,</span><br><span class="line">        <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>])</span><br><span class="line">tensor([  <span class="number">2.</span>,  <span class="number">20.</span>,  <span class="number">24.</span>,  <span class="number">24.</span>,  <span class="number">30.</span>,  <span class="number">43.</span>,  <span class="number">40.</span>,  <span class="number">55.</span>,  <span class="number">45.</span>,  <span class="number">48.</span>,  <span class="number">66.</span>,  <span class="number">73.</span>,</span><br><span class="line">         <span class="number">80.</span>,  <span class="number">76.</span>,  <span class="number">75.</span>,  <span class="number">79.</span>,  <span class="number">88.</span>, <span class="number">104.</span>, <span class="number">100.</span>, <span class="number">107.</span>])</span><br></pre></td></tr></table></figure>
<p>定义训练模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>(<span class="params">t.nn.Module</span>):</span>  <span class="comment">#继承父类:t.nn.Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(LinearRegression,self).__init__() </span><br><span class="line">        self.linear = t.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br></pre></td></tr></table></figure>
<p>super()函数作用：<strong>首先找到父类t.nn.Module,后将类LinearRegression的对象self转换为t.nn.Module的对象,然后“被转换”的类nn.Module对象调用自己的__init__函数.</strong></p>
<p>进行模型的训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = LinearRegression()</span><br><span class="line">criterion = t.nn.MSELoss()  <span class="comment">#定义损失函数</span></span><br><span class="line">optimizer = t.optim.SGD(model.parameters(),<span class="number">0.001</span>)  <span class="comment">#利用SGD优化算法,首先传入需要优化的参数,与学习率</span></span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">1000</span>   <span class="comment">#迭代次数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    input_data = x_train.unsqueeze(<span class="number">1</span>)  <span class="comment">#输入数据进行升维:[20]---&gt;[20,1]</span></span><br><span class="line">    target = y_train.unsqueeze(<span class="number">1</span>)  </span><br><span class="line">    out = model(input_data)   <span class="comment">#模型输出</span></span><br><span class="line">    loss = criterion(out,target) <span class="comment">#将模型输出与target进行比较,计算loss</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment">#梯度信息置为0,作用:将前一个batch的梯度计算结果置为0,因为它没有保留的作用了</span></span><br><span class="line">    loss.backward()   <span class="comment">#利用反向传播计算梯度</span></span><br><span class="line">    optimizer.step()  <span class="comment">#进行单次的优化,参数的更新</span></span><br><span class="line">    <span class="comment">#print(&quot;Epoch:[&#123;&#125;/&#123;&#125;],loss:[&#123;:.4f&#125;]&quot;.format(i+1,num_epochs,loss.item()))  </span></span><br><span class="line">    <span class="comment">#loss.item()可以直接获得loss的值,loss是仅有一个量的张量,故用item()可转换为python标量</span></span><br><span class="line">    <span class="keyword">if</span> ((i+<span class="number">1</span>)%<span class="number">200</span>==<span class="number">0</span>):   </span><br><span class="line">        predict = model(input_data)</span><br><span class="line">        plt.plot(x_train.data.numpy(),predict.squeeze(<span class="number">1</span>).data.numpy())</span><br><span class="line">        loss=criterion(predict,target)</span><br><span class="line">        plt.title(<span class="string">&quot;Loss:&#123;:.4f&#125;&quot;</span>.format(loss.item()))</span><br><span class="line">        plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&quot;Y&quot;</span>)</span><br><span class="line">        plt.scatter(x_train,y_train)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<p>进行1000次迭代，每两百次打印图片</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106213013.png"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106213057.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106213121.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106213202.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106213241.png" alt></p>
<h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>线性回归虽然可以拟合出来一条直线，但精度较低，可以看到上图的线性回归的Loss的值为<strong>43.2122</strong>，是较高的。我们可以通过多项式回归提高模型精度，多项式回归就是提高特征的次数，如前面的线性回归的$x$是1次，实际上可以采用二次、三次的，增加模型的复杂度，可能会带来overfitting。</p>
<h3 id="Example-Ⅰ"><a href="#Example-Ⅰ" class="headerlink" title="Example Ⅰ"></a>Example Ⅰ</h3><p>下面实现一个简单的多项式回归，用模型拟合一个复杂的多项式方程：</p>
<script type="math/tex; mode=display">
f(x)=-1.13x-2.14x^2+3.15x^3-0.01x^4+0.512</script><p>多项式数据准备：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在[-3,3]之间生成50个点,返回一个一维张量</span></span><br><span class="line">x = t.linspace(<span class="number">-3</span>,<span class="number">3</span>,<span class="number">50</span>)  </span><br><span class="line">y = <span class="number">-1.13</span>*x<span class="number">-2.14</span>*t.pow(x,<span class="number">2</span>)+<span class="number">3.15</span>*t.pow(x,<span class="number">3</span>)<span class="number">-0.01</span>*t.pow(x,<span class="number">4</span>)+<span class="number">0.512</span></span><br><span class="line">plt.scatter(x.data.numpy(),y.data.numpy())</span><br></pre></td></tr></table></figure>
<p>借助公式随机产生50个点，可视化图像如下：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210113232837.png" alt></p>
<p>由于现在的输入不再是一元函数的一维，而是现在的四维，输入变成一个矩阵的形式：</p>
<script type="math/tex; mode=display">
X=\left[\array{x_1^1 & \dots &x_1^4\\
\vdots & \ddots & \vdots\\
x_n^1 & \dots & x_n^4}\right]</script><p>其中$x_n^4$代表第n组样本的第四个feature。</p>
<p>下面将数据拼接成如上所示的矩阵形式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">features</span>(<span class="params">x</span>):</span></span><br><span class="line">    x = x.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> t.cat([x**i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>)],<span class="number">1</span>)   <span class="comment">#将各向量连接在一起,形成矩阵形式</span></span><br></pre></td></tr></table></figure>
<p>现在得到了标准的输入矩阵$X$，还缺少$y$的值，而$y$是通过函数$f(x)$计算出来的，通过以下函数来计算得到$y$值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_weight = t.Tensor([<span class="number">-1.13</span>,<span class="number">-2.14</span>,<span class="number">3.15</span>,<span class="number">-0.01</span>])</span><br><span class="line">x_weight = x_weight.unsqueeze(<span class="number">1</span>)</span><br><span class="line">b = t.Tensor([<span class="number">0.512</span>])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">target</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x.mm(x_weight)+b.item()   <span class="comment">#矩阵相乘,表示x*x_weight,b,item()获取b标量中的数值</span></span><br></pre></td></tr></table></figure>
<p>上面的代码用到了Tensor的mm方法，它是表示矩阵相乘（Matrix Multiplication）。现在通过上面两个方法，批量生成用于训练的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 新建一个随机生成输入数据和输出数据的函数，用于生成train data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span>(<span class="params">batch_size</span>):</span></span><br><span class="line">    <span class="comment"># 生成batch个随机的x</span></span><br><span class="line">    batch_x = t.randn(batch_size)</span><br><span class="line">    feature_x = features(batch_x)</span><br><span class="line">    target_y = target(feature_x)</span><br><span class="line">    <span class="keyword">return</span> feature_x,target_y</span><br></pre></td></tr></table></figure>
<p>下面创建多项式回归模型，使用torch.nn.Linear模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PloynomialRegression</span>(<span class="params">t.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(PloynomialRegression,self).__init__()</span><br><span class="line">        self.ploy = t.nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 输入维度为4维，输出维度为1维</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.ploy(x)</span><br></pre></td></tr></table></figure>
<p>模型新建好后开始训练模型，为了动态的显示模型训练结果，在程序中设置每1000个epoch，就对测试数据进行一次预测，并将预测的误差及预测的输出值可视化显示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 开始训练模型，误差函数使用MSELoss，使用SGD优化参数</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">model = PloynomialRegression()</span><br><span class="line">criterion = t.nn.MSELoss()</span><br><span class="line">optimizer = t.optim.SGD(model.parameters(),<span class="number">0.001</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    batch_x,batch_y = get_data(batch_size)</span><br><span class="line">    out = model(batch_x)</span><br><span class="line">    loss = criterion(out,batch_y)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">if</span>(epoch%<span class="number">200</span>==<span class="number">0</span>):</span><br><span class="line">        predict = model(features(x))</span><br><span class="line">        plt.plot(x.data.numpy(),predict.squeeze(<span class="number">1</span>).data.numpy(),CB91_Pink)</span><br><span class="line">        loss = criterion(predict,y.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        plt.title(<span class="string">&quot;Loss:&#123;:.4f&#125;&quot;</span>.format(loss.item()))</span><br><span class="line">        plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&quot;Y&quot;</span>)</span><br><span class="line">        plt.scatter(x,y)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210114151321.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210114151343.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210114151403.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210114151420.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210114151445.png" alt></p>
<p>经过大量的迭代后，模型能够很好的拟合测试数据。</p>
<h3 id="Example-Ⅱ"><a href="#Example-Ⅱ" class="headerlink" title="Example Ⅱ"></a>Example Ⅱ</h3><p>实现更复杂的曲线拟合，这里我们对心形曲线进行拟合，函数表达式如下：</p>
<script type="math/tex; mode=display">
\begin{cases}
x=16sin^3(t)\\
y=13cos(t)-5cos(2t)-2cos(3t)-cos(4t)
\end{cases}</script><p>首先借助公式随机生成10000个点：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">e = t.linspace(<span class="number">-15</span>,<span class="number">15</span>,<span class="number">10000</span>)  </span><br><span class="line">x1 = <span class="number">16</span>*t.sin(e)**<span class="number">3</span></span><br><span class="line">y1 = <span class="number">13</span>*t.cos(e)<span class="number">-5</span>*t.cos(<span class="number">2</span>*e)<span class="number">-2</span>*t.cos(<span class="number">3</span>*e)-t.cos(<span class="number">4</span>*e)</span><br><span class="line">plt.scatter(x1.data.numpy(),y1.data.numpy())</span><br></pre></td></tr></table></figure>
<p>可视化如下：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115182453.png" alt></p>
<p>与Example Ⅰ不同的是，在这里有两个变量，需要对X,Y都要进行预测。</p>
<p>对X，Y两个变量，分别进行数据处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fea_x</span>(<span class="params">e</span>):</span></span><br><span class="line">    e = e.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> t.sin(e)**<span class="number">3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fea_y</span>(<span class="params">e</span>):</span></span><br><span class="line">    e = e.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> t.cat([t.cos(i*e) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>)],<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>有了这两个方法，便可以批量生成X,Y的训练数据了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data</span>(<span class="params">size</span>):</span></span><br><span class="line">    r = t.randn(size)</span><br><span class="line">    x_fea = fea_x(r)</span><br><span class="line">    y_fea = fea_y(r)</span><br><span class="line">    x_tg = <span class="number">16</span>*x_fea</span><br><span class="line">    y_tg = tg_y(y_fea)</span><br><span class="line">    <span class="keyword">return</span> x_fea,y_fea,x_tg,y_tg</span><br></pre></td></tr></table></figure>
<p>接下来就是定义模型，由于有两个变量，需要分别进行预测，故对X,Y定义两个模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HeartModel_x</span>(<span class="params">t.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(HeartModel_x,self).__init__()</span><br><span class="line">        self.heartx = t.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.heartx(x)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HeartModel_y</span>(<span class="params">t.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(HeartModel_y,self).__init__()</span><br><span class="line">        self.hearty = t.nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span>  <span class="comment">#self必须是任意函数的首个参数</span></span><br><span class="line">        <span class="keyword">return</span> self.hearty(x)</span><br></pre></td></tr></table></figure>
<p>最后，进行模型训练，与Example Ⅰ大致相同，不再赘述，不同的是同时对X，Y进行训练：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epoachs = <span class="number">50000</span></span><br><span class="line">batch_size = <span class="number">10000</span></span><br><span class="line">model_x = HeartModel_x()</span><br><span class="line">model_y = HeartModel_y()</span><br><span class="line">criterion = t.nn.MSELoss()</span><br><span class="line">optimizer1 = t.optim.SGD(model_x.parameters(),<span class="number">0.001</span>)</span><br><span class="line">optimizer2 = t.optim.SGD(model_y.parameters(),<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoach <span class="keyword">in</span> range(epoachs):</span><br><span class="line">    x_train,y_train,x_target,y_target = data(batch_size)</span><br><span class="line">    out_x = model_x(x_train)</span><br><span class="line">    out_y = model_y(y_train)</span><br><span class="line">    </span><br><span class="line">    loss_x = criterion(out_x,x_target)</span><br><span class="line">    loss_y = criterion(out_y,y_target)</span><br><span class="line">    </span><br><span class="line">    optimizer1.zero_grad()</span><br><span class="line">    optimizer2.zero_grad()</span><br><span class="line">    </span><br><span class="line">    loss_x.backward()</span><br><span class="line">    loss_y.backward()</span><br><span class="line">    </span><br><span class="line">    optimizer1.step()</span><br><span class="line">    optimizer2.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(epoach%<span class="number">10000</span>==<span class="number">0</span>):</span><br><span class="line">        predict_x = model_x(fea_x(e))</span><br><span class="line">        predict_y = model_y(fea_y(e))</span><br><span class="line">        plt.plot(predict_x.squeeze(<span class="number">1</span>).data.numpy(),predict_y.squeeze(<span class="number">1</span>).data.numpy(),CB91_Pink)</span><br><span class="line">        loss_x = criterion(predict_x,x1.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        loss_y = criterion(predict_y,y1.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        plt.title(<span class="string">&quot;Loss X:&#123;:.4f&#125;,Loss Y:&#123;:.4f&#125;&quot;</span>.format(loss_x,loss_y))</span><br><span class="line">        plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&quot;Y&quot;</span>)</span><br><span class="line">        plt.scatter(x1,y1)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<p>每10000次迭代打印一次训练结果，并将其可视化如下：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115213159.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115213309.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115213335.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115213400.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115213426.png" alt></p>
]]></content>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenCV 基础操作</title>
    <url>/2020/10/14/opencv/</url>
    <content><![CDATA[<h2 id="图像上的算术运算"><a href="#图像上的算术运算" class="headerlink" title="图像上的算术运算"></a>图像上的算术运算</h2><h3 id="图像融合"><a href="#图像融合" class="headerlink" title="图像融合"></a>图像融合</h3><ul>
<li><p><strong>图像融合即也是图像加法，但它是对每个图像乘以相应的权重，使其具有融合或透明的感觉。根据以下等式进行运算。</strong></p>
<script type="math/tex; mode=display">
G(x)=(1-\alpha)f_1(x)+\alpha f_2(x),\quad \alpha\in[0,1]</script></li>
<li><p><strong>f(x)可作为输入的图像，则上式可变为：</strong></p>
<script type="math/tex; mode=display">
dst=\beta\cdot img_1+\alpha\cdot img_2+\gamma\\
\beta+\alpha=1</script></li>
<li><p><strong>在OpenCV中进行图像融合的函数为cv.addWeighted(img1,$\beta$,img2,$\alpha,\gamma$),其有四个参数,img1与img2分别为输入的图像，$\beta,\alpha$分别为各自图像融合时所占的权重，一般情况下$\gamma$为0。</strong></p>
<p>以下为测试代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img1=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\save.png&#x27;</span>)</span><br><span class="line">img2=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\imori.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dst1=cv.addWeighted(img1,<span class="number">0.8</span>,img2,<span class="number">0.2</span>,<span class="number">0</span>)</span><br><span class="line">dst2=cv.addWeighted(img1,<span class="number">0.2</span>,img2,<span class="number">0.8</span>,<span class="number">0</span>)</span><br><span class="line">images=[dst1,dst2]</span><br><span class="line">titles=[<span class="string">&#x27;dst1&#x27;</span>,<span class="string">&#x27;dst2&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    images[i]=cv.cvtColor(images[i],cv.COLOR_BGR2RGB)</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(images[i],<span class="string">&#x27;jet&#x27;</span>)</span><br><span class="line">    plt.title(titles[i],fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>代码解读：</p>
</li>
<li><p>在这里要注意到，使用Matplotlib显示图片时，要先将图片转换为RGB形式，在OpenCV中，图像是以BGR通道存储的，直接显示会出现问题。</p>
</li>
<li><p>上述测试代码里的img1为img2的灰度图，可以发现当$\beta$取0.8，$\alpha$取0.2时，img1融合时占比较高，当$\beta$取0.2，$\alpha$取0.8时，二者的对比结果如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116131929.png" alt="4" style="zoom:33%;"></p>
<p>可以发现前者更加偏灰度，后者更加偏向原图。</p>
</li>
</ul>
<h3 id="按位运算"><a href="#按位运算" class="headerlink" title="按位运算"></a>按位运算</h3><ul>
<li><p>有时候，我们不想将图像相加或者融合，例如，下面这张图片：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132446.jpg" alt="8" style="zoom:50%;"></p>
<p>它就是由两张图片——（西电LOGO和人物图）进行“掩盖”合成的。</p>
</li>
<li><p><strong>按位操作分为以下四种：AND，OR，NOT，XOR，一一介绍它们的运算：</strong></p>
<p><strong>1. AND：即“与”运算，（1&amp;1=1，1&amp;0=0，0&amp;1=0，0&amp;0=0），其具体意义可理解为，只有两值都大于0，其为真。而在像素中，其取值在0~255之间，0为黑色，255为白色，而这里大于0的像素值都可以看作1。在两个都大于0的像素值之间进行AND运算，其结果取较小的像素值！</strong></p>
<p><strong>2. NOT：即“非”运算，（~1=0，~0=1），其具体意义可理解为，取像素的相反值，则当图像为二值图时，原图的黑色变为白色，白色变为黑色。</strong></p>
<p><strong>3. OR：即“与”运算，（1|1=1，1|0=1，0|1=1，0|0=0），其具体意义可理解为，只有当两值都为0，其为假。而对于两像素值，其先进行二进制转换，后对其进行运算，例如3|5：00000011|00000101=00000111，因此3|5=7。</strong></p>
<p><strong>4. XOR：即“异或”运算，（1^1=0，1^0=1，0^1=1，0^0=0），其具体意义可理解为，只有当两值不相同时，其为真。对于两两像素值，先进行二进制转换，后对其进行运算，例如3^5：00000011^00000101=00000110，因此3^5=6。</strong></p>
</li>
<li><p><strong>OpenCV中，进行按位运算的有以下四个函数：cv.bitwise_not，cv.bitwise_and，cv.bitwise_or，cv.bitwise_xor，这四个函数即对应上面的运算规则。下面将以下两张图片合成为上图：</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132002.jpg" alt="6" style="zoom: 33%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116130801.jpg" alt="7" style="zoom:25%;"></p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img1=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\xidian1.jpg&#x27;</span>)</span><br><span class="line">img2=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\girl.jpg&#x27;</span>)</span><br><span class="line">row,cols,channels=img1.shape</span><br><span class="line">roi=img2[<span class="number">0</span>:row,<span class="number">0</span>:cols]</span><br><span class="line"></span><br><span class="line">img1_gray=cv.cvtColor(img1,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret,dst1=cv.threshold(img1_gray,<span class="number">200</span>,<span class="number">255</span>,cv.THRESH_BINARY_INV)</span><br><span class="line">dst1_inv=cv.bitwise_not(dst1)</span><br><span class="line"></span><br><span class="line">img2_bg=cv.bitwise_and(roi,roi,mask=dst1_inv)</span><br><span class="line">img1_bg=cv.bitwise_and(img1,img1,mask=dst1)</span><br><span class="line"></span><br><span class="line">dst=cv.add(img1_bg,img2_bg)</span><br><span class="line">img2[<span class="number">0</span>:row,<span class="number">0</span>:cols]=dst</span><br><span class="line"></span><br><span class="line">cv.imshow(<span class="string">&#x27;images&#x27;</span>,img2)</span><br><span class="line"></span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>代码解读:</p>
<ul>
<li><p><strong>首先由shape得到img1（LOGO）图像的大小，即行数和列数。由此，可以在img2图像上，通过numpy直接划分出与img1图像一样大小的 roi图像（为原图的左上角）。</strong></p>
</li>
<li><p><strong>通过threshold函数可将img1图像二值化，而在这之前需要将img1图像转换为灰度图，才能将其传入。使用 THRESH_BINARY_INV方法将其二值化，二值化图像如下：</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132043.png" alt="8" style="zoom: 33%;"></p>
<p><strong>将得到的二值化图像（dst1）利用“非”运算，得到的图像如下：</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132101.png" alt="9" style="zoom:33%;"></p>
<p><strong>可以发现，进行“非”运算后，对二值图来说，其实即进行了颜色反转。</strong></p>
</li>
</ul>
<hr>
</li>
</ul>
<h2 id="阈值分割"><a href="#阈值分割" class="headerlink" title="阈值分割"></a>阈值分割</h2><h3 id="固定阈值分割"><a href="#固定阈值分割" class="headerlink" title="固定阈值分割"></a>固定阈值分割</h3><ul>
<li><p><strong>阈值分割简单来说，即大于阈值的变成一种值，小于阈值的为另一种值</strong>   </p>
</li>
<li><p>在python的cv2库中实现固定阈值分割的为<strong>cv2.threshold()函数</strong>。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\imori.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">ret, th1 = cv.threshold(img, <span class="number">127</span>, <span class="number">255</span>, cv.THRESH_BINARY)</span><br><span class="line">ret,th2=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY_INV)</span><br><span class="line">ret,th3=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO)</span><br><span class="line">ret,th4=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO_INV)</span><br><span class="line">ret,th5=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TRUNC)</span><br><span class="line"></span><br><span class="line">titles=[<span class="string">&#x27;Org&#x27;</span>,<span class="string">&#x27;Binary&#x27;</span>,<span class="string">&#x27;Binary_inv&#x27;</span>,<span class="string">&#x27;Tozero&#x27;</span>,<span class="string">&#x27;Tozero_inv&#x27;</span>,<span class="string">&#x27;Trunc&#x27;</span>]</span><br><span class="line">images=[img,th1,th2,th3,th4,th5]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(images[i],<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.title(titles[i],fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  运行结果如图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132138.png" alt="1" style="zoom: 50%;"></p>
<ul>
<li><p><strong>cv2.threshold()函数由4个参数组成，img为传入函数的图像，127为设置的阈值大小(threshold)，255为阈值设定方式里的阈值最大值(maxval)，THRESH_BINARY为阈值设定的方式。</strong>      </p>
</li>
<li><p><strong>ret代表当前的阈值。</strong></p>
</li>
<li><p><strong>matplotlib.pyplot中subplot(2,3,i+1)即为将窗口分为2行3列，i+1表示当前的第i+1个子图，imshow()则是对图像进行处理，它不会让图片进行显示，图像显示需要show()</strong></p>
</li>
<li><p><strong>阈值设定有5种方法，分别为:</strong></p>
<p><strong>1.THRESH_BINARY:</strong></p>
</li>
</ul>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
maxval &\ if\ src(x,y)>thresh\\
0 &\ otherwise
\end{cases}</script><p>当原像素值大于阈值，原像素值变为maxval，除此之外为0。</p>
<p>​        <strong>2.THRESH_BINARY_INV:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
0 &\ if \ src(x,y)>thresh\\
maxval &\ otherwise
\end{cases}</script><p>该阈值方法与上述阈值方法相反，从图像也可以观察得到。</p>
<p>​        <strong>3.THRESH_TOZERO:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
src(x,y) &\ if\ src(x,y)>thresh\\
0 &\ oherwise
\end{cases}</script><p>​        当原像素值大于阈值时，原像素值保持不变，除此之外，原像素值变为0。</p>
<p>​        <strong>4.THRESH_TOZERO_INV:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
0 &\ if \ src(x,y)>thresh\\
src(x,y) &\ otherwise
\end{cases}</script><p>​        该阈值方法与上述阈值方法相反。</p>
<p>​        <strong>5.THRESH_TRUNC:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
threshold &\ if \ src(x,y)>thresh\\
src(x,y) &\ otherwise
\end{cases}</script><p>​        放原像素值大于阈值，则原像素值变为阈值，除此之外，原像素值保持不变。</p>
]]></content>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch问题记录</title>
    <url>/2021/03/22/pytorch%20problem/</url>
    <content><![CDATA[<h1 id="PyTorch-中torch-nn-与torch-nn-function-的区别"><a href="#PyTorch-中torch-nn-与torch-nn-function-的区别" class="headerlink" title="PyTorch 中torch.nn. 与torch.nn.function.的区别"></a>PyTorch 中torch.nn. 与torch.nn.function.的区别</h1><p>今天我在定义一个卷积网络的过程中，发现nn.Conv2d与nn.functional.conv2d看似都是相同的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogCat_Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(Dog_Cat,self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size = <span class="number">7</span>,dilation = <span class="number">2</span>)</span><br><span class="line">        self.conv1 = nn.functional.conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size = <span class="number">7</span>,dilation = <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>类似的还有很多，在卷积，激活，池化等操作中，nn.Conv2d与nn.functional.conv2d均很相似。</p>
<p>通过查看PyTorch的文档发现，例如torch.nn.Conv2d是一个类</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210322202215.png" style="zoom: 50%;"></p>
<p>torch.nn.functional.conv2d则更像一个函数：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210322202712.png" style="zoom:50%;"></p>
<p>简而言之：</p>
<ul>
<li><strong>nn.Conv2d可以理解为一个Class类，需要继承nn.Module类</strong></li>
<li><strong>nn.functional.conv2d可以理解为一个纯函数，由def function(input)定义</strong></li>
</ul>
<p>对于nn中定义的类，可以提取变化的学习参数。而nn.functional中的是函数，是一个固定的运算公式。</p>
<p>由于在深度学习中会有很多权重是不断变化的，所以需要采用类的方式，以确保参数改变后，仍能正确进行运算。</p>
<p><strong>所以一般在建立网络模型时，在__init__函数中，通常使用nn中的类；而在forward函数中，通常使用nn.functional中的函数。</strong></p>
<p>例如以下网络模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog_Cat</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(Dog_Cat,self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size = <span class="number">7</span>,dilation = <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span> ,kernel_size = <span class="number">5</span>)</span><br><span class="line">        self.dropout = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">55696</span>,<span class="number">1000</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">1000</span>,<span class="number">50</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">50</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x),<span class="number">2</span>))</span><br><span class="line">        x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)),<span class="number">2</span>))</span><br><span class="line">        x = x.view(<span class="number">-1</span>,<span class="number">55696</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.dropout(x)</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = F.dropout(x)</span><br><span class="line">        x = F.log_softmax(self.fc3(x),dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>当网络模型使用了Dropout层时，其只需要在模型训练的时候进行使用，在模型测试时并不会使用它。</p>
<p><strong>当Dropout层是使用nn.Dropout定义时，在模型训练时，可以通过model.train()开启它，在模型测试时，可以通过model.eval()关闭它。</strong></p>
<p><strong>但当Dropout层是用nn.functional.dropout定义的，那么在使用model.eval()后，也没办法关闭Dropout层。</strong></p>
<p>以上提到了model.train()与model.eval()，那么它们的具体作用是什么呢？为什么要在模型训练与测试的时候加上这两段代码呢？</p>
<h3 id="model-train"><a href="#model-train" class="headerlink" title="model.train()"></a>model.train()</h3><p>如果模型中有BN层(Batch Normalization）和Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的<strong>均值</strong>和<strong>方差</strong>。对于Dropout，<strong>model.train()是随机取一部分网络连接来训练更新参数。</strong></p>
<h3 id="model-eval"><a href="#model-eval" class="headerlink" title="model.eval()"></a>model.eval()</h3><p>如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即<strong>不进行随机舍弃神经元。</strong></p>
<p>训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，<strong>框架会自动把BN和Dropout固定住，不会取平均，而是用训练好的值</strong>，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的的性质。</p>
<p>而在模型测试中，通常还会看见torch.no_grad()，它有什么作用呢？</p>
<ul>
<li>用于停止autograd模块的工作，起到加速和节省显存的作用（具体行为就是停止gradient计算，从而节省了GPU算力和显存）</li>
<li>不会影响dropout和batchnorm层的行为</li>
</ul>
<p>model.eval()与torch.no_grad()同时用，可以更加节省cpu算力，加速计算。</p>
]]></content>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux学习笔记(ubuntu)</title>
    <url>/2021/04/01/Linux/</url>
    <content><![CDATA[<h1 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h1><p>对于Linux来说，使用统一的目录树结构：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/bin        二进制文件，系统常规命令</span><br><span class="line">/boot       系统启动分区，系统启动时读取的文件</span><br><span class="line">/dev        设备文件</span><br><span class="line">/etc        大多数配置文件</span><br><span class="line">/home       普通用户的家目录</span><br><span class="line">/lib        32位函数库</span><br><span class="line">/lib64      64位库</span><br><span class="line">/media      手动临时挂载点</span><br><span class="line">/mnt        手动临时挂载点</span><br><span class="line">/opt        第三方软件安装位置</span><br><span class="line">/proc       进程信息及硬件信息</span><br><span class="line">/root       临时设备的默认挂载点</span><br><span class="line">/sbin       系统管理命令</span><br><span class="line">/srv        数据</span><br><span class="line">/var        数据</span><br><span class="line">/sys        内核相关信息</span><br><span class="line">/tmp        临时文件</span><br><span class="line">/usr        用户相关设定</span><br></pre></td></tr></table></figure>
<p>Linux下没有C：D：这种盘符的概念</p>
<p>在\home目录下存放着用户目录</p>
<p>超级用户root，其用户目录为/root</p>
<h1 id="Linux命令行"><a href="#Linux命令行" class="headerlink" title="Linux命令行"></a>Linux命令行</h1><h2 id="命令行含义"><a href="#命令行含义" class="headerlink" title="命令行含义"></a>命令行含义</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">示例：root@app00:~<span class="comment"># </span></span><br><span class="line">root    //用户名，root为超级用户</span><br><span class="line">@       //分隔符</span><br><span class="line">app00   //主机名称</span><br><span class="line">~       //当前所在目录，默认用户目录为~，会随着目录切换而变化，例如：（root@app00:/bin<span class="comment"># ，当前位置在bin目录下）</span></span><br><span class="line"><span class="comment">#       //表示当前用户是超级用户，普通用户为$，例如：（&quot;yao@app00:/root$&quot; ，表示使用用户&quot;yao&quot;访问/root文件夹）</span></span><br></pre></td></tr></table></figure>
<h2 id="命令行的组成"><a href="#命令行的组成" class="headerlink" title="命令行的组成"></a>命令行的组成</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">示例: 命令 参数名 参数值</span><br></pre></td></tr></table></figure>
<h2 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h2><h3 id="重启系统"><a href="#重启系统" class="headerlink" title="重启系统"></a>重启系统</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)立刻关机</span><br><span class="line">  shutdown -h now 或者 poweroff</span><br><span class="line">(2)两分钟后关机</span><br><span class="line">  shutdown -h 2</span><br></pre></td></tr></table></figure>
<h3 id="关闭系统"><a href="#关闭系统" class="headerlink" title="关闭系统"></a>关闭系统</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)立刻重启</span><br><span class="line">  shutdown -r now 或者 reboot</span><br><span class="line">(2)两分钟后重启</span><br><span class="line">  shutdown -r 2 </span><br></pre></td></tr></table></figure>
<h3 id="帮助命令"><a href="#帮助命令" class="headerlink" title="帮助命令"></a>帮助命令</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ifconfig --<span class="built_in">help</span>   //查看ifconfig命令的用法</span><br></pre></td></tr></table></figure>
<h3 id="命令说明书（man"><a href="#命令说明书（man" class="headerlink" title="命令说明书（man)"></a>命令说明书（man)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">man shutdown         //打开命令说明后，可按<span class="string">&quot;q&quot;</span>键退出</span><br></pre></td></tr></table></figure>
<h3 id="切换用户"><a href="#切换用户" class="headerlink" title="切换用户"></a>切换用户</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Lehrmann Lehrmanncc   //切换为用户<span class="string">&quot;yao&quot;</span>,输入后回车需要输入该用户的密码</span><br><span class="line"><span class="built_in">exit</span>                 //退出当前用户man shutdown         //打开命令说明后，可按<span class="string">&quot;q&quot;</span>键退出</span><br></pre></td></tr></table></figure>
<h2 id="目录操作"><a href="#目录操作" class="headerlink" title="目录操作"></a>目录操作</h2><h3 id="切换目录-cd"><a href="#切换目录-cd" class="headerlink" title="切换目录(cd)"></a>切换目录(cd)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /                 //切换到根目录</span><br><span class="line"><span class="built_in">cd</span> /bin              //切换到根目录下的bin目录</span><br><span class="line"><span class="built_in">cd</span> ../               //切换到上一级目录 或者使用命令：<span class="built_in">cd</span> ..</span><br><span class="line"><span class="built_in">cd</span> ~                 //切换到home目录</span><br><span class="line"><span class="built_in">cd</span> -                 //切换到上次访问的目录</span><br><span class="line"><span class="built_in">cd</span> xx(文件夹名)       //切换到本目录下的名为xx的文件目录，如果目录不存在报错</span><br><span class="line"><span class="built_in">cd</span> /xxx/xx/x         //可以输入完整的路径，直接切换到目标目录，输入过程中可以使用tab键快速补全</span><br></pre></td></tr></table></figure>
<h3 id="查看目录-ls"><a href="#查看目录-ls" class="headerlink" title="查看目录(ls)"></a>查看目录(ls)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls                   //查看当前目录下的所有目录和文件</span><br><span class="line">ls -a                //查看当前目录下的所有目录和文件（包括隐藏的文件）</span><br><span class="line">ls -l                //列表查看当前目录下的所有目录和文件（列表查看，显示更多信息），与命令<span class="string">&quot;ll&quot;</span>效果一样</span><br><span class="line">ls /bin              //查看指定目录下的所有目录和文件 </span><br></pre></td></tr></table></figure>
<h3 id="创建目录-mkdir"><a href="#创建目录-mkdir" class="headerlink" title="创建目录(mkdir)"></a>创建目录(mkdir)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir tools   			//在当前目录下创建一个名为tools的目录</span><br><span class="line">mkdir /bin/tools		//在指定目录下创建一个名为tools的目录</span><br><span class="line">mkdir -p /abc/123/<span class="built_in">test</span>   //使用-p参数，可以将路径的层次目录全部创建</span><br></pre></td></tr></table></figure>
<h3 id="删除目录与文件-rm"><a href="#删除目录与文件-rm" class="headerlink" title="删除目录与文件(rm)"></a>删除目录与文件(rm)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rm 文件名              //删除当前目录下的文件</span><br><span class="line">rm -f 文件名           //删除当前目录的的文件（不询问）</span><br><span class="line">rm -r 目录名         	//递归删除当前目录下此名的目录</span><br><span class="line">rm -rf 目录名        	//递归删除当前目录下此名的目录（不询问）</span><br><span class="line">rm -rf *              //将当前目录下的所有目录和文件全部删除</span><br><span class="line">rm -rf /*             //将根目录下的所有文件全部删除【慎用！相当于格式化系统】</span><br></pre></td></tr></table></figure>
<h3 id="修改目录-mv"><a href="#修改目录-mv" class="headerlink" title="修改目录(mv)"></a>修改目录(mv)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv 当前目录名 新目录名        //修改目录名，同样适用与文件操作</span><br><span class="line">mv /usr/tmp/tool /opt       //将/usr/tmp目录下的tool目录剪切到 /opt目录下面</span><br><span class="line">mv -r /usr/tmp/tool /opt    //递归剪切目录中所有文件和文件夹</span><br></pre></td></tr></table></figure>
<h3 id="拷贝目录-cp"><a href="#拷贝目录-cp" class="headerlink" title="拷贝目录(cp)"></a>拷贝目录(cp)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cp /usr/tmp/tool /opt       //将/usr/tmp目录下的tool目录复制到 /opt目录下面</span><br><span class="line">cp -rf /usr/tmp/tool /opt    //递归强制复制目录中所有文件和文件夹</span><br></pre></td></tr></table></figure>
<h3 id="搜索目录-find"><a href="#搜索目录-find" class="headerlink" title="搜索目录(find)"></a>搜索目录(find)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find /bin -name <span class="string">&#x27;a*&#x27;</span>  		//查找/bin目录下的所有以a开头的文件或者目录</span><br></pre></td></tr></table></figure>
<h3 id="查看当前目录（pwd）"><a href="#查看当前目录（pwd）" class="headerlink" title="查看当前目录（pwd）"></a>查看当前目录（pwd）</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">pwd</span>                         //显示当前位置路径</span><br></pre></td></tr></table></figure>
<h2 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h2><h3 id="新增文件-touch"><a href="#新增文件-touch" class="headerlink" title="新增文件(touch)"></a>新增文件(touch)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">touch example/test.txt    //在example目录下创建test.txt文件</span><br></pre></td></tr></table></figure>
<h3 id="文件权限"><a href="#文件权限" class="headerlink" title="文件权限"></a>文件权限</h3><h3 id="文件权限说明"><a href="#文件权限说明" class="headerlink" title="文件权限说明"></a>文件权限说明</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">文件权限简介：<span class="string">&#x27;ower&#x27;</span>代表文件的属主ower，<span class="string">&#x27;r&#x27;</span> 代表可读<span class="built_in">read</span>（4），<span class="string">&#x27;w&#x27;</span> 代表可写write（2），<span class="string">&#x27;x&#x27;</span> 代表执行权限excute（1），括号内代表<span class="string">&quot;8421法&quot;</span></span><br></pre></td></tr></table></figure>
<p>例如以下用ls-l命令查看example目录下的文件</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403110420.png" style="zoom:67%;"></p>
<p>第一段就是文件的权限，第二段为文件属主，第三段为属主所在用户组，第四段为文件大小，第五段为创建时间，第六段为创建文件名。</p>
<p>文件权限的含义如下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">文件权限信息示例：-rw-rw-r--</span><br><span class="line">-第一位：<span class="string">&#x27;-&#x27;</span>就代表是文件，<span class="string">&#x27;d&#x27;</span>代表是目录，<span class="string">&#x27;l&#x27;</span>代表软连接</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403111138.png" style="zoom:50%;"></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-第一组三位：拥有者的权限</span><br><span class="line">-第二组三位：拥有者所在的组，组员的权限</span><br><span class="line">-第三组三位：代表的是其他用户的权限</span><br></pre></td></tr></table></figure>
<p>于是-rw-rw-r—即表示：用户自己可以读写，但不能执行；同用户组的成员也可以读写，但无法执行；其他用户仅能读，但无法写、执行。</p>
<h3 id="修改文件权限"><a href="#修改文件权限" class="headerlink" title="修改文件权限"></a>修改文件权限</h3><p>文件的权限仅能由文件的属主或root用户进行修改。</p>
<p>chmod，change file mode 修改文件的访问权限</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">普通授权    chmod o+w a.txt     // o表示other，w表示增加write的权限</span><br><span class="line">		  chmod o-w a.txt	  // 表示other减少write权限</span><br><span class="line">		  chmod a-w a.txt 	  // a表示all，即所有人都不具有write权限</span><br><span class="line">		  chmod u-w a.txt  	  // u表示user，即用户自己减少wirte权限</span><br><span class="line">		  chmod +w a.txt  	  // 省略写法，默认为修改用户与本组的权限</span><br><span class="line">8421法     chmod 777 a.txt     //1+2+4=7，<span class="string">&quot;7&quot;</span>说明授予所有权限</span><br></pre></td></tr></table></figure>
<p>使用普通授权，可以发现train1.txt文件其他用户组已经增加了write的权限。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403112446.png" style="zoom:67%;"></p>
<h3 id="修改文件属主"><a href="#修改文件属主" class="headerlink" title="修改文件属主"></a>修改文件属主</h3><p>一般来说，每个用户仅操作自己的用户目录，所以修改文件属主并不常用。</p>
<p>使用chown命令，change owner修改文件的属主</p>
<p>只有文件的owner和root用户才有权限修改文件属主</p>
<h2 id="打包与解压"><a href="#打包与解压" class="headerlink" title="打包与解压"></a>打包与解压</h2><h3 id="压缩文件说明"><a href="#压缩文件说明" class="headerlink" title="压缩文件说明"></a>压缩文件说明</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">.zip、.rar        //windows系统中压缩文件的扩展名</span><br><span class="line">.tar              //Linux中打包文件的扩展名</span><br><span class="line">.gz               //Linux中压缩文件的扩展名</span><br><span class="line">.tar.gz           //Linux中打包并压缩文件的扩展名</span><br></pre></td></tr></table></figure>
<h3 id="打包文件"><a href="#打包文件" class="headerlink" title="打包文件"></a>打包文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -cvf 打包压缩后的文件名 要打包的文件</span><br><span class="line">参数说明： c：打包文件; v：显示运行过程; f：指定文件名;</span><br><span class="line">示例：</span><br><span class="line">tar -cvf a.tar file1 file2,...      //多个文件打包</span><br><span class="line">tar -zcvf a.tar.gz file1            //文件打包并压缩</span><br></pre></td></tr></table></figure>
<h3 id="解压文件"><a href="#解压文件" class="headerlink" title="解压文件"></a>解压文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf a.tar                      //解包至当前目录</span><br><span class="line">tar -zxvf a.tar -C /usr------        //‘C’表示切换目录，指定解压的位置</span><br><span class="line">unzip test.zip             //解压*.zip文件 </span><br><span class="line">unzip -l test.zip          //查看*.zip文件的内容 </span><br></pre></td></tr></table></figure>
<h2 id="关机与重启"><a href="#关机与重启" class="headerlink" title="关机与重启"></a>关机与重启</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">shutdown -h now   //参数h为halt，即立即关机</span><br><span class="line">shutdown -r now   //参数r为reboot，即为立即重启</span><br><span class="line">shutdown -h 20:30 //即8点半关机</span><br></pre></td></tr></table></figure>
<p><strong>但如果是远程管理服务器的话，应当注意不能将服务器关机，之能进行重启</strong></p>
<h2 id="软连接"><a href="#软连接" class="headerlink" title="软连接"></a>软连接</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用ln命令来创建软连接</span></span><br><span class="line">ln -s example example3    // 其中-s表示soft软连接（默认为硬）</span><br><span class="line"><span class="comment"># example为原始目录或文件名，example2为软连接名</span></span><br></pre></td></tr></table></figure>
<ol>
<li>删除软连接，对原目录没有任何影响</li>
<li>删除原目录，则软连接失效</li>
</ol>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210402165356.png" style="zoom:67%;"></p>
<p>可以发现根目录下有较多的软连接，例如bin、lib等，bin其实指向目录user/bin。</p>
<h2 id="用户操作"><a href="#用户操作" class="headerlink" title="用户操作"></a>用户操作</h2><h3 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo useradd -m test1</span><br></pre></td></tr></table></figure>
<p>其中，sudo（super use)表示以管理员身份运行；-m参数表示在/home目录下添加用户目录。</p>
<h3 id="添加密码"><a href="#添加密码" class="headerlink" title="添加密码"></a>添加密码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo passwd test1</span><br></pre></td></tr></table></figure>
<h3 id="删除用户"><a href="#删除用户" class="headerlink" title="删除用户"></a>删除用户</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo userdel test1</span><br></pre></td></tr></table></figure>
<p>但这个时候虽然已经将test1用户删除了，但在\home目录下仍有\test1用户目录，所以需要将其目录删除</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo rm -rf /home/test1/</span><br></pre></td></tr></table></figure>
<ol>
<li>在登陆系统时，Linux默认不能使用root用户进行登陆</li>
<li>只有特殊的用户才能执行sudo，例如刚才创建的新用户test1就不能使用sudo。而用户lehrmann便可以。在Linux下，把能执行sudo命令的用户叫sudoer。</li>
</ol>
<h2 id="root用户"><a href="#root用户" class="headerlink" title="root用户"></a>root用户</h2><p>首次使用root用户，需要给root用户设置密码(只需要设置一次，需要妥善保管root用户密码)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo passwd root</span><br></pre></td></tr></table></figure>
<p>切换root用户，使用su（switch user)命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">su root</span><br></pre></td></tr></table></figure>
<p>su root仅会切换终端(terminal)用户为root用户，桌面环境并不会切换</p>
<p>退出root用户，</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
<h1 id="可执行脚本"><a href="#可执行脚本" class="headerlink" title="可执行脚本"></a>可执行脚本</h1><p>脚本script，一种解释执行的程序</p>
<p>Linux下有常见的三种脚本程序</p>
<ol>
<li>Shell脚本    *.sh</li>
<li>Perl脚本      *.pl</li>
<li>Python脚本 *.py</li>
</ol>
<p>脚本程序都是由脚本解释器来运行的</p>
<p>例如：Python脚本解释器：/bin/python3</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403134944.png" style="zoom:67%;"></p>
<p>执行一个脚本时，可以采用以下命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/bin/python3 hello.py</span><br></pre></td></tr></table></figure>
<p>也可以采用以下方式，首先创建python脚本后，在第一行添加python脚本解释器路径</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#！/bin/python3</span></span><br></pre></td></tr></table></figure>
<p>但这个时候通过以下命令运行hello.py仍会出错</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403143205.png" style="zoom:67%;"></p>
<p>为什么会出现这种情况呢？提示权限不够，于是我用以下命令查看hello.py</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403143600.png" style="zoom:67%;"></p>
<p>会发现hello.py在用户的权限中仅能读写，不能执行，于是我们给用户增加权限</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">chmod u+x hello.py</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403143916.png" style="zoom:67%;"></p>
<p>现在hello.py便可用过 ./hello.py 成功运行。</p>
<h1 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h1><h2 id="用户环境变量"><a href="#用户环境变量" class="headerlink" title="用户环境变量"></a>用户环境变量</h2><p>用户环境变量，定义在 ~/.profile中，在Linux中，以 . 开头的文件为隐藏文件，可以通过以下命令查看</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls -a </span><br></pre></td></tr></table></figure>
<p>采用gedit可以对.profile文件进行编辑</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">gedit .profile</span><br></pre></td></tr></table></figure>
<p>用户环境变量仅对当前用户有效，若切换用户后，则之前编辑的用户环境变量无效，因为每个用户都有自己的 .profile文件。</p>
<h2 id="系统环境变量"><a href="#系统环境变量" class="headerlink" title="系统环境变量"></a>系统环境变量</h2><p>系统环境变量定义在：/etc/profile 中，这里面的环境变量对所有的用户都有效。</p>
<p><strong>修改系统变量需要root用户权限，因为/etc下的东西都是需要超级用户才能进行修改。但官方建议不要直接修改/etc/profile文件，一般建议在/etc/profile.d/ 目录下创建一个自定义脚本。因为在profile中对profile.d目录下的文件进行嵌套调用。</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403152245.png" style="zoom:67%;"></p>
<h2 id="PATH环境变量"><a href="#PATH环境变量" class="headerlink" title="PATH环境变量"></a>PATH环境变量</h2><p>在运行python脚本时，若直接输入脚本名称进行运行，我们发现会出现如下情况</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403161159.png" style="zoom:67%;"></p>
<p>为什么会找不到命令呢？这就和PATH环境变量有关系。</p>
<p>PATH是一个最常见的环境变量，用于描述可执行文件的搜索路径。</p>
<p>使用echo命令将PATH环境变量的值显示出来</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403161550.png" style="zoom:67%;"></p>
<p>可以发现PATH环境变量的值就是一堆路径，然后用  ：进行分隔开。</p>
<p>默认系统从以下目录搜索可执行程序</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/bin            //例如tar命令</span><br><span class="line">/usr/sbin		   //例如useradd命令</span><br><span class="line">/usr/<span class="built_in">local</span>/bin</span><br><span class="line">/usr/<span class="built_in">local</span>/sbin</span><br></pre></td></tr></table></figure>
<ol>
<li>bin表示普通用户就能执行的程序，其中sbin为超级用户root才能执行的程序</li>
<li>/usr/为系统自带的程序</li>
<li>/usr/local/是用户安装的程序</li>
</ol>
<p>由于在输入hello.py后，系统会从PATH的路径下搜索命令，而搜索完后会发现没有hello.py这个命令，所以会返回未找到命令。</p>
<p>所以我们可以在用户环境变量或系统环境变量里，修改PATH的值，然后注销便可生效。</p>
<p>例如在系统环境变量中</p>
<ol>
<li><p>编辑/etc/profile.d/myprofile.sh，myprofile.sh为自定义的shell脚本</p>
</li>
<li><p>设定PATH环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export PATH=$PATH:/opt/tomcat/bin</span><br></pre></td></tr></table></figure>
<p>为PATH环境变量新增路径  /opt/tomcat/bin。</p>
</li>
</ol>
<h1 id="SSH服务器"><a href="#SSH服务器" class="headerlink" title="SSH服务器"></a>SSH服务器</h1><p>使用SSH协议，可以实现：</p>
<ol>
<li>远程操控Linux终端（Xshell实现）</li>
<li>进行文件传输至Linux服务器（Xftp实现）</li>
</ol>
<h2 id="虚拟机准备工作"><a href="#虚拟机准备工作" class="headerlink" title="虚拟机准备工作"></a>虚拟机准备工作</h2><p>首先需要保证ubuntu防火墙处于关闭状态，查看防火墙状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ufw status</span><br></pre></td></tr></table></figure>
<p>接着安装ssh</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get installl openssh-server</span><br></pre></td></tr></table></figure>
<p>进入查看ssh，若安装成功，则如下所示</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403191811.png" style="zoom:67%;"></p>
<p>接着查看虚拟机的ip地址</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ifconfig</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403191941.png" style="zoom:67%;"></p>
<p>其中，ens33中inet即为ip地址：192.168.233.128。</p>
<p>最后在本地打开cmd，如过可以ping通虚拟机ip地址，则说明可以连接了</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403192436.jpg" style="zoom:67%;"></p>
<p>如果没有ping通，则可以看一下ubuntu的防火墙有没有关闭。</p>
<h2 id="Xshell连接"><a href="#Xshell连接" class="headerlink" title="Xshell连接"></a>Xshell连接</h2><p>点击【文件】-&gt;【新建】打开新建会话属性弹框，输入虚拟机的主机ip</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403192713.png" style="zoom:67%;"></p>
<p>切换选项卡【用户身份验证】。输入用户名，密码</p>
<p>注意用户名可以在命令行中查看</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">who</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403192916.png" style="zoom:67%;"></p>
<p>lehrmann : 0中，lehrmann即为用户名</p>
<p>设置完属性 点击【确定】按钮就不出意外可以连接上了</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403193313.png" style="zoom:67%;"></p>
<p>上图即连接成功后的，至此Xshell即可远程对虚拟机进行操作了。</p>
<h2 id="Xftp传输文件"><a href="#Xftp传输文件" class="headerlink" title="Xftp传输文件"></a>Xftp传输文件</h2><p>安装Xftp后，打开Xshell，连接虚拟机后，如图所示</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403231817.png" style="zoom:67%;"></p>
<p>右键点击localhost，如图</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403231935.png" style="zoom:67%;"></p>
<p>点击即可打开Xftp，如图</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403232116.png" style="zoom:67%;"></p>
<p>即可进行文件传输</p>
<h1 id="vim编辑器"><a href="#vim编辑器" class="headerlink" title="vim编辑器"></a>vim编辑器</h1><p>vi / vim 是一个基于控制台的文本编辑器，vim为vi的升级版。</p>
<p>gedit是一个基于GUI的文本编辑器</p>
<p>由于以后可能没有图形界面，只有一个控制台终端，故无法使用gedit，之能只用vim</p>
<h2 id="打开文本编辑"><a href="#打开文本编辑" class="headerlink" title="打开文本编辑"></a>打开文本编辑</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim test.txt</span><br></pre></td></tr></table></figure>
<p>如果目标文件存在，则打开编辑器；如果不存才会创建一个文件</p>
<p>如果系统上没有vim编辑器，则可以进行安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install vim</span><br></pre></td></tr></table></figure>
<h2 id="切换模式"><a href="#切换模式" class="headerlink" title="切换模式"></a>切换模式</h2><ul>
<li>编辑模式 Insert Mode：按i键</li>
<li>命令模式 Command Mode：按Esc键</li>
</ul>
<h2 id="退出编辑"><a href="#退出编辑" class="headerlink" title="退出编辑"></a>退出编辑</h2><ol>
<li><p>按Esc键，进入命令模式</p>
</li>
<li><p>输入 :wq  保存并退出</p>
<p>输入 :q     退出</p>
<p>输入 :q!    强制退出（放弃保存）</p>
</li>
</ol>
<p>例如</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim test.txt</span><br></pre></td></tr></table></figure>
<p>进入以下页面</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403220728.png" style="zoom:67%;"></p>
<p>输入响应的文本后，可按Esc键，接着输入:wq，按回车即可退出</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat test.txt</span><br></pre></td></tr></table></figure>
<p>即可得到test.txt的文本内容</p>
<h1 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h1><p>每运行一个可执行程序，其相应的在系统中都会有相应的进程</p>
<p>在Linux系统中，可以使用如下命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ps -ef</span><br></pre></td></tr></table></figure>
<p>便可得到如下的进程信息</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210403225647.png" style="zoom:67%;"></p>
<p>其中，各字段的含义：</p>
<ol>
<li>UID：执行者</li>
<li>PID：进程ID</li>
<li>PPID：父进程ID</li>
<li>STIME：启动时间</li>
<li>CMD：启动时调用的命令行</li>
</ol>
<p>如果要查特定的进程时，可以使用如下命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ps -ef | grep vi</span><br></pre></td></tr></table></figure>
<p>将前者输出的信息，重定向给grep命令进行过滤处理</p>
<h2 id="进程监视"><a href="#进程监视" class="headerlink" title="进程监视"></a>进程监视</h2><p>查看所有进程，并且可以动态的刷新显示进程的状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">top</span><br></pre></td></tr></table></figure>
<ol>
<li>按上下键翻阅</li>
<li>按 q 或CTRL + C中止退出</li>
</ol>
<p>可以查看特定进程，</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">top -p NUM</span><br></pre></td></tr></table></figure>
<p>其中NUM为特定进程的PID，所以我们可以先使用 ps 命令查看进程PID。</p>
<h2 id="杀死进程"><a href="#杀死进程" class="headerlink" title="杀死进程"></a>杀死进程</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 NUM</span><br></pre></td></tr></table></figure>
<h2 id="前台进程与后台进程"><a href="#前台进程与后台进程" class="headerlink" title="前台进程与后台进程"></a>前台进程与后台进程</h2><p>前台进程：运行在前台</p>
<ul>
<li>有父进程，父进程为当前终端</li>
<li>当终端关闭时，前台进程被一同关闭</li>
</ul>
<p>后台进程：运行在后台</p>
<ul>
<li>父进程为系统进程（1号进程）</li>
<li>当终端关闭时，后台进程不被影响</li>
</ul>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine learning</title>
    <url>/2020/10/23/ML/</url>
    <content><![CDATA[<h1 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a><strong>Regression</strong></h1><h2 id="Linear-regression-线性回归"><a href="#Linear-regression-线性回归" class="headerlink" title="Linear regression(线性回归)"></a>Linear regression(线性回归)</h2><h3 id="问题的导入：预测神奇宝贝进化后的战斗力（CP）值"><a href="#问题的导入：预测神奇宝贝进化后的战斗力（CP）值" class="headerlink" title="问题的导入：预测神奇宝贝进化后的战斗力（CP）值"></a>问题的导入：预测神奇宝贝进化后的战斗力（CP）值</h3><ul>
<li><p><strong>输入：进化前神奇宝贝A的CP值，种类，血量（HP），重量（Weight)，高度（Height）</strong></p>
</li>
<li><p><strong>输出：进化后神奇宝贝A的CP值</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233907.png" alt="1" style="zoom:67%;"></p>
</li>
</ul>
<h3 id="机器学习的主要步骤："><a href="#机器学习的主要步骤：" class="headerlink" title="机器学习的主要步骤："></a>机器学习的主要步骤：</h3><p><strong>step 1：Model（确定模型）</strong></p>
<p><strong>step 2：Goodless of function（确定损失函数）</strong></p>
<p><strong>step 3：Best function（找到最好的函数）</strong></p>
<h3 id="S-Model"><a href="#S-Model" class="headerlink" title="$\S $Model"></a>$\S $Model</h3><p>在这里，根据常理推测，神奇宝贝进化后的CP值和进化前的CP值有较大的关系，故我们可以假设进化前的CP值与进化后的CP值呈线性关系。</p>
<p>设神奇宝贝进化前的CP值为$x_{cp}$，进化后的CP值为$y$，则可建立以下线性关系：</p>
<script type="math/tex; mode=display">
y=b+wx_{cp} \tag 1</script><p>在这里我们仅考虑了神奇宝贝进化前的CP值这一特征，如果神奇宝贝影响进化后的CP值的不止这一特征，若有n个特征，那么我们可将$x_{cp}$推广到$x_i$，于是便可得到<strong>Linear model</strong>（线性模型）：</p>
<script type="math/tex; mode=display">
\begin{cases}
y=b+\sum_i^nw_ix_i\\
x_i:feature\ (特征)\\
w_i:weight \ (权重)\\
b:bias \ (偏差) \\
\end{cases} \tag 2</script><p>在这个模型里，一般而言，我们会有<strong>training data</strong>，假设现在我们有10组训练数据，即：</p>
<script type="math/tex; mode=display">
(x^1,\widehat{y}^1)\\
(x^2,\widehat{y}^2)\\
\vdots\\
(x^{10},\widehat{y}^{10}) \tag 3</script><p>对于以上数据来说，它们是已知的，其上标代表了它们的序号，而现在，我们可以将以上数据带入简化模型（式1）中，可得：</p>
<script type="math/tex; mode=display">
\widehat{y}^1=b+wx_{cp}^1\\
\widehat{y}^2=b+wx_{cp}^2\\
\vdots\\
\widehat{y}^{10}=b+wx_{cp}^{10}\\  \tag 4</script><p>对于式4，即就是我们训练数据带入模型后所得，而其只有两个未知数，即$b,w$，而现在我们需要做的就是<strong>找到最适合的$b,w$参数值</strong>。现在就需要损失函数发挥作用了。</p>
<h3 id="S-Goodless-of-function"><a href="#S-Goodless-of-function" class="headerlink" title="$\S $Goodless of  function"></a>$\S $Goodless of  function</h3><ul>
<li><p><strong>损失函数的作用</strong></p>
<script type="math/tex; mode=display">
Loss\ function\ L:
\begin{cases}
input:&\ a\ function\\
output:&\ how\ bad\ it\ is
\end{cases}</script><p>这里要注意评价函数的输入，即为函数$f$，而其输出是它的好坏，但由上式可知，式（4）其<strong>未知量只有待求参数$b,w$，</strong>那么函数评价函数$L(f)$即为：</p>
<script type="math/tex; mode=display">
L(f)=L(w,b) \tag 5</script><p>那么现在，<strong>对于输出所评价函数的好坏即可转化为评价参数$w,b$的好坏。</strong></p>
</li>
<li><p><strong>常见的损失函数</strong></p>
<p>现在我们已有真实数据，那么如何反映其参数的好坏呢，我们可以<strong>通过真实的CP值与预测CP值的差进行衡量</strong>，如下所示：</p>
<script type="math/tex; mode=display">
L(f)=\sum_{i=1}^{10}(\widehat{y}^i-f(x_{cp}^i))^2\Longrightarrow\ L(w,b)=\sum_{i=1}^{10}(\widehat{y}^i-(b+w\cdot x_{cp}^i))^2 \tag 6</script><p>当真实值与误差值较小时，那么此时的$w,b$可认为是最好的，则我们的目标即为：</p>
<script type="math/tex; mode=display">
min\quad L(w,b)</script><p>于是我们现在就需要对其进行求解。</p>
</li>
</ul>
<h3 id="S-Best-function-——-Gradient-Descent（梯度下降法）"><a href="#S-Best-function-——-Gradient-Descent（梯度下降法）" class="headerlink" title="$\S $Best  function ——-Gradient  Descent（梯度下降法）"></a>$\S $Best  function ——-Gradient  Descent（梯度下降法）</h3><p>我们需要通过$Loss\ Function\Longrightarrow”the \ best\ function”$，即达到我们的目标，令：</p>
<script type="math/tex; mode=display">
w*,b*=arg\ \ min_{w,b}\ \ L(w,b)=arg\ \ min_{w,b}\ \ \sum_{i=1}^{10}(\widehat{y}^i-(b+w\cdot x_{cp}^i))^2 \tag 7</script><p>对于以上，我们可以采用穷举法，求得$L(w,b)$的最小值，但下面介绍更加高效的算法。</p>
<ul>
<li><p>梯度下降法</p>
<p>若现在损失函数$L(w,b)$只考虑其$w$单变量情况，有以下函数图像：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233452.png" alt="2" style="zoom:67%;"></p>
<p>当我们现在<strong>随机</strong>在函数上选择一点$w^0$，</p>
<p>1.如果函数在该点的斜率为负，即导数$\frac{\mathrm{d}L}{\mathrm{d}w}&lt;0$，那么此时函数应该为<strong>递减的</strong>，于是可以考虑增大$w^0$。</p>
<p>2.如果函数在该点的斜率为正，即导数$\frac{\mathrm{d}L}{\mathrm{d}w}&gt;0$，那么此时函数应该为<strong>递增的</strong>，于是可以考虑减少$w^0$。</p>
<p>增大多少或减少多少？</p>
<p>定义每次移动的步长为：</p>
<script type="math/tex; mode=display">
step=\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0 \tag 8</script><p>在式（8）中，$\eta$称为”learning rate（学习率）”，<strong>我们可以通过控制$\eta$的大小，从而增加step，以此可以提高求解效率，但若$\eta$过大，则会导致步长过大，那么有可能会错过最优解，故我们需要选取合适的$\eta$。而$\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0$，其微分值越大$\rightarrow$曲线越陡峭$\rightarrow$移动步长越大$\rightarrow$提高求解效率。</strong></p>
<p>以上图为例，其算法如下：</p>
<script type="math/tex; mode=display">
w^0-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0\Longrightarrow w^1\\
w^1-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^1\Longrightarrow w^2\\
\vdots \ \ 更新循环\\
w^{t-1}-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{t-1}\Longrightarrow w^t</script><p>在上式中，因为$\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{0}&lt;0$，故需给其添加负号。对点不断更新循环，直到$(\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{t})=0$停止，此时$w=w^t$即所求参数的最优解。</p>
<p><strong>但从上图我们可以发现$w^t$其实并不为global optimal solution(全局最优解)，其为local optimal solution(局部最优解)，不过由于我们研究的为线性回归，所以不存在局部最优解。故在线性回归中，梯度下降法是有效的。</strong></p>
<p>现在，我们可以将单变量$w$推广到多变量$w,b$，函数如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233543.png" alt="3" style="zoom:67%;"></p>
<p>随机选取点$(w^0,b^0)$，计算函数在该点的偏导$\frac{\partial L}{\partial w}|w=w^0,b=b^0,\frac{\partial L}{\partial b}|w=w^0,b=b^0$。</p>
<p>那么函数在该点的梯度即为：</p>
<script type="math/tex; mode=display">
grad\ L(w^0,b^0)=\nabla L(w^0,b^0)=(\frac{\partial L}{\partial w}|w^0,b^0)\overrightarrow{i}+(\frac{\partial L}{\partial b}|w^0,b^0)\overrightarrow{j}\tag 9</script><p>其中$\nabla$为向量微分算子。</p>
<p><strong>而对于梯度向量，其方向为曲线在等值线上的法线方向（高数中有对其的证明），其通过不断的增加步长以更新点，从而达到最优解，如上图所示。</strong></p>
<p>与单变量相同，多变量算法与其相同：</p>
<script type="math/tex; mode=display">
\begin{cases}
w^0-\eta\frac{\partial L}{\partial w}|w=w^0,b=b^0\Longrightarrow w^1\\
b^0-\eta\frac{\partial L}{\partial b}|w=w^0,b=b^0\Longrightarrow b^1
\end{cases}\\
\begin{cases}
w^1-\eta\frac{\partial L}{\partial w}|w=w^1,b=b^1\Longrightarrow w^2\\
b^1-\eta\frac{\partial L}{\partial b}|w=w^1,b=b^1\Longrightarrow b^2
\end{cases}\\
\vdots\ \ 更新循环\\
\begin{cases}
w^t-\eta\frac{\partial L}{\partial w}|w=w^t,b=b^t\Longrightarrow w^t\\
b^t-\eta\frac{\partial L}{\partial b}|w=w^t,b=b^t\Longrightarrow b^t
\end{cases}\\</script><p>最后直到$\nabla L(w^t,b^t)=0$停止，此时<strong>$w^t,b^t$即为所求得的最优解，由于在线性回归中，所以不同担心会由于初始值的选取，而影响达到全局最优解。</strong></p>
</li>
</ul>
<h3 id="S-Result"><a href="#S-Result" class="headerlink" title="$\S $Result"></a>$\S $Result</h3><p>如果现在通过十组训练数据，得到的结果如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233626.png" alt="4" style="zoom:67%;"></p>
<p>如图所示，其通过梯度下降法，所求得的最优参数$b,w$分别为-188.4，2.7，其中，损失值可用$\frac{1}{10}\sum_{n=1}^{10}\mathrm{e}^n$表示，<strong>$e^n$即为第n组真实值与预测值的差值。</strong></p>
<p>现在将model改用不同函数，观察其结果变化情况，如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233707.png" alt="5" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233746.png" alt="6" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233820.png" alt="7" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233844.png" alt="8" style="zoom:67%;"></p>
<p>可以发现，<strong>随着Model改用函数的阶数的升高，训练集损失值越来越小，但可以发现测试集损失值却并不是越来越少，相反，当阶数大于4后，其测试集损失值增大较多，而这种现象叫做”Overfitting”(过拟合)。</strong></p>
<p>那么我们如何才能防止过拟合的发生呢？</p>
<ul>
<li><p><strong>Regularization(正则化)</strong></p>
<p><strong>由于过拟合是model过度拟合其在训练集的数据，则其会造成曲线波动较大，而如果能让曲线变得smooth平滑，便可防止过拟合。</strong></p>
<p><strong>于是考虑，增加一个与参数$w$（斜率）相关的值，如果loss function 越小的话，那么对应这个值也会越小，即$w$也会越小，于是便会使函数更加平滑。</strong></p>
<p>于是可在线性回归模型中，将loss function变为：</p>
<script type="math/tex; mode=display">
L=\sum_{i=1}^{10}(\widehat{y}^i-(b+\sum_jw_jx_j))^2+\lambda\sum_j(w_j)^2 \tag{10}</script><p>而在式（10）中，$\lambda\sum_j(w_j)^2$称为正则化项，$\lambda$称为正则化参数，它的作用就是平衡loss function这两项，若$\lambda$取得非常大，即对参数$w_j$惩罚的非常大，那么相当于$w_j$趋于0，即相当于原线性回归模型中删除了这些项，那么函数就变成了一条$y=b$的水平直线。</p>
<p>那么为什么减小$w_j$能够使其函数不宜发生overfitting呢？</p>
<p>若现在令$\varDelta x_j$为输入时的干扰项，那么输出所形成的误差即为：</p>
<script type="math/tex; mode=display">
y=b+\sum_jw_j(x_j+\varDelta x_j) \Longrightarrow \varDelta y=w_j\varDelta x_j</script><p>由此我们可以发现，当$w_j$较小时，产生的误差$\varDelta y$也更小，则更加光滑的函数会受更少的影响。</p>
<p>但观察正则化项，我们会发现为什么没有$b$呢？</p>
<p><strong>其实，正则化调整的是函数的平滑程度，但我们调整b对函数的平滑程度不会造成影响，它只能让函数上下移动。</strong></p>
</li>
</ul>
<h1 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a><strong>Classification</strong></h1><p>什么是分类呢?例如在现实生活中，当我们看见一张关于猫与狗的照片时，我们可以很容易区分，照片里的属于猫还是狗，那么如果现在有一些不同类别的大量数据，显然由人进行分类是不现实的，那么我们希望可以借助计算机来帮我们进行数据的分类……</p>
<p>那么我们如何解决分类问题呢？例如下图：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233933.png" alt="9" style="zoom:67%;"></p>
<p>希望能<strong>建立一个函数，输入一个神奇宝贝，就能返回其属性（类别）。</strong></p>
<p>首先我们需要将神奇宝贝数字化，即通过它的特征（Hp，Attack，Defense……）来描述这只神奇宝贝。首先将分类问题简化，只考虑其只有两类。</p>
<h2 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h2><p>我们的目标仍是寻找一个函数，输入x输出其类别，即为：</p>
<ul>
<li><p>Function (Model):</p>
<script type="math/tex; mode=display">
x \rightarrow \ f(x)=\begin{cases}
g(x)>0 \quad &output=class\ 1\\
else &output=class\ 2
\end{cases}</script><p>输入x，则当函数值&gt;0，其属于class 1，否则其属于class 2。而这个函数我们可以利用概率模型进行描述，即<script type="math/tex">P(C_1|x)>0.5</script></p>
</li>
<li><p>Loss Function:</p>
<p>Loss Function的目的是用来评价函数的好坏，那么我们想用模型分类的结果，与正确的模型结果进行对比，统计错误的次数。</p>
<script type="math/tex; mode=display">
L(f)=\sum_n\delta(f(x^n)\neq\widehat{y}^n)</script></li>
<li><p>Find the best function:</p>
<p>Example:perception(感知机算法)、svm(支持向量机)</p>
</li>
</ul>
<h2 id="贝叶斯概率模型"><a href="#贝叶斯概率模型" class="headerlink" title="贝叶斯概率模型"></a>贝叶斯概率模型</h2><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233955.png" alt="10" style="zoom:67%;"></p>
<p>在Box 1中我们可以计算得到抽出蓝色球的概率为<script type="math/tex">\frac{4}{5}</script>，绿色球的概率为<script type="math/tex">\frac{1}{5}</script>，若已知我们抽取Box 1的概率为<script type="math/tex">\frac{2}{3}</script>，那么我们就可以利用贝叶斯公式计算得到，已知抽取的小球为蓝色，则从Box 1抽出的机率为多少，即<script type="math/tex">P(B_1|Blue)</script>。</p>
<p>而现在我们可以将这些球看作是一个个数据，从而可以利用贝叶斯公式生成概率模型。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234009.png" alt="11" style="zoom:67%;"></p>
<ul>
<li>先验概率：<script type="math/tex">P(C)</script>，即每种类别发生的概率。</li>
<li>类条件概率：<script type="math/tex">P(x|C)</script>，在某种类别的条件下，某事发生的概率。</li>
<li>后验概率：<script type="math/tex">P(C|x)</script>，某事发生了，它属于某种类别的概率。</li>
</ul>
<p>如果我们可以计算得到<script type="math/tex">P(C_1)</script>，<script type="math/tex">P(C_2)</script>，与<script type="math/tex">P(x|C_1)</script>，<script type="math/tex">P(x|C_2)</script>，那么我们可以通过贝叶斯公式计算得到<script type="math/tex">P(C_1|x)</script>，<script type="math/tex">P(C_2|x)</script>，如果<script type="math/tex">P(C_1|x)>0.5</script>，那么可以认为<script type="math/tex">x</script>是属于Class 1 ，相反可认为<script type="math/tex">x</script>是属于Class 2的。</p>
<p>那么我们的目标就是计算<script type="math/tex">P(C_1)</script>，<script type="math/tex">P(C_2)</script>，与<script type="math/tex">P(x|C_1)</script>，<script type="math/tex">P(x|C_2)</script>。</p>
<ul>
<li><p><strong>先验概率的计算</strong></p>
<p>假设现在两个类别，Class 1：Water(水系)，Class 2：Normal(其他系)。</p>
<p><strong>水系有79只神奇宝贝，其他系有61只神奇宝贝，总样本共有140只神奇宝贝。</strong></p>
<script type="math/tex; mode=display">
\implies P(C_1)=\frac{79}{140}=0.56,\ P(C_2)=\frac{61}{140}=0.44</script><p>即通过所得样本中各自种类的占比得到先验概率，其计算较为简单。</p>
</li>
<li><p><strong>类条件概率的计算</strong></p>
<p>假设水系精灵中有6杰尼龟(水系)，那么我们可以认为在水系精灵中，杰尼龟的概率是<script type="math/tex">P(x|C_1)=\frac{6}{79}=0.076</script>吗？</p>
<p>事实上，这是不对的，<strong>因为该水系精灵样本较少，所以我们无法直接仅由样本中所占比例来推得类条件概率。假设该水系样本精灵中没有水箭龟，但水箭鬼确实是水系精灵，那么我们能说明水箭龟在水系精灵中的概率为0吗？这显然是有问题的。</strong></p>
<p>那么我们如何计算<script type="math/tex">P(x|C)</script>呢？我们利用<strong>极大似然估计法</strong>。</p>
</li>
</ul>
<h3 id="S-极大似然估计"><a href="#S-极大似然估计" class="headerlink" title="$\S$ 极大似然估计"></a>$\S$ 极大似然估计</h3><p>对于<script type="math/tex">P(x|C)</script>的计算，我们很难去寻找到随机变量<script type="math/tex">x</script>所遵循的概率密度函数。<strong>那么如果我们假设已知随机变量<script type="math/tex">x</script>所遵循的概率密度函数，转而计算所假设概率密度函数中的参数值，如果找到最佳的参数值，那么我们就能用该概率密度函数代替类条件概率的密度函数了。</strong></p>
<p>若有样本集<script type="math/tex">X=\{x^1,x^2,x^3,...,x^n\}</script>，每个样本集中的样本都是所谓独立同分布的随机变量。</p>
<p><strong>似然函数定义为：似然函数<script type="math/tex">L(\theta|x)</script>是给定样本x时，关于参数θ的函数，其在数值上等于给定参数<script type="math/tex">\theta</script>后变量X的概率</strong>：</p>
<script type="math/tex; mode=display">
L(\theta|x)=P(X=x|\theta)=P(x^1|\theta)P(x^2|\theta)\dots P(x^n|\theta)</script><p>在当<script type="math/tex">x_i</script>为离散型随机变量时，<script type="math/tex">f(x,\theta)</script>为概率密度函数，则<script type="math/tex">P(x|\theta)=f(x,\theta)</script>。</p>
<script type="math/tex; mode=display">
L(\theta|x)=f(x^1|\theta)f(x^2|\theta)\dots f(x^n|\theta)</script><p><strong>由于<script type="math/tex">X=\{x^1,x^2,x^3,...,x^n\}</script>即为已知数据，那么似然函数即为<script type="math/tex">L(\theta)</script>关于参数<script type="math/tex">\theta</script>的函数，而该似然函数的值越大，说明该参数的估计越佳，那么我们的目标即转变为找到最大的似然函数值，即为</strong></p>
<script type="math/tex; mode=display">
\theta^*=arg \ \ max\ L(\theta)</script><h3 id="S-类条件概率计算"><a href="#S-类条件概率计算" class="headerlink" title="$\S$类条件概率计算"></a>$\S$类条件概率计算</h3><p>回到类条件概率的计算，我们设水系精灵的样本集<script type="math/tex">X=\{x^1,x^2,x^3,...,x^{79}\}</script>，<strong>若每个<script type="math/tex">x^i</script>考虑n个特征</strong>，即<script type="math/tex">x^i=[x_1,x_2,\dots,x_n]</script>，服从<script type="math/tex">Gaussian\ distribution</script>(高斯分布) ，则有：</p>
<script type="math/tex; mode=display">
f_{\mu,B}(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|B|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)B^{-1}(x-\mu)^T}</script><p><strong>现在每个样本<script type="math/tex">x^i</script>仅考虑2个特征</strong>，即<script type="math/tex">x^i=[x_1,x_2]</script>，那么<script type="math/tex">x_i\thicksim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)</script>，即属于二维高斯分布，其概率密度函数为：</p>
<script type="math/tex; mode=display">
f_{\mu,B}(x)=\frac{1}{2\pi|B|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)B^{-1}(x-\mu)^T}</script><p><strong>其中<script type="math/tex">B</script>为协方差矩阵，<script type="math/tex">\mu</script>为均值向量。</strong></p>
<p>根据极大似然估计法，<script type="math/tex">x^i</script>均为独立同分布，且<script type="math/tex">x^i</script>均为离散型数据，则有<script type="math/tex">P(x^i|C)=f_{\mu,B}(x^i)</script>，即：</p>
<script type="math/tex; mode=display">
L(\mu,B)=P(x^1|C_1)P(x^2|C_1)\dots P(x^{79}|C_1)=f_{\mu,B}(x^1)f_{\mu,B}(x^2)\dots f_{\mu,B}(x^{79})=\prod_{i=1}^{79}f_{\mu,B}(x^i)</script><p><strong>那么似然函数<script type="math/tex">L(\mu,B)</script>就是关于参数<script type="math/tex">\mu,B</script>的函数。则最佳参数值即为：</strong></p>
<script type="math/tex; mode=display">
\mu^*,B^*=arg \ \ max_{\mu,B}\ L(\mu,B)</script><p>通过计算，求得最佳参数即为：</p>
<script type="math/tex; mode=display">
\mu^*=\frac{1}{79}\sum_{i=1}^{79}x^i\ \ , \ \ B^*=\frac{1}{79}\sum_{i=1}^{79}(x^i-\mu^*)(x^i-\mu^*)^T</script><p>带入已知数据集，可以分别计算得到两个类别的最佳参数值：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234034.png" alt="12" style="zoom:67%;"></p>
<h3 id="S-Result-1"><a href="#S-Result-1" class="headerlink" title="$\S$Result"></a>$\S$Result</h3><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234048.png" alt="13" style="zoom:67%;"></p>
<p><strong>观察可以发现，在Testing data上仅有47%的正确率。于是将<script type="math/tex">x^i</script>考虑的特征由2升为7，即<script type="math/tex">x^i</script>变成7维向量，但测试正确率仍只有54%。</strong></p>
<h3 id="S-模型改进"><a href="#S-模型改进" class="headerlink" title="$\S$模型改进"></a>$\S$模型改进</h3><p><strong>在最初的模型里，我们是利用似然函数，分别对Class 1 与Class 2进行参数估计，从而会产生两类参数，现在，我们将这两类公用同一个协方差矩阵<script type="math/tex">B</script>，而Class 1 与Class 2也共同用一个似然函数</strong>，即：</p>
<script type="math/tex; mode=display">
L(\mu_1,\mu_2,B)=f_{\mu_1,B}(x^1)f_{\mu_1,B}(x^2)\dots f_{\mu_1,B}(x^{79})f_{\mu_2,B}(x^{80})\dots f_{\mu_2,B}(x^{140})</script><p><strong>由此计算得到的<script type="math/tex">\mu_1，\mu_2</script>与最初计算结果相同，而<script type="math/tex">B=\frac{79}{140}B_1+\frac{61}{140}B_2</script>。</strong></p>
<p>而现在得到的结果如下所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234104.png" alt="14" style="zoom:67%;"></p>
<p>可以发现，当公用协方差矩阵<script type="math/tex">B</script>后，正确率可以提高，那么为什么会出现这种结果呢？<strong>因为当两个Class公用相同的协方差矩阵<script type="math/tex">B</script>后，可以减少参数，从而来防止模型发生Overfitting。</strong></p>
<h3 id="S-Naive-Bayes-Classifier"><a href="#S-Naive-Bayes-Classifier" class="headerlink" title="$\S$Naive Bayes Classifier"></a>$\S$Naive Bayes Classifier</h3><p>在上文中，我们的特征只有两个，如果特征变成多个应该如何处理呢？<script type="math/tex">x=[x_1,x_2,x_3,\dots,x_k ]</script>，<strong>如果各特征是在相互独立的情况下</strong>，那么有如下关系：</p>
<script type="math/tex; mode=display">
P(x|C_1)=P(x_1|C_1)P(x_2|C_1)\dots P(x_k|C_1)</script><p><strong>而这样做的好处是：<script type="math/tex">P(x_i|C_1)</script>遵循一维高斯分布，相较于二维高斯分布计算量大大降低，而这种方法叫做Naive Bayes Classifier(朴素贝叶斯分类器)。</strong></p>
<h3 id="S-概率密度函数选择"><a href="#S-概率密度函数选择" class="headerlink" title="$\S$概率密度函数选择"></a>$\S$概率密度函数选择</h3><p>在前文中，我们选择了随机变量服从高斯分布，事实上我们可以选择其他分布函数，这是比较随意的，但当随机变量呈现相关性质时，有一些特定的分布效果会更加好。</p>
<ul>
<li>当样本数据x取实数值时，采用高斯分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DN(\mu_{jc},\sigma_{jc})</script></li>
<li>当每种特征<script type="math/tex">x_j\in\{0,1\}</script>时，采用伯努利分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DBer(\mu_{jc},\sigma_{jc})</script></li>
<li>当每种特征取值<script type="math/tex">x_j\in\{1,2,3,\dots,k\}</script>时，采用分类分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DCat(\mu_{jc},\sigma_{jc})</script></li>
</ul>
<h2 id="Logistic-Regression-逻辑回归"><a href="#Logistic-Regression-逻辑回归" class="headerlink" title="Logistic Regression(逻辑回归)"></a>Logistic Regression(逻辑回归)</h2><p>在上节中，我们利用了贝叶斯公式，现在对贝叶斯公式进行一些变换：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\frac{1}{1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}</script><p>令<script type="math/tex">z=\ln\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}</script>，则有：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\frac{1}{1+\mathrm{e}^{-z}}=\sigma(z)</script><p>而<script type="math/tex">\sigma(z)</script>称为sigmoid function，其函数图像如下所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201120170942.png" style="zoom:67%;"></p>
<p>可以发现，<strong>sigmoid函数的值域在<script type="math/tex">(0,1)</script>之间，而它通常用于隐层神经元输出，可作为激活函数。</strong></p>
<h3 id="S-手推公式（可跳过）"><a href="#S-手推公式（可跳过）" class="headerlink" title="$\S$手推公式（可跳过）"></a>$\S$手推公式（可跳过）</h3><p>前文中，令<script type="math/tex">z=\ln\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}</script>，对其进行展开：</p>
<script type="math/tex; mode=display">
z=\ln\frac{P(x|C_1)}{P(x|C_2)}+\ln\frac{P(C_1)}{P(C_2)}\implies \ln\frac{P(x|C_1)}{P(x|C_2)}+\ln\frac{\frac{N_1}{N_1+N_2}}{\frac{N_2}{N_1+N_2}}</script><script type="math/tex; mode=display">
P(x|c_1)=\frac{1}{(2\pi)^{\frac{n}{2}}|B^1|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T}</script><script type="math/tex; mode=display">
P(x|c_2)=\frac{1}{(2\pi)^{\frac{n}{2}}|B^2|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T}</script><script type="math/tex; mode=display">
\begin{align*}
\ln\frac{\frac{1}{(2\pi)^{\frac{n}{2}}|B^1|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T}}{\frac{1}{(2\pi)^{\frac{n}{2}}|B^2|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T}}
&=\ln\frac{|B^2|^\frac{1}{2}}{|B^1|^\frac{1}{2}}\exp\{-\frac{1}{2}[(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T-(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T]\}\\
&=\ln\frac{|B^2|^\frac{1}{2}}{|B^1|^\frac{1}{2}}-\frac{1}{2}[(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T-(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T]
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}
(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T
&=(x-\mu^1)(B^1)^{-1}(x^T-(\mu^1)^T)\\
&=x(B^1)^{-1}x^T-x(B^1)^{-1}(\mu^1)^T-\mu^1(B^1)x^T+\mu^1(B^1)(\mu^1)^{T}\quad B^1\mbox{为对称矩阵}\\
&=x(B^1)^{-1}x^T-2\mu^1(B^1)x^T+\mu^1(B^1)(\mu^1)^{T}
\end{align*}</script><script type="math/tex; mode=display">
z=\ln\frac{|B^2|^\frac{1}{2}}{|B^1|^\frac{1}{2}}-\frac{1}{2}x(B^1)^{-1}x^T+\mu^1(B^1)^{-1}x^T-\frac{1}{2}\mu^1(B^1)^{-1}(\mu^1)^{T}+\frac{1}{2}x(B^2)^{-1}x^T-\mu^2(B^2)^{-1}x^T+\frac{1}{2}\mu^2(B^2)^{-1}(\mu^2)^{T}+\ln\frac{N_1}{N_2}</script><p>若假设<script type="math/tex">B^1=B^2=B</script>，则有：</p>
<script type="math/tex; mode=display">
\begin{align*}
z&=(\mu^1-\mu^2)B^{-1}x^T-\frac{1}{2}\mu^1B^{-1}(\mu^1)^{T}+\frac{1}{2}\mu^2B^{-1}(\mu^2)^{T}+\ln\frac{N_1}{N_2}\\
&=wx^T+b
\end{align*}</script><p>综上所述：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(w\cdot x+b)</script><h3 id="S-Function-set-函数集"><a href="#S-Function-set-函数集" class="headerlink" title="$\S$Function set(函数集)"></a>$\S$Function set(函数集)</h3><p>由以上公式推导可以得到:</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(z),\ z=w\cdot x+b=\sum_iw_ix_i+b</script><p>可以发现，<script type="math/tex">z</script>可用<script type="math/tex">\sum_iw_ix_i+b</script>进行线性表示。其中<script type="math/tex">w_i</script>是每个<script type="math/tex">x_i</script>的权重，<script type="math/tex">b</script>即为偏差，在这里<script type="math/tex">w\cdot x+b</script>即为决策边界，而这也是上文中为什么<strong>当两个class公用相同的协方差矩阵后，其决策边界变为直线的缘故</strong>。如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201121132947.png" style="zoom:67%;"></p>
<p><strong>这种函数集的分类问题叫做logistic regression(逻辑回归)。</strong></p>
<h3 id="S-Goodness-of-a-Function"><a href="#S-Goodness-of-a-Function" class="headerlink" title="$\S$Goodness of a Function"></a>$\S$Goodness of a Function</h3><p>假设现在有以下的training data:</p>
<script type="math/tex; mode=display">
\begin{align*}
data:&\ x^1 \ \ x^2 \ \ x^3 \dots x^n\\
class:&\ C_1 \ C_1 \ C_2 \dots C_1
\end{align*}</script><p>令<script type="math/tex">f_{w,b}(x)=P(C_1|x)</script>，则由极大似然估计，可定义以下损失函数：</p>
<script type="math/tex; mode=display">
L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\dots f_{w,b}(x^n)</script><p>而目标即为找到最大的<script type="math/tex">L(w,b)</script>，此时该函数的参数即为最佳参数值。</p>
<script type="math/tex; mode=display">
w^*,b^*=arg\ \ max \ L(w,b)</script><p>变形可得：</p>
<script type="math/tex; mode=display">
w^*,b^*=arg\ \ min \ -\ln L(w,b)</script><p>这样做的好处是，可以<strong>利用梯度下降算法求最小值，并且对<script type="math/tex">L(w,b)</script>取对数，可将乘积形式化为求和形式，易于后续计算。</strong></p>
<script type="math/tex; mode=display">
-\ln L(w,b)=-\ln f_{w,b}(x^1)-\ln f_{w,b}(x^2)-\ln[1- f_{w,b}(x^3)]\dots-\ln f_{w,b}(x^n)</script><p><strong>对class1，class2进行符号转换：</strong></p>
<script type="math/tex; mode=display">
class1:\widehat{y}^1=1,\widehat{y}^2=1\ \ class2:\widehat{y}^3=0\ \cdots</script><p>则：</p>
<script type="math/tex; mode=display">
-\ln f_{w,b}(x^1)\implies -[\widehat{y}^1\ln f(x^1)+(1-\widehat{y}^1)\ln (1-f(x^1))]</script><p>这样每个<script type="math/tex">-\ln f_{w,b}(x^i)</script>都可以统一成上式:</p>
<script type="math/tex; mode=display">
-\ln L(w,b)=\sum_i\color{orange}-[\widehat{y}^i\ln f(x^i)+(1-\widehat{y}^i)\ln (1-f(x^i))]</script><p>而橙色部分其实就是<strong>两个伯努利分布的交叉熵（Cross entropy)。</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201121154624.png" style="zoom:67%;"></p>
<p>如图所示，假设有两个分布：<script type="math/tex">p,q</script>如蓝色框所示，那么交叉熵的计算方式即$H(p,q)=-\sum_xp(x)ln(q(x))$。交叉熵代表的含义就是<strong>这两个分布有多接近</strong>。当两个分布相同时，计算的交叉熵就是熵。</p>
<h3 id="S-Find-the-best-function"><a href="#S-Find-the-best-function" class="headerlink" title="$\S$Find the best function"></a>$\S$Find the best function</h3><p>梯度下降求最小，计算步骤如图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201123003816.png" style="zoom:67%;"></p>
<p>对$\ln L(w,b)$求$w_i$的偏微分，只需要计算出$\frac{\ln f_{w,b}(x^n)}{\partial w_i}$与$\frac{\ln (1-f_{w,b}(x^n))}{\partial w_i}$。$f_{w,b}(x)$即为sigmoid 函数，$z=\sum_nw_ix_i+b$。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201123005251.png" style="zoom:67%;"></p>
<p>计算可得到$\frac{\ln (1-f_{w,b}(x^n))}{\partial w_i}$，将两个偏微分计算带入可得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
-\frac{\ln L(w,b)}{\partial w_i}&=\sum_n-[\widehat{y}^n(1-f_{w,b}(x^n_i))x^n_i+(1-\widehat{y}^n)f_{w,b}(x^n_i)x^n_i]\\
&=\sum_n-{\color{purple}(\widehat{y}^n-f_{w,b}(x^n))}x^n_i
\end{align*}</script><p>紫色部分其实直观的表示了真实数据值与函数值之间的差距大小。</p>
<h3 id="S-逻辑回归与线性回归的比较"><a href="#S-逻辑回归与线性回归的比较" class="headerlink" title="$\S$逻辑回归与线性回归的比较"></a>$\S$逻辑回归与线性回归的比较</h3><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201123011427.png" style="zoom:67%;"></p>
<p>可以发现，<strong>逻辑回归其实是在线性回归的基础上，对原线性模型再加上一层sigmoid 函数，从而将函数的输出介于(0,1)之间。而线性回归其输出可为任意值。</strong></p>
<p>在Logistic regression中，<strong>target的值为0或1</strong>，而在Linear regression中，<strong>target的值可为任意值</strong>。</p>
<h3 id="S-Discriminative-V-S-Generative-判别模型VS生成模型"><a href="#S-Discriminative-V-S-Generative-判别模型VS生成模型" class="headerlink" title="$\S$Discriminative V.S. Generative(判别模型VS生成模型)"></a>$\S$Discriminative V.S. Generative(判别模型VS生成模型)</h3><p>在前文中，其实已经介绍了两种不同的方法进行分类。</p>
<ol>
<li>通过假设概率密度函数求解类条件概率，从而通过贝叶斯公式分类，而这种分类方法就叫做生成模型(Generative model)。</li>
<li>通过利用梯度下降算法求解$w,b$。于是可带入逻辑回归模型进行分类，而这种分类方法就叫做判别模型(Discriminative model)。</li>
</ol>
<p>而判别模型和生成模型有什么区别呢?可以用一个例子来说明：</p>
<ul>
<li>判别模型：要确定一只羊是🐐还是🐏，用判别模型的方法是从历史数据中，直接学习到模型，然后通过提取到这只羊的特征带入模型进行预测这只羊是山羊的概率，是绵羊的概率。</li>
<li>生成模型：利用生成模型首先根据山羊的特征学习出山羊的模型，然后根据绵羊的特征学习出绵羊的模型。然后从这只羊中提取特征，放到山羊的模型中，看概率是多少；放到绵羊的模型中，看概率是多少，哪个概率大就是哪个。</li>
</ul>
<p>而对于这两种模型，<strong>它们的函数集都是相同的</strong>：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(wx+b)</script><p>对于判别模型，仅需要利用梯度下降算法，直接求解出参数$w,b$。</p>
<p>对于生成模型，需要通过假设概率分布求解出$\mu_1,\mu_2,B^{-1}$，而在<strong>手推公式</strong>这一节中，有如下代换：</p>
<script type="math/tex; mode=display">
\begin{align*}
w&=(\mu^1-\mu^2)B^{-1}\\
b&=-\frac{1}{2}\mu^1B^{-1}(\mu^1)^{T}+\frac{1}{2}\mu^2B^{-1}(\mu^2)^{T}+\ln\frac{N_1}{N_2}
\end{align*}</script><p>那么这两个不同模型得到的$w,b$相同吗？</p>
<p><strong>其实是不同的</strong>，为什么呢？因为在生成模型中，它是<strong>有假设</strong>的，例如它<strong>假设data满足高斯分布或伯努利分布等等</strong>，但在逻辑回归中，<strong>并没有这些假设</strong>，所以这导致了虽然它们的函数相同却最后计算得到的参数不同。</p>
<p>有些时候生成模型可能会出现一些问题。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201125003150.png" style="zoom:67%;"></p>
<p>例如，在这个例子中，共有13组数据，其中1组为class1，另外12组全部为class2。每组数据均有两个特征。现在当给出一个testing data，问其属于哪个类别。</p>
<p>对于人来说，第一反应它应该就是class1的。那么现在我们看看朴素贝叶斯分类器(naive bayes)是什么结果。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201125003002.png" style="zoom:67%;"></p>
<p>可以发现最后计算结果$P(C_1|x)&lt;0.5$，那么即朴素贝叶斯分类器认为当前tesing data属于class2，那么为什么会造成这个结果呢？<strong>朴素贝叶斯分类器是有假设这种情况存在的（机器脑补这种可能性）。所以结果和人类直观判断的结果不太一样。</strong></p>
<p>对于这两种模型，它们各自有各自的优势：</p>
<ol>
<li>当data数量较少时，<strong>生成模型受数据影响与判别模型相比，影响较小</strong>，因为它有<strong>自己的假设</strong>，甚至无视一些data。而当data较大时，判别模型的优势就较为明显了，因为当数据越多，<strong>它的误差就可以越小</strong>，从而模型越精确。</li>
<li><strong>当遇到一些data有问题时，而生成模型有做假设，有时候就可以把data中有问题的部分忽略掉。</strong></li>
</ol>
<h3 id="S-Multi-class-Classification-多类别分类"><a href="#S-Multi-class-Classification-多类别分类" class="headerlink" title="$\S$Multi-class Classification(多类别分类)"></a>$\S$Multi-class Classification(多类别分类)</h3><p>假定现在有三个类别，它们分别有自己的weight和bias。如下所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201129213423.png" style="zoom:67%;"></p>
<p>把$z_1,z_2,z_3$带入Softmax function​中，最后可得到输出$y_1,y_2,y_3$。可以发现$z_1,z_2,z_3$在Softmax中首先进行指数化，得到$e^{z_1},e^{z_2},e^{z_3}$，后除以$\sum_{j=1}^{3}e^{z_j}$，便可得到输出值。</p>
<p>总结可得Softmax function的函数形式即：</p>
<script type="math/tex; mode=display">
Softmax(z_i)=\frac{e^{z_i}}{\sum_{c=1}^{C}e^{c}}</script><p>那么为什么叫做Softmax呢？从字面意思上看，有soft和max，而max就是最大的意思，而Softmax的核心在于soft。通常情况下，给定一组数，我们要求它的最大值，而这里的最大值就是hardmax，而最大值只有一个，即非黑即白。但在多类别分类中，我们希望找出x的最大概率分类，而这并不能仅仅是一个hardmax。而Softmax的含义在于<strong>不再唯一的确定某个最大值，而是为每个输出分类的结果赋予一个概率值，代表属于该分类的可能性。</strong>它可以对最大的值进行强化，这样会导致大的值和小的值的差距会拉大。</p>
<p>也就是说，<strong>Softmax的输出值可以代表x属于每个类别的概率，即近似表示后验概率。</strong>为什么呢？当假设有三个Class服从高斯分布，当它们公用一个协方差矩阵时，就可以推导得到Softmax。</p>
<h3 id="S-Limitation-of-Logistic-Regression"><a href="#S-Limitation-of-Logistic-Regression" class="headerlink" title="$\S$Limitation of Logistic Regression"></a>$\S$Limitation of Logistic Regression</h3><p>假定现在一组数据有两个feature，则根据Logistic regression则有以下过程：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130191550.png" style="zoom:50%;"></p>
<p>对$x_1,x_2$分别赋予weight和bias，得到$z$带入sigmiod函数便可得到输出的概率值，根据$y$的值从而进行分类。这看似是没有任何问题的，但若现在有下面四组数据：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130192029.png" style="zoom: 67%;"></p>
<p>对两个feature分别赋予0,1，并且已经知道了它们各自的类别。现在我们可以把这四组数据当作四个点，表示在二维平面上。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130192414.png" style="zoom: 50%;"></p>
<p>观察图可以发现，蓝色的点就是属于Class 2的数据，红色的点就是属于Class 1的数据。<strong>如果用逻辑回归处理数据的话，根据逻辑回归的物理意义，它的分界面就是一条直线，使得这两组Class分开，从而达到分类的效果</strong>。</p>
<p>但很明显，观察上图可以发现，不存在这样的直线，能够使红点和蓝点分开。那么如何才能让它们可以用逻辑回归处理呢？</p>
<h3 id="S-Feature-Transformation-特征变换"><a href="#S-Feature-Transformation-特征变换" class="headerlink" title="$\S$Feature Transformation(特征变换)"></a>$\S$Feature Transformation(特征变换)</h3><p>所谓特征变换就是让特征通过某种变换变成新的特征。举个例子，现在如果有定义两个特征：$x_1,x_2$，它们经过某种变换变为新特征：$x_1^{’},x_2^{‘}$。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130195306.png" style="zoom:50%;"></p>
<p>现在假若定义如下变换：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130200034.png" style="zoom:67%;"></p>
<p>那么可以原来二维平面的坐标点可变为新平面下的坐标点：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130200631.png" style="zoom:67%;"></p>
<p>可以发现，在新平面下，原来不可分的点，现在是可分的了。这是为什么呢？</p>
<p>实际上，在通常情况下，我们的数据不一定是完全线性可分的，很多情况下会存在线性不可分，如同上面所举例子。而我们要将数据处理成线性可分，<strong>则需要进行特征变换，将数据投影到别的空间</strong>，例如上图，数据在$X$空间不可分，但在$X^{’}$空间就是可分的。<strong>而$X^{’}$空间的维度一般是高于$X$空间的维度的。</strong></p>
<p>但有时候很难去寻找有效的特征变换，于是能否有一种通用的方法进行特征变换？</p>
<h3 id="S-Cascading-logistic-regression-models-级联逻辑回归模型"><a href="#S-Cascading-logistic-regression-models-级联逻辑回归模型" class="headerlink" title="$\S$Cascading logistic regression models(级联逻辑回归模型)"></a>$\S$Cascading logistic regression models(级联逻辑回归模型)</h3><p>级联逻辑回归模型从字面意思上就可以看出就是将多个逻辑回归连接起来。而它就是为了解决上述特征变换的问题。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201201133501.png" style="zoom:67%;"></p>
<p>原理过程如图所示，由特征变换和分类两个过程组成。特征$x_1,x_2$作为输入，带入逻辑回归模型，可得到输出$x_1^{’},x_2^{‘}$，而这便是经过逻辑回归得到的两个新的特征。将新特征再作为输入带入逻辑回归模型，可得到分类概率$y$。</p>
<p>下面还是用一个例子来说明。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201201161411.png" style="zoom:67%;"></p>
<p>四组数据，通过调整参数$w_1,w_2,w_3,w_4,b_1,b_2$，可以得到四组数据经过逻辑回归的新特征$x_1^{’},x_2^{‘}$。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201201161240.png" style="zoom:67%;"></p>
<p>将新特征$x_1^{’},x_2^{‘}$带入新逻辑回归模型中，可以发现，新特征新特征$x_1^{’},x_2^{‘}$是线性可分的，由图可知，存在一条直线，将两组Class分开。</p>
<p>至此我们可以引入一个新的概念！</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201201163205.png" style="zoom:67%;"></p>
<p>由以上可知，<strong>一个逻辑回归的输入可以来源于其他逻辑回归的输出，这个逻辑回归的输出也可以是其他逻辑回归的输入，将每一个逻辑回归称为Neuron（神经元），把这些神经元连接起来的网络叫做Neuron Network（神经网络）！</strong></p>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2021/04/22/PyTorch%E6%95%B0%E6%8D%AE%E7%BB%B4%E5%BA%A6/</url>
    <content><![CDATA[<h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><p>在机器学习中，我们一直在和数据打交道。于是总免不了对数据进行处理。本篇博文将记录一些常见数据处理的函数用法。</p>
<h2 id="tensor-view"><a href="#tensor-view" class="headerlink" title="tensor.view()"></a>tensor.view()</h2><p>我们经常会遇到需要对数据进行维度、形状变换的问题。在pytorch中，tensor.view()可直接改变数据维度。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210422151608.png" style="zoom: 67%;"></p>
<p>这里有较为特殊的即为tensor.view(-1)，-1所在的维度是根据其他维度而计算得到的。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210422151843.png" style="zoom: 50%;"></p>
<p>-1所在的维度是通过：64/8而计算得到的，为8。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210422152036.png" style="zoom:67%;"></p>
<p>上图也是如此。</p>
]]></content>
  </entry>
  <entry>
    <title>GAN理论知识</title>
    <url>/2021/04/20/GAN/</url>
    <content><![CDATA[<h1 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h1><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>GAN由两部分网络组成：<strong>生成网络与判别网络</strong>。</p>
<p>通俗的来讲，<strong>生成网络可以理解为一个画赝品的人，而判别网络可以理解为一个判别真假的人。两者不断对抗，通过训练，从而达到使生成网络画赝品越来越高超，甚至以假乱真。</strong></p>
<p>下面我们来了解Generator与Discriminator。</p>
<h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><p>Generator也即为一个网络，那么我们希望这个网络达到什么目标呢？</p>
<p>首先，假如有一些人脸图片，那么我们希望这个网络可以生产出非常逼真的人脸图片，即这些人脸是原先并不存在的。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210420213437.png" style="zoom:50%;"></p>
<p>如图所示，我们希望向Generator中input一个从正态分布sample出来的vector。然后会输出一个Distribution，即$P_G$。而$P_{data}$为真实的Distribution。我们的目标是：让$P_G$与$P_{data}$越接近越好。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210420214728.png" style="zoom:50%;"></p>
<p>假如现在输入一个一维向量，那么经过Generator可能会input出绿色的一维向量，而真实的一维向量是蓝色的。所以我们的目标函数即为$G^*$，即最小化$Div(P_G,P_{data})$。这个Divergence越小，就代表$P_G$与$P_{data}$越相似。这个Divergence可以表示为两个向量的距离。</p>
<p>所以Generator的Loss function即为：$Div(P_G,P_{data})$，但由于实际的Divergence式子非常复杂。于是便有这样的问题，如何计算这个Loss function。</p>
<p>而GAN则可以解决这个问题。我们不需要知道$P_G$和$P_{data}$的formulation是什么样子。只要可以从它们间sample出来东西，便可以计算出Divergence。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210420221425.png" style="zoom:50%;"></p>
<p>而计算Divergence便需要Discriminator了。</p>
<h3 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h3><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210420222322.png" style="zoom:50%;"></p>
<p>首先，data中含有真实data与机器生成的data，Discriminator就相当于将从所有data中分辨出哪些是真实的data与假data。其中判别出真data后，给其较大的分数，判别出假data后给其较小的分数。</p>
<p>而我们想让目标函数$V$越大越好，即让$D(y)$越大越好。而目标函数$V$是与cross entropy是有联系的。即$V=-cross \ entropy$，那么现在这个优化问题便可以当成一个分类问题来做。而$V$其实是与$Divergence$有关。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210420223629.png" style="zoom:50%;"></p>
<p>如果$P_G$与$P_{data}$非常相近，那么它们之间的$Divergence$会比较小。那么这便会给$Discriminator$带来困难，其没有办法很好的分辨$P_{G}$与$P_{data}$。那么目标函数$V$的值便不会很大。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210420224236.png" style="zoom:50%;"></p>
<p>于是我们可以将Divergence替换为$max_{D}\ V(D,G)$，于是最初的优化问题便成为min max问题。于是上图训练的算法。尽管如此，GAN的train还是很困难的。</p>
<h2 id="GAN训练技巧"><a href="#GAN训练技巧" class="headerlink" title="GAN训练技巧"></a>GAN训练技巧</h2><h3 id="Wasserstein-distance"><a href="#Wasserstein-distance" class="headerlink" title="Wasserstein distance"></a>Wasserstein distance</h3><p><img src="C:/Users/79347/AppData/Roaming/Typora/typora-user-images/image-20210420232334222.png" style="zoom:50%;"></p>
<p>现在有$P$与$Q$两个distribution，可以想象$P$为一堆土，$Q$为土堆放的地方。有一台推土机，将$P$移动到$Q$的距离即为Wasserstein distace。假设P、Q都集中到一点，则Wasserstein distace=d。</p>
<p>但如果P、Q是比较复杂的distribution</p>
<p><img src="C:/Users/79347/AppData/Roaming/Typora/typora-user-images/image-20210420232949828.png" style="zoom:50%;"></p>
<p>例如P变成Q这样可以有多种move的方法。于是将Wasserstein distance定义为：穷举所有moving plan，看哪种方法所用的距离虽少，则即为Wasserstein distance。</p>
<p>求解下图优化问题，即得Wasserstein distance。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210421102506.png" style="zoom:50%;"></p>
<h2 id="Generator-评估"><a href="#Generator-评估" class="headerlink" title="Generator 评估"></a>Generator 评估</h2><p>当我们train一个GAN时，我们怎么判断Generator的output的好坏呢？如果我们的输出是一张人靓图片，我们可能会用肉眼进行直接观察，判断其是否逼真。但这样其实是无法准确的进行量度其质量的。</p>
<p><img src="C:/Users/79347/AppData/Roaming/Typora/typora-user-images/image-20210421111223876.png" style="zoom:50%;"></p>
<p><strong>我们可以将图片输入到一个图像分类系统，它的输出为一个概率分布。如果。如果这个概率分布越集中，就代表现在生成的图片可能会比较好。</strong></p>
<p>尽管我们不知道这个image是什么东西，可能是狗、猫、人。但如果分类系统输出的分布非常集中，则证明分类系统非常肯定它看见了狗或猫，所以Generator可能产生了比较逼真的图片。</p>
<p>但这样仍有问题，因为GAN可能会发生Mode Collapse（模式崩溃）。</p>
<h3 id="Mode-Collapse"><a href="#Mode-Collapse" class="headerlink" title="Mode Collapse"></a>Mode Collapse</h3><p><img src="C:/Users/79347/AppData/Roaming/Typora/typora-user-images/image-20210421112724484.png" style="zoom:50%;"></p>
<p>由上图可以发现Generator生成的图片会频繁出现一种动漫人物的脸。这是为什么呢？</p>
<p><strong>由于Discriminator有盲点。即对于刚才频繁出现的动漫人物，它没有办法进行辨识出它为假图片，于是Generator便会频繁生成这类图片。于是便会发生Mode Collapase现象。</strong>但Mode Collapse问题现在仍没有好的解决方案。</p>
<h3 id="Mode-Dropping"><a href="#Mode-Dropping" class="headerlink" title="Mode Dropping"></a>Mode Dropping</h3><p>我们在train的时候，也经常会发生一种类似Mode Collapase的现象——<strong>Mode Dropping</strong>。简单的描述就是其结果的多样性有问题，没有办法达到真实数据的多样性。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210421124319.png" style="zoom:50%;"></p>
<p>在第t个iteration时，我们还没发现有什么问题。但在第t+1个iteration时，我们会发现，它的肤色基本是相同的，其多样性是有问题的。这便是出现了Mode Dropping。</p>
<p>那么如何解决Mode Dropping呢？</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210421124808.png" style="zoom:50%;"></p>
<p>我们的方法就是把<strong>生成的图片都输入进image classify里面，看它被判断成哪一个class。每一张图片都会输出一个distribution。我们将所有distribution取一个平均。如果平均的distribution非常集中，那么多样性便不够。</strong></p>
<p>而这个评估似乎与评估图片质量的方法是相反的。评估图片质量（quality），要求其产生的分布越集中越好；而评估生成的图片多样性（diversity），要求其产生的分布越平均越好。</p>
<p>但其实它们的评估的范围是不同的。<strong>quality是要求输入一张图片产生的distribution越集中越好，而diversity要求输入所有的图片，最后平均的distribution越平均越好。</strong>故两者是并不冲突的。</p>
<h3 id="Evaluation-Method"><a href="#Evaluation-Method" class="headerlink" title="Evaluation  Method"></a>Evaluation  Method</h3><p>使用FID方法。我们不取最后softmax计算得到的类别，而是取在它前面的hidden layer的输出，即incepton network的输出。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210421131636.png" style="zoom:50%;"></p>
<p><strong>我们将这两组高维向量拿出来，假设它们都是Gaussian distribution，然后计算它们的Frechet distance。由于是distance，所以越小越好，即代表两个分布越接近。</strong></p>
<p>但仍有一些问题，即两个分布可能并不是高斯分布，但我们仍将其假设为高斯分布，会比较奇怪。而如果要准确的得到network的分布，可能需要sample很多次。</p>
<h2 id="Condition-Generator"><a href="#Condition-Generator" class="headerlink" title="Condition Generator"></a>Condition Generator</h2><p>如果我们想要输入一个描述性的文字，然后得到相应的图片，应该怎么做呢？这便是condition generator的作用。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210421133834.png" style="zoom:50%;"></p>
<p>在原来distribution Z的基础上，我们加入condition X。假如X即为text，我们希望可以输出与text想对应的image。</p>
<p>那么我们要如何训练呢？</p>
<p>传统的discriminator只需要输入图片，这显然是对condition gan训练是没有作用的。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210421134935.png" style="zoom:50%;"></p>
<p>因此必须要加入condition X也作为discriminator的输入。<strong>只有图像与文字匹配了才会给高分。如果图像与文字不匹配</strong>，则给低分。所以在训练discriminator时，我们需要有标注的图片。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210421135526.png" style="zoom:50%;"></p>
<p>尽管有positive sample 与negative sample 往往是不够的，我们还需要把<strong>生成的图片故意打乱配上错误的文字</strong>，以此来告诉discriminator遇到这样的情况也要给低分。</p>
]]></content>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2021/04/21/SVM/</url>
    <content><![CDATA[<h1 id="SVM理论知识"><a href="#SVM理论知识" class="headerlink" title="SVM理论知识"></a>SVM理论知识</h1><h2 id="拉格朗日对偶性"><a href="#拉格朗日对偶性" class="headerlink" title="拉格朗日对偶性"></a>拉格朗日对偶性</h2><h3 id="Primal"><a href="#Primal" class="headerlink" title="Primal"></a>Primal</h3><p>如果有如下优化问题：</p>
<script type="math/tex; mode=display">
\begin{split}
&min \quad f(w)\\
&s.t.\begin{cases}
g_i(w)\le 0\\
h_i(w)=0
\end{cases}
\end{split}</script><p>我们可以相应的构造它的<strong>拉格朗日函数</strong>：</p>
<script type="math/tex; mode=display">
L(w,\alpha,\beta)=f(w)+\sum_{i=1}^k\alpha_ig_i(w)+\sum_{i=1}^l\beta_ih_i(w)</script><p>定义</p>
<script type="math/tex; mode=display">
\theta_p(w)=max_{\alpha,\beta} \quad L(w,\alpha,\beta),\quad \alpha_i,\beta_i\ge0</script><ul>
<li>注意$\theta_p(w)$是对$L(w,\alpha,\beta)中\alpha与\beta$的优化。</li>
<li>如果存在$w$使得其不满足约束条件，则$\theta_p(w)$的值即为$+\infty$（因为如果存在某个$g_i(w)&gt;0或h_i(w)\neq0$，则令$\alpha_i\rightarrow\infty或\beta_i\rightarrow\infty $，即可得到$L的最大值$）。</li>
<li>如果$w$都符合约束条件，则$\theta_p(w)$的值即为$f(w)$（后两项为负值，若$\alpha_i=0且\beta_i=0 $，即为$f(w)$）。</li>
</ul>
<p>所以我们原先求取的目标函数即可变为：</p>
<script type="math/tex; mode=display">
min_{w} \quad\theta_p(w)=min_{w} \quad max \ L(w,\alpha,\beta),\quad \alpha_i,\beta_i\ge0</script><p>定义原始问题（Primal）的最优值为：</p>
<script type="math/tex; mode=display">
P^*=min_{w} \quad\theta_p(w)</script><h3 id="Dual"><a href="#Dual" class="headerlink" title="Dual"></a>Dual</h3><p>定义</p>
<script type="math/tex; mode=display">
\theta_D(\alpha,\beta)=min_{w} \quad L(w,\alpha,\beta),\quad \alpha_i,\beta_i\ge0</script><ul>
<li>注意$\theta_D(\alpha,\beta)$是对$L(w,\alpha,\beta)中w$的优化。</li>
</ul>
<p>而极大化$\theta_D(\alpha,\beta)$，即为：</p>
<script type="math/tex; mode=display">
max_{\alpha,\beta}\quad\theta_D(\alpha,\beta)=max_{\alpha,\beta} \quad min \ L(w,\alpha,\beta),\quad \alpha_i,\beta_i\ge0</script><p>定义对偶问题（Dual）的最优值为：</p>
<script type="math/tex; mode=display">
D^*=max_{\alpha,\beta} \quad\theta_D(\alpha,\beta)</script><h3 id="Primal与Daul的关系"><a href="#Primal与Daul的关系" class="headerlink" title="Primal与Daul的关系"></a>Primal与Daul的关系</h3><p>对于原始问题与对偶问题求解得到的最优值$P^<em>,D^</em>$，它们有如下关系：</p>
<script type="math/tex; mode=display">
P^*=min_{w} \quad\theta_p(w)\le max_{\alpha,\beta} \quad\theta_D(\alpha,\beta)=D^*</script><p>而它们只有<strong>KKT条件</strong>下是相等的，即$P^<em>=D^</em>$。那么我们便可以通过求解Dual问题来求解Primal问题。</p>
<p>同时满足原问题与对偶问题的最优解$w^<em>,\alpha^</em>,\beta^<em>$满足的<em>*KKT条件</em></em>如下：</p>
<script type="math/tex; mode=display">
\begin{split}
&\nabla_wL(w^*,\alpha^*,\beta^*)=0\\
&\alpha_i^*g_i(w^*)=0\\
&g_i(w^*)\le0\\
&\alpha^*\ge0
\end{split}</script><p>通过KKT条件，我们可以求解最优解。其第二个条件需着重观察，可以发现，若数据带入$g_i(w)$中，若其不为0，则说明$\alpha_i=0$。</p>
<h2 id="线性可分的SVM"><a href="#线性可分的SVM" class="headerlink" title="线性可分的SVM"></a>线性可分的SVM</h2><p>在一个二维平面上，有两种类型的数据，如下图所示</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210421185958.png" style="zoom:50%;"></p>
<p>我们可以很容易找到一条线将它们分开，并且这种线是有很多的。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210421185849.png" style="zoom: 25%;"></p>
<p>而支持向量（support vector）的定义即是：<strong>存在一条线到a数据边界的distance与到b数据边界的distance是相同的，这条线便是支持向量决策线。</strong></p>
<p>找到的支持向量便是SVM算法得到的最优解。</p>
<p>将两条边界上的线的距离定义为margin，而SVM则希望找一个最大的margin，所以SVM还有另外一个名字：max margin classifier。</p>
<p>假设现在有两个support vector，如图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210421192000.png" style="zoom: 33%;"></p>
<p>我们将用数学公式，对两个支持向量的距离进行推导</p>
<p>设边界上的两条线分别为：</p>
<script type="math/tex; mode=display">
\begin{cases}
w^Tx_1+b=1\\
w^Tx_2+b=-1
\end{cases}</script><p>将两式相减便可得到：</p>
<script type="math/tex; mode=display">
w^T(x_1-x_2)=2</script><p>而$w^T$与$(x_1-x_2)$都为向量，故它们相乘实际上是做内积</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210421192629.png" style="zoom:33%;"></p>
<p>故可得：</p>
<script type="math/tex; mode=display">
\Vert w\Vert_2\Vert x_1-x_2\Vert_2\cos\theta=2</script><p>则便可得：</p>
<script type="math/tex; mode=display">
d_1+d_2=\Vert x_1-x_2\Vert_2\cos\theta=\frac{2}{\Vert w\Vert_2}\\
d_1=d_2=\frac{1}{2}\Vert x_1-x_2\Vert_2\cos\theta=\frac{1}{\Vert w\Vert_2}</script><p>上面提到SVM的目标就是max-margin，而$margin=\frac{2}{\Vert w\Vert_2}$，故可以得到SVM的数学模型：</p>
<script type="math/tex; mode=display">
\begin{split}
&min \quad \frac{1}{2}\Vert w\Vert^2\\
&s.t. \quad y^{(i)}(wx^{i}+b)\geqslant1
\end{split}</script><p>上面的目标函数并不难理解，对原距离求其分母的最小值，便是求远距离的最大值。</p>
<p>而约束条件的意思即：有这样的一个vector $w$与bias $b$，对空间中任意一点$x^i$，带入$wx+b$中，再乘以$y^{i}$（数据的标签，可为1或-1），都是大于等于1的。我理解的意思便是：<strong>数据都被较好的分类，即1类数据都在1类的平面里，-1类的数据都在-1类的平面里。</strong></p>
<p>我们可以将上式改写为如下形式：</p>
<script type="math/tex; mode=display">
g_i(w,b)=-y^{(i)}(wx^{i}+b)+1\leqslant0</script><p>将其拉格朗日化可得：</p>
<script type="math/tex; mode=display">
L(w,b,\alpha)=\frac{1}{2}\Vert w\Vert^2-\sum_{i=1}^n\alpha_i[y^{(i)}(wx^{i}+b)-1]</script><p>则由KKT条件可得：</p>
<script type="math/tex; mode=display">
\begin{cases}
\nabla_wL=w-\sum_{i=1}^n\alpha_iy^{(i)}x^i=0\\
\nabla_bL=-\sum_{i=1}^n\alpha_iy^{(i)}=0
\end{cases}</script><p>带入原式可得：</p>
<script type="math/tex; mode=display">
L(\alpha)=\sum_i^n\alpha_i-\frac{1}{2}\sum_{i,j=1}^ny^{(i)}y^{(j)}\alpha_i\alpha_j(x^{(i)})^Tx^{(j)}</script><p>上述经过KKT条件的带入，已对$w,b$进行了优化，得到的$L(\alpha)$即上文Dual问题中的$\theta_D$。而现在便可将其转换为Dual问题：</p>
<script type="math/tex; mode=display">
\begin{split}
&max \quad L(\alpha)=\sum_i^n\alpha_i-\frac{1}{2}\sum_{i,j=1}^ny^{(i)}y^{(j)}\alpha_i\alpha_j(x^{(i)})^Tx^{(j)}\\
&s.t.\begin{cases}
\alpha_i\geq0\\
\sum_{i=1}^n\alpha_iy^{(i)}=0
\end{cases}
\end{split}</script><p>通过构造Dual问题，进行求解得到$\alpha$的值。</p>
<p>而SVM的决策子如下：</p>
<script type="math/tex; mode=display">
w^Tx+b=(\sum_{i=1}^n\alpha_iy^{(i)}x^i)^Tx+b=\sum_{i=1}^n\alpha_iy^{(i)}\lang x^i,x\rang+b</script><p>因为$\alpha_iy^{(i)}$为常数，故$(x^i)^Tx=\lang x^i,x\rang$做内积。</p>
<p>但这样可以发现，当输入值$x$时，需要与所有的训练数据进行内积并求和。如果数据过大，则每次输入$x$时，太过花费时间与内存。</p>
<p>但为什么SVM会很流行呢？因为根据KKT条件，我们会发现这个规则：</p>
<script type="math/tex; mode=display">
\alpha_i^*g_i(w^*)=0</script><p>而在这里$g_i(w,b)=-y^{(i)}(wx^{i}+b)+1\leqslant0$</p>
<p><strong>也就是说，如果$g_i(w)$带入的数据$(x^i,y^{(i)})$并不等于0，那么其对应的$\alpha_i=0$。而事实上较多的训练数据带入约束函数中并不等0，故其相应的$\alpha_i$等于0，于是SVM决策子便会大大简单。当输入$x$后，也并不会与以往所有的数据做一次内积求和。事实上真正保留下来的$\alpha_i$为数据正好为支持向量，其所对应的$\alpha_i$。</strong></p>
<h2 id="带松弛变量的SVM"><a href="#带松弛变量的SVM" class="headerlink" title="带松弛变量的SVM"></a>带松弛变量的SVM</h2><p>上文为标准的支持向量机模型。但在实际情况中，其往往会出现一些问题。</p>
<p>当我们的数据有一些异常值的时候，如下所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210430172633.png" style="zoom:33%;"></p>
<p>如果按照SVM标准模型，则支持向量决策线为图中的虚线，但我们可以看出，蓝色点有一个异常值，如果不管那个异常值，则支持向量决策线应为上图中的蓝色线。</p>
<p>而对于这种情况，我们可以通过放松限制来解决：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210430173124.png" style="zoom:33%;"></p>
<p>以下为带松弛变量的SVM模型：</p>
<script type="math/tex; mode=display">
\mbox{Primal问题:}\begin{split}
&min \quad \frac{1}{2}\Vert w\Vert^2+C\sum_{i=1}^n\zeta_i\\
&s.t. \begin{cases}
y^{(i)}(wx^{i}+b)\geqslant1-\zeta_i, \ i=1,2,\dots,n\\
\zeta_i\ge0
\end{cases}
\end{split}</script><p>可以发现，引入的$\zeta_i$即为松弛变量，这样我们可以使异常值也可以满足限制条件。但我们不能使$\zeta_i$过大，因为这样的话限制条件便没有作用了，于是我们在目标函数中加入$C\sum_{i=1}^n\zeta_i$，以使它放松并不能太大。而参数$C$是需要我们自己进行调参的，通过调节$C$，可以控制$\zeta_i$的大小。</p>
<p>而其Dual问题为：</p>
<script type="math/tex; mode=display">
\begin{split}
&max \quad L(\alpha)=\sum_i^n\alpha_i-\frac{1}{2}\sum_{i,j=1}^ny^{(i)}y^{(j)}\alpha_i\alpha_j\lang x^{(i)},x^{(j)}\rang\\
&s.t.\begin{cases}
0\leq\alpha_i\leq C\\
\sum_{i=1}^n\alpha_iy^{(i)}=0
\end{cases}
\end{split}</script><p><strong>在这里$y^{(i)}y^{(j)}$为不同数据的标签，可以发现如果类别相同，则其为正数，而类别不同，则其为负数。所以类别相同，其值增加；类别不同，其值减少。而对于$\lang x^{(i)},x^{(j)}\rang$，则是两个数据作内积，其衡量两个数据之间的相似性。</strong></p>
<p>对于$\sum_{i=1}^n\alpha_iy^{(i)}=0$而言，其表示不同数据点的权重是不相同的，即每个数据的看重程度是不同的。<strong>由于$y^{(i)}$的取值为-1或1，则为使其和为0，则不同类别的数据其权重应该是相同的。</strong></p>
<p>而KKT条件也发生了一些变化：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210430175353.png" alt></p>
<p>而除了用上述最小化问题或Dual问题来理解，我们也可以用<strong>Hinge Loss</strong>来理解。</p>
<h3 id="Hinge-Loss-合页损失函数"><a href="#Hinge-Loss-合页损失函数" class="headerlink" title="Hinge Loss(合页损失函数)"></a>Hinge Loss(合页损失函数)</h3><p>根据上述最小化问题的约束，</p>
<script type="math/tex; mode=display">
y^{(i)}(wx^{i}+b)\geqslant1-\zeta_i, \ i=1,2,\dots,n\\
\zeta_i\ge0</script><p>可以得到：</p>
<script type="math/tex; mode=display">
\zeta_i\geqslant1-y^{(i)}(wx^{i}+b)\\
\zeta_i\geq0</script><p>则以上两个式子即为：</p>
<script type="math/tex; mode=display">
\zeta_i= max(0,1-y^{(i)}(wx^i+b))</script><p>而化成这样的好处是，它是一个可求导的函数，这样我们便可使用SGD进行优化，利用PyTorch的Auto grading进行自动求梯度。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210430185638.png" alt></p>
<h2 id="带kernel的SVM"><a href="#带kernel的SVM" class="headerlink" title="带kernel的SVM"></a>带kernel的SVM</h2><p>使用核函数：</p>
<script type="math/tex; mode=display">
\begin{split}
&max \quad L(\alpha)=\sum_i^n\alpha_i-\frac{1}{2}\sum_{i,j=1}^ny^{(i)}y^{(j)}\alpha_i\alpha_jk(x^{(i)},x^{(j)})\\
&s.t.\begin{cases}
0\leq\alpha_i\leq C\\
\sum_{i=1}^n\alpha_iy^{(i)}=0
\end{cases}
\end{split}</script><p>对于上节的SVM模型，其改变的地方仅从目标函数由$\lang x^{(i)},x^{(j)}\rang$变为$k(x^{(i)},x^{(j)})$，其中$k(x^{(i)},x^{(j)})$即称为<strong>核函数</strong>。</p>
<p>核函数定义如下：</p>
<script type="math/tex; mode=display">
K(x^i,x^j)=\lang \varPhi(x^i),\varPhi(x^j)\rang</script><p>其做了两步操作：</p>
<ul>
<li>首先使用$\varPhi(x)$函数对数据进行高维投影</li>
<li>然后对其求内积</li>
</ul>
<p>这里要说明的是，使用kernel函数与只内积，它们所求得的$\alpha$都是相同的。也是因为这样，使用kernel trick才是有意义的。</p>
<p>而使用核函数后，其预测公式为：</p>
<script type="math/tex; mode=display">
w^T\varPhi(x)+b=\sum_{i=1}^n\alpha_iy^{(i)}k(x^i,x)+b</script><p>只有当$x_i$为支持向量的时候，$\alpha_i&gt;0$。</p>
<h3 id="为什么使用核函数"><a href="#为什么使用核函数" class="headerlink" title="为什么使用核函数"></a>为什么使用核函数</h3><p>那么我们为什么要使用核函数呢?观察下图</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210430201924.png" style="zoom:33%;"></p>
<p>对于上图中的数据，其使用传统的SVM模型是无法进行分类的。而使用核函数的原因就是<strong>处理线性不可分</strong>。</p>
<p>那么kernel为什么可以处理线性不可分呢?</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210430213812.png" alt></p>
<p>当一维数据$x$经过函数$\varPhi(x)$处理后，数据升维。之前线性不可分的数据经过升维后就可以线性可分了。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210430215033.png" alt></p>
<p>例如输入数据有两个feature$[x_{i1},x_{i2}]$，经过升维后，它就变为$[x_{i1},x_{i2},x_{i1}x_{i2},x_{i1}^2,x_{i2}^2]$，可以发现其feature是变多了。</p>
<p>那么既然通过升维可以使线性不可分变为线性可分，那所有的数据我们都来升维？<strong>但如果当一条数据的feature特别多时，升维后，其feature将变得非常多，可能不利于我们计算。（计算量与数据量和每一条数据的维度呈正相关）</strong></p>
<h3 id="成为kernel的条件"><a href="#成为kernel的条件" class="headerlink" title="成为kernel的条件"></a>成为kernel的条件</h3><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210430220126.png" alt></p>
<p>即由kernel形成的矩阵$G_{ij}$，其需要满足两个条件：</p>
<ul>
<li>为<strong>对称矩阵</strong></li>
<li>为<strong>半正定矩阵</strong>，$z^TGz\geq 0,z\in R^n$。</li>
</ul>
<h3 id="常用的kernel"><a href="#常用的kernel" class="headerlink" title="常用的kernel"></a>常用的kernel</h3><h4 id="多项式核（Polynomial-Kernel）"><a href="#多项式核（Polynomial-Kernel）" class="headerlink" title="多项式核（Polynomial Kernel）"></a>多项式核（Polynomial Kernel）</h4><script type="math/tex; mode=display">
K(x_i,x_j)=(\lang x_i,x_j\rang+c)^d</script><ul>
<li>$c\ge0$控制低阶项的强度</li>
<li>特殊情况，当$c=0,d=1$成为线性核（Linear kernel），相当于无核SVM一样。</li>
</ul>
<p>下面可以通过一个例子来理解：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210430225019.png" alt></p>
<p>这里的polynomial kernel中$c=0,d=2$。</p>
<h4 id="高斯核（Gaussian-Kernel或Radial-Basis-Function-RBF-Kernel）"><a href="#高斯核（Gaussian-Kernel或Radial-Basis-Function-RBF-Kernel）" class="headerlink" title="高斯核（Gaussian Kernel或Radial Basis Function-RBF Kernel）"></a>高斯核（Gaussian Kernel或Radial Basis Function-RBF Kernel）</h4><script type="math/tex; mode=display">
K(x_i,x_j)=exp(-\frac{\| x_i-x_j\|_2^2}{2\sigma^2})</script><p>当$x_i$=$x_j$时，其值为1。当$x_i$与$x_j$的距离增加，其值倾向于0。<strong>但当使用高斯核之前需要将其特征做标准化。</strong>因为<strong>不同特征之间其值变化范围可能不一样。</strong></p>
<p>下面是随着$\sigma^2$值的变化，高斯核的变化。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210430231527.png" alt></p>
<p>下面看一个高斯核的例子：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210430232438.png" alt></p>
<h4 id="Sigmoid-Kernel"><a href="#Sigmoid-Kernel" class="headerlink" title="Sigmoid Kernel"></a>Sigmoid Kernel</h4><script type="math/tex; mode=display">
K(x_i,x_j)=\tanh (\beta x_i^Tx_j+c)</script><p>tanh为双曲正切函数，由$\sinh=\frac{e^x-e^{-x}}{2}$与$\cosh=\frac{e^x+e^{-x}}{2}$得到，即为$\frac{e^x-e^{-x}}{e^x+e^{-x}}$。</p>
<h4 id="Cosine-Similarly-Kernel"><a href="#Cosine-Similarly-Kernel" class="headerlink" title="Cosine Similarly Kernel"></a>Cosine Similarly Kernel</h4><script type="math/tex; mode=display">
K(x_i,x_j)=\frac{x_i^Tx_j}{\|x_i\|_2\|x_j\|_2}</script><ul>
<li>常用于衡量两段文字的相似性。</li>
<li>相当于衡量两个向量的余弦相似度（向量夹角的余弦值）</li>
</ul>
<h4 id="Chi-Squared-Kernel"><a href="#Chi-Squared-Kernel" class="headerlink" title="Chi Squared Kernel"></a>Chi Squared Kernel</h4><script type="math/tex; mode=display">
K(x_i,x_j)=\frac{x_i^Tx_j}{\|x_i+x_j\|_{L1}}</script><ul>
<li><p>常用于计算机视觉</p>
</li>
<li><p>衡量两个概率分布的相似性</p>
</li>
<li><p>输入数据必须是非负的，且必须使用L1归一化。</p>
<p>其中L1归一化公式为：</p>
<script type="math/tex; mode=display">
y_i=\frac{x_i}{\sum_{i=1}^nx_i}</script><p>L2归一化公式为：</p>
<script type="math/tex; mode=display">
y_i=\frac{x_i}{\sqrt{\sum_{i=1}^nx_i^2}}</script></li>
</ul>
<h2 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h2><p>对于已经建立好的SVM数学模型：</p>
<script type="math/tex; mode=display">
\begin{split}
&max \quad L(\alpha)=\sum_i^n\alpha_i-\frac{1}{2}\sum_{i,j=1}^ny^{(i)}y^{(j)}\alpha_i\alpha_jk(x^{(i)},x^{(j)})\\
&s.t.\begin{cases}
0\leq\alpha_i\leq C\\
\sum_{i=1}^n\alpha_iy^{(i)}=0
\end{cases}
\end{split}</script><p>我们一般采用SMO算法对其求解，SMO算法一般有下面两个步骤：</p>
<ul>
<li><p>重复直到收敛：</p>
<ol>
<li>选择下一步需要优化的$\alpha_i$与$\alpha_j$，使用启发式的方式优化$\alpha_i$与$\alpha_j$，使目标函数向着最大值的进发最快。</li>
<li>保持其他的$\alpha$值不变，仅仅通过一次改变一组$\alpha_i$与$\alpha_j$来优化$L(\alpha)$。（为什么SMO一次需要选择两个参数进行优化？因为如果每次只选择一个参数进行优化，固定其他参数。那么它会不满足数学模型的第二条约束条件。</li>
</ol>
</li>
<li><p>使用KKT条件判断是否收敛。</p>
</li>
</ul>
<p>我们可以通过以下例子来理解：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210501214227.png" alt></p>
<p>而现在我们的目的就是通过变化$\alpha_1$与$\alpha_2$来使得$L(\alpha)$达到最大。</p>
<p>我们可以将上式变换得到：</p>
<script type="math/tex; mode=display">
\alpha_1=(\zeta-\alpha_2y^{(2)})\frac{1}{y^{(1)}}</script><p>可以将上式带入$L(\alpha)$中，便得到：</p>
<script type="math/tex; mode=display">
L(\alpha_1,\alpha_2,\dots,\alpha_n)=L((\zeta-\alpha_2y^{(2)})\frac{1}{y^{(1)}},\alpha_2,\dots,\alpha_n)</script><p>由于$\alpha_3,\alpha_4\dots,\alpha_n$为常数，则目标函数就变为了关于$\alpha_2$的函数。通过求导，便可得到该函数的最值。</p>
<p>下面为SMO具体的算法：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210501221431.png" alt></p>
]]></content>
  </entry>
</search>
