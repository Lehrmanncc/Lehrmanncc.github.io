<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Love record</title>
    <url>/2020/11/10/Love/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="e33c091047010d6935b3f264258306604fdd4b3a48b8391216b6112f40eac053">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <tags>
        <tag>Love record</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenCV 基础操作</title>
    <url>/2020/10/14/opencv/</url>
    <content><![CDATA[<h2 id="图像上的算术运算"><a href="#图像上的算术运算" class="headerlink" title="图像上的算术运算"></a>图像上的算术运算</h2><h3 id="图像融合"><a href="#图像融合" class="headerlink" title="图像融合"></a>图像融合</h3><ul>
<li><p><strong>图像融合即也是图像加法，但它是对每个图像乘以相应的权重，使其具有融合或透明的感觉。根据以下等式进行运算。</strong></p>
<script type="math/tex; mode=display">
G(x)=(1-\alpha)f_1(x)+\alpha f_2(x),\quad \alpha\in[0,1]</script></li>
<li><p><strong>f(x)可作为输入的图像，则上式可变为：</strong></p>
<script type="math/tex; mode=display">
dst=\beta\cdot img_1+\alpha\cdot img_2+\gamma\\
\beta+\alpha=1</script></li>
<li><p><strong>在OpenCV中进行图像融合的函数为cv.addWeighted(img1,$\beta$,img2,$\alpha,\gamma$),其有四个参数,img1与img2分别为输入的图像，$\beta,\alpha$分别为各自图像融合时所占的权重，一般情况下$\gamma$为0。</strong></p>
<p>以下为测试代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img1=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\save.png&#x27;</span>)</span><br><span class="line">img2=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\imori.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dst1=cv.addWeighted(img1,<span class="number">0.8</span>,img2,<span class="number">0.2</span>,<span class="number">0</span>)</span><br><span class="line">dst2=cv.addWeighted(img1,<span class="number">0.2</span>,img2,<span class="number">0.8</span>,<span class="number">0</span>)</span><br><span class="line">images=[dst1,dst2]</span><br><span class="line">titles=[<span class="string">&#x27;dst1&#x27;</span>,<span class="string">&#x27;dst2&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    images[i]=cv.cvtColor(images[i],cv.COLOR_BGR2RGB)</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(images[i],<span class="string">&#x27;jet&#x27;</span>)</span><br><span class="line">    plt.title(titles[i],fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>代码解读：</p>
</li>
<li><p>在这里要注意到，使用Matplotlib显示图片时，要先将图片转换为RGB形式，在OpenCV中，图像是以BGR通道存储的，直接显示会出现问题。</p>
</li>
<li><p>上述测试代码里的img1为img2的灰度图，可以发现当$\beta$取0.8，$\alpha$取0.2时，img1融合时占比较高，当$\beta$取0.2，$\alpha$取0.8时，二者的对比结果如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116131929.png" alt="4" style="zoom:33%;"></p>
<p>可以发现前者更加偏灰度，后者更加偏向原图。</p>
</li>
</ul>
<h3 id="按位运算"><a href="#按位运算" class="headerlink" title="按位运算"></a>按位运算</h3><ul>
<li><p>有时候，我们不想将图像相加或者融合，例如，下面这张图片：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132446.jpg" alt="8" style="zoom:50%;"></p>
<p>它就是由两张图片——（西电LOGO和人物图）进行“掩盖”合成的。</p>
</li>
<li><p><strong>按位操作分为以下四种：AND，OR，NOT，XOR，一一介绍它们的运算：</strong></p>
<p><strong>1. AND：即“与”运算，（1&amp;1=1，1&amp;0=0，0&amp;1=0，0&amp;0=0），其具体意义可理解为，只有两值都大于0，其为真。而在像素中，其取值在0~255之间，0为黑色，255为白色，而这里大于0的像素值都可以看作1。在两个都大于0的像素值之间进行AND运算，其结果取较小的像素值！</strong></p>
<p><strong>2. NOT：即“非”运算，（~1=0，~0=1），其具体意义可理解为，取像素的相反值，则当图像为二值图时，原图的黑色变为白色，白色变为黑色。</strong></p>
<p><strong>3. OR：即“与”运算，（1|1=1，1|0=1，0|1=1，0|0=0），其具体意义可理解为，只有当两值都为0，其为假。而对于两像素值，其先进行二进制转换，后对其进行运算，例如3|5：00000011|00000101=00000111，因此3|5=7。</strong></p>
<p><strong>4. XOR：即“异或”运算，（1^1=0，1^0=1，0^1=1，0^0=0），其具体意义可理解为，只有当两值不相同时，其为真。对于两两像素值，先进行二进制转换，后对其进行运算，例如3^5：00000011^00000101=00000110，因此3^5=6。</strong></p>
</li>
<li><p><strong>OpenCV中，进行按位运算的有以下四个函数：cv.bitwise_not，cv.bitwise_and，cv.bitwise_or，cv.bitwise_xor，这四个函数即对应上面的运算规则。下面将以下两张图片合成为上图：</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132002.jpg" alt="6" style="zoom: 33%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116130801.jpg" alt="7" style="zoom:25%;"></p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img1=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\xidian1.jpg&#x27;</span>)</span><br><span class="line">img2=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\girl.jpg&#x27;</span>)</span><br><span class="line">row,cols,channels=img1.shape</span><br><span class="line">roi=img2[<span class="number">0</span>:row,<span class="number">0</span>:cols]</span><br><span class="line"></span><br><span class="line">img1_gray=cv.cvtColor(img1,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret,dst1=cv.threshold(img1_gray,<span class="number">200</span>,<span class="number">255</span>,cv.THRESH_BINARY_INV)</span><br><span class="line">dst1_inv=cv.bitwise_not(dst1)</span><br><span class="line"></span><br><span class="line">img2_bg=cv.bitwise_and(roi,roi,mask=dst1_inv)</span><br><span class="line">img1_bg=cv.bitwise_and(img1,img1,mask=dst1)</span><br><span class="line"></span><br><span class="line">dst=cv.add(img1_bg,img2_bg)</span><br><span class="line">img2[<span class="number">0</span>:row,<span class="number">0</span>:cols]=dst</span><br><span class="line"></span><br><span class="line">cv.imshow(<span class="string">&#x27;images&#x27;</span>,img2)</span><br><span class="line"></span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>代码解读:</p>
<ul>
<li><p><strong>首先由shape得到img1（LOGO）图像的大小，即行数和列数。由此，可以在img2图像上，通过numpy直接划分出与img1图像一样大小的 roi图像（为原图的左上角）。</strong></p>
</li>
<li><p><strong>通过threshold函数可将img1图像二值化，而在这之前需要将img1图像转换为灰度图，才能将其传入。使用 THRESH_BINARY_INV方法将其二值化，二值化图像如下：</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132043.png" alt="8" style="zoom: 33%;"></p>
<p><strong>将得到的二值化图像（dst1）利用“非”运算，得到的图像如下：</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132101.png" alt="9" style="zoom:33%;"></p>
<p><strong>可以发现，进行“非”运算后，对二值图来说，其实即进行了颜色反转。</strong></p>
</li>
</ul>
<hr>
</li>
</ul>
<h2 id="阈值分割"><a href="#阈值分割" class="headerlink" title="阈值分割"></a>阈值分割</h2><h3 id="固定阈值分割"><a href="#固定阈值分割" class="headerlink" title="固定阈值分割"></a>固定阈值分割</h3><ul>
<li><p><strong>阈值分割简单来说，即大于阈值的变成一种值，小于阈值的为另一种值</strong>   </p>
</li>
<li><p>在python的cv2库中实现固定阈值分割的为<strong>cv2.threshold()函数</strong>。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\imori.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">ret, th1 = cv.threshold(img, <span class="number">127</span>, <span class="number">255</span>, cv.THRESH_BINARY)</span><br><span class="line">ret,th2=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY_INV)</span><br><span class="line">ret,th3=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO)</span><br><span class="line">ret,th4=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO_INV)</span><br><span class="line">ret,th5=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TRUNC)</span><br><span class="line"></span><br><span class="line">titles=[<span class="string">&#x27;Org&#x27;</span>,<span class="string">&#x27;Binary&#x27;</span>,<span class="string">&#x27;Binary_inv&#x27;</span>,<span class="string">&#x27;Tozero&#x27;</span>,<span class="string">&#x27;Tozero_inv&#x27;</span>,<span class="string">&#x27;Trunc&#x27;</span>]</span><br><span class="line">images=[img,th1,th2,th3,th4,th5]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(images[i],<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.title(titles[i],fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  运行结果如图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201116132138.png" alt="1" style="zoom: 50%;"></p>
<ul>
<li><p><strong>cv2.threshold()函数由4个参数组成，img为传入函数的图像，127为设置的阈值大小(threshold)，255为阈值设定方式里的阈值最大值(maxval)，THRESH_BINARY为阈值设定的方式。</strong>      </p>
</li>
<li><p><strong>ret代表当前的阈值。</strong></p>
</li>
<li><p><strong>matplotlib.pyplot中subplot(2,3,i+1)即为将窗口分为2行3列，i+1表示当前的第i+1个子图，imshow()则是对图像进行处理，它不会让图片进行显示，图像显示需要show()</strong></p>
</li>
<li><p><strong>阈值设定有5种方法，分别为:</strong></p>
<p><strong>1.THRESH_BINARY:</strong></p>
</li>
</ul>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
maxval &\ if\ src(x,y)>thresh\\
0 &\ otherwise
\end{cases}</script><p>当原像素值大于阈值，原像素值变为maxval，除此之外为0。</p>
<p>​        <strong>2.THRESH_BINARY_INV:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
0 &\ if \ src(x,y)>thresh\\
maxval &\ otherwise
\end{cases}</script><p>该阈值方法与上述阈值方法相反，从图像也可以观察得到。</p>
<p>​        <strong>3.THRESH_TOZERO:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
src(x,y) &\ if\ src(x,y)>thresh\\
0 &\ oherwise
\end{cases}</script><p>​        当原像素值大于阈值时，原像素值保持不变，除此之外，原像素值变为0。</p>
<p>​        <strong>4.THRESH_TOZERO_INV:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
0 &\ if \ src(x,y)>thresh\\
src(x,y) &\ otherwise
\end{cases}</script><p>​        该阈值方法与上述阈值方法相反。</p>
<p>​        <strong>5.THRESH_TRUNC:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
threshold &\ if \ src(x,y)>thresh\\
src(x,y) &\ otherwise
\end{cases}</script><p>​        放原像素值大于阈值，则原像素值变为阈值，除此之外，原像素值保持不变。</p>
]]></content>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine learning</title>
    <url>/2020/10/23/ML/</url>
    <content><![CDATA[<h1 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a><strong>Regression</strong></h1><h2 id="Linear-regression-线性回归"><a href="#Linear-regression-线性回归" class="headerlink" title="Linear regression(线性回归)"></a>Linear regression(线性回归)</h2><h3 id="问题的导入：预测神奇宝贝进化后的战斗力（CP）值"><a href="#问题的导入：预测神奇宝贝进化后的战斗力（CP）值" class="headerlink" title="问题的导入：预测神奇宝贝进化后的战斗力（CP）值"></a>问题的导入：预测神奇宝贝进化后的战斗力（CP）值</h3><ul>
<li><p><strong>输入：进化前神奇宝贝A的CP值，种类，血量（HP），重量（Weight)，高度（Height）</strong></p>
</li>
<li><p><strong>输出：进化后神奇宝贝A的CP值</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233907.png" alt="1" style="zoom:67%;"></p>
</li>
</ul>
<h3 id="机器学习的主要步骤："><a href="#机器学习的主要步骤：" class="headerlink" title="机器学习的主要步骤："></a>机器学习的主要步骤：</h3><p><strong>step 1：Model（确定模型）</strong></p>
<p><strong>step 2：Goodless of function（确定损失函数）</strong></p>
<p><strong>step 3：Best function（找到最好的函数）</strong></p>
<h3 id="S-Model"><a href="#S-Model" class="headerlink" title="$\S $Model"></a>$\S $Model</h3><p>在这里，根据常理推测，神奇宝贝进化后的CP值和进化前的CP值有较大的关系，故我们可以假设进化前的CP值与进化后的CP值呈线性关系。</p>
<p>设神奇宝贝进化前的CP值为$x_{cp}$，进化后的CP值为$y$，则可建立以下线性关系：</p>
<script type="math/tex; mode=display">
y=b+wx_{cp} \tag 1</script><p>在这里我们仅考虑了神奇宝贝进化前的CP值这一特征，如果神奇宝贝影响进化后的CP值的不止这一特征，若有n个特征，那么我们可将$x_{cp}$推广到$x_i$，于是便可得到<strong>Linear model</strong>（线性模型）：</p>
<script type="math/tex; mode=display">
\begin{cases}
y=b+\sum_i^nw_ix_i\\
x_i:feature\ (特征)\\
w_i:weight \ (权重)\\
b:bias \ (偏差) \\
\end{cases} \tag 2</script><p>在这个模型里，一般而言，我们会有<strong>training data</strong>，假设现在我们有10组训练数据，即：</p>
<script type="math/tex; mode=display">
(x^1,\widehat{y}^1)\\
(x^2,\widehat{y}^2)\\
\vdots\\
(x^{10},\widehat{y}^{10}) \tag 3</script><p>对于以上数据来说，它们是已知的，其上标代表了它们的序号，而现在，我们可以将以上数据带入简化模型（式1）中，可得：</p>
<script type="math/tex; mode=display">
\widehat{y}^1=b+wx_{cp}^1\\
\widehat{y}^2=b+wx_{cp}^2\\
\vdots\\
\widehat{y}^{10}=b+wx_{cp}^{10}\\  \tag 4</script><p>对于式4，即就是我们训练数据带入模型后所得，而其只有两个未知数，即$b,w$，而现在我们需要做的就是<strong>找到最适合的$b,w$参数值</strong>。现在就需要损失函数发挥作用了。</p>
<h3 id="S-Goodless-of-function"><a href="#S-Goodless-of-function" class="headerlink" title="$\S $Goodless of  function"></a>$\S $Goodless of  function</h3><ul>
<li><p><strong>损失函数的作用</strong></p>
<script type="math/tex; mode=display">
Loss\ function\ L:
\begin{cases}
input:&\ a\ function\\
output:&\ how\ bad\ it\ is
\end{cases}</script><p>这里要注意评价函数的输入，即为函数$f$，而其输出是它的好坏，但由上式可知，式（4）其<strong>未知量只有待求参数$b,w$，</strong>那么函数评价函数$L(f)$即为：</p>
<script type="math/tex; mode=display">
L(f)=L(w,b) \tag 5</script><p>那么现在，<strong>对于输出所评价函数的好坏即可转化为评价参数$w,b$的好坏。</strong></p>
</li>
<li><p><strong>常见的损失函数</strong></p>
<p>现在我们已有真实数据，那么如何反映其参数的好坏呢，我们可以<strong>通过真实的CP值与预测CP值的差进行衡量</strong>，如下所示：</p>
<script type="math/tex; mode=display">
L(f)=\sum_{i=1}^{10}(\widehat{y}^i-f(x_{cp}^i))^2\Longrightarrow\ L(w,b)=\sum_{i=1}^{10}(\widehat{y}^i-(b+w\cdot x_{cp}^i))^2 \tag 6</script><p>当真实值与误差值较小时，那么此时的$w,b$可认为是最好的，则我们的目标即为：</p>
<script type="math/tex; mode=display">
min\quad L(w,b)</script><p>于是我们现在就需要对其进行求解。</p>
</li>
</ul>
<h3 id="S-Best-function-——-Gradient-Descent（梯度下降法）"><a href="#S-Best-function-——-Gradient-Descent（梯度下降法）" class="headerlink" title="$\S $Best  function ——-Gradient  Descent（梯度下降法）"></a>$\S $Best  function ——-Gradient  Descent（梯度下降法）</h3><p>我们需要通过$Loss\ Function\Longrightarrow”the \ best\ function”$，即达到我们的目标，令：</p>
<script type="math/tex; mode=display">
w*,b*=arg\ \ min_{w,b}\ \ L(w,b)=arg\ \ min_{w,b}\ \ \sum_{i=1}^{10}(\widehat{y}^i-(b+w\cdot x_{cp}^i))^2 \tag 7</script><p>对于以上，我们可以采用穷举法，求得$L(w,b)$的最小值，但下面介绍更加高效的算法。</p>
<ul>
<li><p>梯度下降法</p>
<p>若现在损失函数$L(w,b)$只考虑其$w$单变量情况，有以下函数图像：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233452.png" alt="2" style="zoom:67%;"></p>
<p>当我们现在<strong>随机</strong>在函数上选择一点$w^0$，</p>
<p>1.如果函数在该点的斜率为负，即导数$\frac{\mathrm{d}L}{\mathrm{d}w}&lt;0$，那么此时函数应该为<strong>递减的</strong>，于是可以考虑增大$w^0$。</p>
<p>2.如果函数在该点的斜率为正，即导数$\frac{\mathrm{d}L}{\mathrm{d}w}&gt;0$，那么此时函数应该为<strong>递增的</strong>，于是可以考虑减少$w^0$。</p>
<p>增大多少或减少多少？</p>
<p>定义每次移动的步长为：</p>
<script type="math/tex; mode=display">
step=\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0 \tag 8</script><p>在式（8）中，$\eta$称为”learning rate（学习率）”，<strong>我们可以通过控制$\eta$的大小，从而增加step，以此可以提高求解效率，但若$\eta$过大，则会导致步长过大，那么有可能会错过最优解，故我们需要选取合适的$\eta$。而$\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0$，其微分值越大$\rightarrow$曲线越陡峭$\rightarrow$移动步长越大$\rightarrow$提高求解效率。</strong></p>
<p>以上图为例，其算法如下：</p>
<script type="math/tex; mode=display">
w^0-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0\Longrightarrow w^1\\
w^1-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^1\Longrightarrow w^2\\
\vdots \ \ 更新循环\\
w^{t-1}-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{t-1}\Longrightarrow w^t</script><p>在上式中，因为$\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{0}&lt;0$，故需给其添加负号。对点不断更新循环，直到$(\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{t})=0$停止，此时$w=w^t$即所求参数的最优解。</p>
<p><strong>但从上图我们可以发现$w^t$其实并不为global optimal solution(全局最优解)，其为local optimal solution(局部最优解)，不过由于我们研究的为线性回归，所以不存在局部最优解。故在线性回归中，梯度下降法是有效的。</strong></p>
<p>现在，我们可以将单变量$w$推广到多变量$w,b$，函数如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233543.png" alt="3" style="zoom:67%;"></p>
<p>随机选取点$(w^0,b^0)$，计算函数在该点的偏导$\frac{\partial L}{\partial w}|w=w^0,b=b^0,\frac{\partial L}{\partial b}|w=w^0,b=b^0$。</p>
<p>那么函数在该点的梯度即为：</p>
<script type="math/tex; mode=display">
grad\ L(w^0,b^0)=\nabla L(w^0,b^0)=(\frac{\partial L}{\partial w}|w^0,b^0)\overrightarrow{i}+(\frac{\partial L}{\partial b}|w^0,b^0)\overrightarrow{j}\tag 9</script><p>其中$\nabla$为向量微分算子。</p>
<p><strong>而对于梯度向量，其方向为曲线在等值线上的法线方向（高数中有对其的证明），其通过不断的增加步长以更新点，从而达到最优解，如上图所示。</strong></p>
<p>与单变量相同，多变量算法与其相同：</p>
<script type="math/tex; mode=display">
\begin{cases}
w^0-\eta\frac{\partial L}{\partial w}|w=w^0,b=b^0\Longrightarrow w^1\\
b^0-\eta\frac{\partial L}{\partial b}|w=w^0,b=b^0\Longrightarrow b^1
\end{cases}\\
\begin{cases}
w^1-\eta\frac{\partial L}{\partial w}|w=w^1,b=b^1\Longrightarrow w^2\\
b^1-\eta\frac{\partial L}{\partial b}|w=w^1,b=b^1\Longrightarrow b^2
\end{cases}\\
\vdots\ \ 更新循环\\
\begin{cases}
w^t-\eta\frac{\partial L}{\partial w}|w=w^t,b=b^t\Longrightarrow w^t\\
b^t-\eta\frac{\partial L}{\partial b}|w=w^t,b=b^t\Longrightarrow b^t
\end{cases}\\</script><p>最后直到$\nabla L(w^t,b^t)=0$停止，此时<strong>$w^t,b^t$即为所求得的最优解，由于在线性回归中，所以不同担心会由于初始值的选取，而影响达到全局最优解。</strong></p>
</li>
</ul>
<h3 id="S-Result"><a href="#S-Result" class="headerlink" title="$\S $Result"></a>$\S $Result</h3><p>如果现在通过十组训练数据，得到的结果如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233626.png" alt="4" style="zoom:67%;"></p>
<p>如图所示，其通过梯度下降法，所求得的最优参数$b,w$分别为-188.4，2.7，其中，损失值可用$\frac{1}{10}\sum_{n=1}^{10}\mathrm{e}^n$表示，<strong>$e^n$即为第n组真实值与预测值的差值。</strong></p>
<p>现在将model改用不同函数，观察其结果变化情况，如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233707.png" alt="5" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233746.png" alt="6" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233820.png" alt="7" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233844.png" alt="8" style="zoom:67%;"></p>
<p>可以发现，<strong>随着Model改用函数的阶数的升高，训练集损失值越来越小，但可以发现测试集损失值却并不是越来越少，相反，当阶数大于4后，其测试集损失值增大较多，而这种现象叫做”Overfitting”(过拟合)。</strong></p>
<p>那么我们如何才能防止过拟合的发生呢？</p>
<ul>
<li><p><strong>Regularization(正则化)</strong></p>
<p><strong>由于过拟合是model过度拟合其在训练集的数据，则其会造成曲线波动较大，而如果能让曲线变得smooth平滑，便可防止过拟合。</strong></p>
<p><strong>于是考虑，增加一个与参数$w$（斜率）相关的值，如果loss function 越小的话，那么对应这个值也会越小，即$w$也会越小，于是便会使函数更加平滑。</strong></p>
<p>于是可在线性回归模型中，将loss function变为：</p>
<script type="math/tex; mode=display">
L=\sum_{i=1}^{10}(\widehat{y}^i-(b+\sum_jw_jx_j))^2+\lambda\sum_j(w_j)^2 \tag{10}</script><p>而在式（10）中，$\lambda\sum_j(w_j)^2$称为正则化项，$\lambda$称为正则化参数，它的作用就是平衡loss function这两项，若$\lambda$取得非常大，即对参数$w_j$惩罚的非常大，那么相当于$w_j$趋于0，即相当于原线性回归模型中删除了这些项，那么函数就变成了一条$y=b$的水平直线。</p>
<p>那么为什么减小$w_j$能够使其函数不宜发生overfitting呢？</p>
<p>若现在令$\varDelta x_j$为输入时的干扰项，那么输出所形成的误差即为：</p>
<script type="math/tex; mode=display">
y=b+\sum_jw_j(x_j+\varDelta x_j) \Longrightarrow \varDelta y=w_j\varDelta x_j</script><p>由此我们可以发现，当$w_j$较小时，产生的误差$\varDelta y$也更小，则更加光滑的函数会受更少的影响。</p>
<p>但观察正则化项，我们会发现为什么没有$b$呢？</p>
<p><strong>其实，正则化调整的是函数的平滑程度，但我们调整b对函数的平滑程度不会造成影响，它只能让函数上下移动。</strong></p>
</li>
</ul>
<h1 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a><strong>Classification</strong></h1><p>什么是分类呢?例如在现实生活中，当我们看见一张关于猫与狗的照片时，我们可以很容易区分，照片里的属于猫还是狗，那么如果现在有一些不同类别的大量数据，显然由人进行分类是不现实的，那么我们希望可以借助计算机来帮我们进行数据的分类……</p>
<p>那么我们如何解决分类问题呢？例如下图：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233933.png" alt="9" style="zoom:67%;"></p>
<p>希望能<strong>建立一个函数，输入一个神奇宝贝，就能返回其属性（类别）。</strong></p>
<p>首先我们需要将神奇宝贝数字化，即通过它的特征（Hp，Attack，Defense……）来描述这只神奇宝贝。首先将分类问题简化，只考虑其只有两类。</p>
<h2 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h2><p>我们的目标仍是寻找一个函数，输入x输出其类别，即为：</p>
<ul>
<li><p>Function (Model):</p>
<script type="math/tex; mode=display">
x \rightarrow \ f(x)=\begin{cases}
g(x)>0 \quad &output=class\ 1\\
else &output=class\ 2
\end{cases}</script><p>输入x，则当函数值&gt;0，其属于class 1，否则其属于class 2。而这个函数我们可以利用概率模型进行描述，即<script type="math/tex">P(C_1|x)>0.5</script></p>
</li>
<li><p>Loss Function:</p>
<p>Loss Function的目的是用来评价函数的好坏，那么我们想用模型分类的结果，与正确的模型结果进行对比，统计错误的次数。</p>
<script type="math/tex; mode=display">
L(f)=\sum_n\delta(f(x^n)\neq\widehat{y}^n)</script></li>
<li><p>Find the best function:</p>
<p>Example:perception(感知机算法)、svm(支持向量机)</p>
</li>
</ul>
<h2 id="贝叶斯概率模型"><a href="#贝叶斯概率模型" class="headerlink" title="贝叶斯概率模型"></a>贝叶斯概率模型</h2><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233955.png" alt="10" style="zoom:67%;"></p>
<p>在Box 1中我们可以计算得到抽出蓝色球的概率为<script type="math/tex">\frac{4}{5}</script>，绿色球的概率为<script type="math/tex">\frac{1}{5}</script>，若已知我们抽取Box 1的概率为<script type="math/tex">\frac{2}{3}</script>，那么我们就可以利用贝叶斯公式计算得到，已知抽取的小球为蓝色，则从Box 1抽出的机率为多少，即<script type="math/tex">P(B_1|Blue)</script>。</p>
<p>而现在我们可以将这些球看作是一个个数据，从而可以利用贝叶斯公式生成概率模型。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234009.png" alt="11" style="zoom:67%;"></p>
<ul>
<li>先验概率：<script type="math/tex">P(C)</script>，即每种类别发生的概率。</li>
<li>类条件概率：<script type="math/tex">P(x|C)</script>，在某种类别的条件下，某事发生的概率。</li>
<li>后验概率：<script type="math/tex">P(C|x)</script>，某事发生了，它属于某种类别的概率。</li>
</ul>
<p>如果我们可以计算得到<script type="math/tex">P(C_1)</script>，<script type="math/tex">P(C_2)</script>，与<script type="math/tex">P(x|C_1)</script>，<script type="math/tex">P(x|C_2)</script>，那么我们可以通过贝叶斯公式计算得到<script type="math/tex">P(C_1|x)</script>，<script type="math/tex">P(C_2|x)</script>，如果<script type="math/tex">P(C_1|x)>0.5</script>，那么可以认为<script type="math/tex">x</script>是属于Class 1 ，相反可认为<script type="math/tex">x</script>是属于Class 2的。</p>
<p>那么我们的目标就是计算<script type="math/tex">P(C_1)</script>，<script type="math/tex">P(C_2)</script>，与<script type="math/tex">P(x|C_1)</script>，<script type="math/tex">P(x|C_2)</script>。</p>
<ul>
<li><p><strong>先验概率的计算</strong></p>
<p>假设现在两个类别，Class 1：Water(水系)，Class 2：Normal(其他系)。</p>
<p><strong>水系有79只神奇宝贝，其他系有61只神奇宝贝，总样本共有140只神奇宝贝。</strong></p>
<script type="math/tex; mode=display">
\implies P(C_1)=\frac{79}{140}=0.56,\ P(C_2)=\frac{61}{140}=0.44</script><p>即通过所得样本中各自种类的占比得到先验概率，其计算较为简单。</p>
</li>
<li><p><strong>类条件概率的计算</strong></p>
<p>假设水系精灵中有6杰尼龟(水系)，那么我们可以认为在水系精灵中，杰尼龟的概率是<script type="math/tex">P(x|C_1)=\frac{6}{79}=0.076</script>吗？</p>
<p>事实上，这是不对的，<strong>因为该水系精灵样本较少，所以我们无法直接仅由样本中所占比例来推得类条件概率。假设该水系样本精灵中没有水箭龟，但水箭鬼确实是水系精灵，那么我们能说明水箭龟在水系精灵中的概率为0吗？这显然是有问题的。</strong></p>
<p>那么我们如何计算<script type="math/tex">P(x|C)</script>呢？我们利用<strong>极大似然估计法</strong>。</p>
</li>
</ul>
<h3 id="S-极大似然估计"><a href="#S-极大似然估计" class="headerlink" title="$\S$ 极大似然估计"></a>$\S$ 极大似然估计</h3><p>对于<script type="math/tex">P(x|C)</script>的计算，我们很难去寻找到随机变量<script type="math/tex">x</script>所遵循的概率密度函数。<strong>那么如果我们假设已知随机变量<script type="math/tex">x</script>所遵循的概率密度函数，转而计算所假设概率密度函数中的参数值，如果找到最佳的参数值，那么我们就能用该概率密度函数代替类条件概率的密度函数了。</strong></p>
<p>若有样本集<script type="math/tex">X=\{x^1,x^2,x^3,...,x^n\}</script>，每个样本集中的样本都是所谓独立同分布的随机变量。</p>
<p><strong>似然函数定义为：似然函数<script type="math/tex">L(\theta|x)</script>是给定样本x时，关于参数θ的函数，其在数值上等于给定参数<script type="math/tex">\theta</script>后变量X的概率</strong>：</p>
<script type="math/tex; mode=display">
L(\theta|x)=P(X=x|\theta)=P(x^1|\theta)P(x^2|\theta)\dots P(x^n|\theta)</script><p>在当<script type="math/tex">x_i</script>为离散型随机变量时，<script type="math/tex">f(x,\theta)</script>为概率密度函数，则<script type="math/tex">P(x|\theta)=f(x,\theta)</script>。</p>
<script type="math/tex; mode=display">
L(\theta|x)=f(x^1|\theta)f(x^2|\theta)\dots f(x^n|\theta)</script><p><strong>由于<script type="math/tex">X=\{x^1,x^2,x^3,...,x^n\}</script>即为已知数据，那么似然函数即为<script type="math/tex">L(\theta)</script>关于参数<script type="math/tex">\theta</script>的函数，而该似然函数的值越大，说明该参数的估计越佳，那么我们的目标即转变为找到最大的似然函数值，即为</strong></p>
<script type="math/tex; mode=display">
\theta^*=arg \ \ max\ L(\theta)</script><h3 id="S-类条件概率计算"><a href="#S-类条件概率计算" class="headerlink" title="$\S$类条件概率计算"></a>$\S$类条件概率计算</h3><p>回到类条件概率的计算，我们设水系精灵的样本集<script type="math/tex">X=\{x^1,x^2,x^3,...,x^{79}\}</script>，<strong>若每个<script type="math/tex">x^i</script>考虑n个特征</strong>，即<script type="math/tex">x^i=[x_1,x_2,\dots,x_n]</script>，服从<script type="math/tex">Gaussian\ distribution</script>(高斯分布) ，则有：</p>
<script type="math/tex; mode=display">
f_{\mu,B}(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|B|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)B^{-1}(x-\mu)^T}</script><p><strong>现在每个样本<script type="math/tex">x^i</script>仅考虑2个特征</strong>，即<script type="math/tex">x^i=[x_1,x_2]</script>，那么<script type="math/tex">x_i\thicksim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)</script>，即属于二维高斯分布，其概率密度函数为：</p>
<script type="math/tex; mode=display">
f_{\mu,B}(x)=\frac{1}{2\pi|B|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)B^{-1}(x-\mu)^T}</script><p><strong>其中<script type="math/tex">B</script>为协方差矩阵，<script type="math/tex">\mu</script>为均值向量。</strong></p>
<p>根据极大似然估计法，<script type="math/tex">x^i</script>均为独立同分布，且<script type="math/tex">x^i</script>均为离散型数据，则有<script type="math/tex">P(x^i|C)=f_{\mu,B}(x^i)</script>，即：</p>
<script type="math/tex; mode=display">
L(\mu,B)=P(x^1|C_1)P(x^2|C_1)\dots P(x^{79}|C_1)=f_{\mu,B}(x^1)f_{\mu,B}(x^2)\dots f_{\mu,B}(x^{79})=\prod_{i=1}^{79}f_{\mu,B}(x^i)</script><p><strong>那么似然函数<script type="math/tex">L(\mu,B)</script>就是关于参数<script type="math/tex">\mu,B</script>的函数。则最佳参数值即为：</strong></p>
<script type="math/tex; mode=display">
\mu^*,B^*=arg \ \ max_{\mu,B}\ L(\mu,B)</script><p>通过计算，求得最佳参数即为：</p>
<script type="math/tex; mode=display">
\mu^*=\frac{1}{79}\sum_{i=1}^{79}x^i\ \ , \ \ B^*=\frac{1}{79}\sum_{i=1}^{79}(x^i-\mu^*)(x^i-\mu^*)^T</script><p>带入已知数据集，可以分别计算得到两个类别的最佳参数值：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234034.png" alt="12" style="zoom:67%;"></p>
<h3 id="S-Result-1"><a href="#S-Result-1" class="headerlink" title="$\S$Result"></a>$\S$Result</h3><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234048.png" alt="13" style="zoom:67%;"></p>
<p><strong>观察可以发现，在Testing data上仅有47%的正确率。于是将<script type="math/tex">x^i</script>考虑的特征由2升为7，即<script type="math/tex">x^i</script>变成7维向量，但测试正确率仍只有54%。</strong></p>
<h3 id="S-模型改进"><a href="#S-模型改进" class="headerlink" title="$\S$模型改进"></a>$\S$模型改进</h3><p><strong>在最初的模型里，我们是利用似然函数，分别对Class 1 与Class 2进行参数估计，从而会产生两类参数，现在，我们将这两类公用同一个协方差矩阵<script type="math/tex">B</script>，而Class 1 与Class 2也共同用一个似然函数</strong>，即：</p>
<script type="math/tex; mode=display">
L(\mu_1,\mu_2,B)=f_{\mu_1,B}(x^1)f_{\mu_1,B}(x^2)\dots f_{\mu_1,B}(x^{79})f_{\mu_2,B}(x^{80})\dots f_{\mu_2,B}(x^{140})</script><p><strong>由此计算得到的<script type="math/tex">\mu_1，\mu_2</script>与最初计算结果相同，而<script type="math/tex">B=\frac{79}{140}B_1+\frac{61}{140}B_2</script>。</strong></p>
<p>而现在得到的结果如下所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234104.png" alt="14" style="zoom:67%;"></p>
<p>可以发现，当公用协方差矩阵<script type="math/tex">B</script>后，正确率可以提高，那么为什么会出现这种结果呢？<strong>因为当两个Class公用相同的协方差矩阵<script type="math/tex">B</script>后，可以减少参数，从而来防止模型发生Overfitting。</strong></p>
<h3 id="S-Naive-Bayes-Classifier"><a href="#S-Naive-Bayes-Classifier" class="headerlink" title="$\S$Naive Bayes Classifier"></a>$\S$Naive Bayes Classifier</h3><p>在上文中，我们的特征只有两个，如果特征变成多个应该如何处理呢？<script type="math/tex">x=[x_1,x_2,x_3,\dots,x_k ]</script>，<strong>如果各特征是在相互独立的情况下</strong>，那么有如下关系：</p>
<script type="math/tex; mode=display">
P(x|C_1)=P(x_1|C_1)P(x_2|C_1)\dots P(x_k|C_1)</script><p><strong>而这样做的好处是：<script type="math/tex">P(x_i|C_1)</script>遵循一维高斯分布，相较于二维高斯分布计算量大大降低，而这种方法叫做Naive Bayes Classifier(朴素贝叶斯分类器)。</strong></p>
<h3 id="S-概率密度函数选择"><a href="#S-概率密度函数选择" class="headerlink" title="$\S$概率密度函数选择"></a>$\S$概率密度函数选择</h3><p>在前文中，我们选择了随机变量服从高斯分布，事实上我们可以选择其他分布函数，这是比较随意的，但当随机变量呈现相关性质时，有一些特定的分布效果会更加好。</p>
<ul>
<li>当样本数据x取实数值时，采用高斯分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DN(\mu_{jc},\sigma_{jc})</script></li>
<li>当每种特征<script type="math/tex">x_j\in\{0,1\}</script>时，采用伯努利分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DBer(\mu_{jc},\sigma_{jc})</script></li>
<li>当每种特征取值<script type="math/tex">x_j\in\{1,2,3,\dots,k\}</script>时，采用分类分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DCat(\mu_{jc},\sigma_{jc})</script></li>
</ul>
<h2 id="Logistic-Regression-逻辑回归"><a href="#Logistic-Regression-逻辑回归" class="headerlink" title="Logistic Regression(逻辑回归)"></a>Logistic Regression(逻辑回归)</h2><p>在上节中，我们利用了贝叶斯公式，现在对贝叶斯公式进行一些变换：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\frac{1}{1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}</script><p>令<script type="math/tex">z=\ln\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}</script>，则有：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\frac{1}{1+\mathrm{e}^{-z}}=\sigma(z)</script><p>而<script type="math/tex">\sigma(z)</script>称为sigmoid function，其函数图像如下所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201120170942.png" style="zoom:67%;"></p>
<p>可以发现，<strong>sigmoid函数的值域在<script type="math/tex">(0,1)</script>之间，而它通常用于隐层神经元输出，可作为激活函数。</strong></p>
<h3 id="S-手推公式（可跳过）"><a href="#S-手推公式（可跳过）" class="headerlink" title="$\S$手推公式（可跳过）"></a>$\S$手推公式（可跳过）</h3><p>前文中，令<script type="math/tex">z=\ln\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}</script>，对其进行展开：</p>
<script type="math/tex; mode=display">
z=\ln\frac{P(x|C_1)}{P(x|C_2)}+\ln\frac{P(C_1)}{P(C_2)}\implies \ln\frac{P(x|C_1)}{P(x|C_2)}+\ln\frac{\frac{N_1}{N_1+N_2}}{\frac{N_2}{N_1+N_2}}</script><script type="math/tex; mode=display">
P(x|c_1)=\frac{1}{(2\pi)^{\frac{n}{2}}|B^1|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T}</script><script type="math/tex; mode=display">
P(x|c_2)=\frac{1}{(2\pi)^{\frac{n}{2}}|B^2|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T}</script><script type="math/tex; mode=display">
\begin{align*}
\ln\frac{\frac{1}{(2\pi)^{\frac{n}{2}}|B^1|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T}}{\frac{1}{(2\pi)^{\frac{n}{2}}|B^2|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T}}
&=\ln\frac{|B^2|^\frac{1}{2}}{|B^1|^\frac{1}{2}}\exp\{-\frac{1}{2}[(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T-(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T]\}\\
&=\ln\frac{|B^2|^\frac{1}{2}}{|B^1|^\frac{1}{2}}-\frac{1}{2}[(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T-(x-\mu^2)(B^2)^{-1}(x-\mu^2)^T]
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}
(x-\mu^1)(B^1)^{-1}(x-\mu^1)^T
&=(x-\mu^1)(B^1)^{-1}(x^T-(\mu^1)^T)\\
&=x(B^1)^{-1}x^T-x(B^1)^{-1}(\mu^1)^T-\mu^1(B^1)x^T+\mu^1(B^1)(\mu^1)^{T}\quad B^1\mbox{为对称矩阵}\\
&=x(B^1)^{-1}x^T-2\mu^1(B^1)x^T+\mu^1(B^1)(\mu^1)^{T}
\end{align*}</script><script type="math/tex; mode=display">
z=\ln\frac{|B^2|^\frac{1}{2}}{|B^1|^\frac{1}{2}}-\frac{1}{2}x(B^1)^{-1}x^T+\mu^1(B^1)^{-1}x^T-\frac{1}{2}\mu^1(B^1)^{-1}(\mu^1)^{T}+\frac{1}{2}x(B^2)^{-1}x^T-\mu^2(B^2)^{-1}x^T+\frac{1}{2}\mu^2(B^2)^{-1}(\mu^2)^{T}+\ln\frac{N_1}{N_2}</script><p>若假设<script type="math/tex">B^1=B^2=B</script>，则有：</p>
<script type="math/tex; mode=display">
\begin{align*}
z&=(\mu^1-\mu^2)B^{-1}x^T-\frac{1}{2}\mu^1B^{-1}(\mu^1)^{T}+\frac{1}{2}\mu^2B^{-1}(\mu^2)^{T}+\ln\frac{N_1}{N_2}\\
&=wx^T+b
\end{align*}</script><p>综上所述：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(w\cdot x+b)</script><h3 id="S-Function-set-函数集"><a href="#S-Function-set-函数集" class="headerlink" title="$\S$Function set(函数集)"></a>$\S$Function set(函数集)</h3><p>由以上公式推导可以得到:</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(z),\ z=w\cdot x+b=\sum_iw_ix_i+b</script><p>可以发现，<script type="math/tex">z</script>可用<script type="math/tex">\sum_iw_ix_i+b</script>进行线性表示。其中<script type="math/tex">w_i</script>是每个<script type="math/tex">x_i</script>的权重，<script type="math/tex">b</script>即为偏差，在这里<script type="math/tex">w\cdot x+b</script>即为决策边界，而这也是上文中为什么<strong>当两个class公用相同的协方差矩阵后，其决策边界变为直线的缘故</strong>。如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201121132947.png" style="zoom:67%;"></p>
<p><strong>这种函数集的分类问题叫做logistic regression(逻辑回归)。</strong></p>
<h3 id="S-Goodness-of-a-Function"><a href="#S-Goodness-of-a-Function" class="headerlink" title="$\S$Goodness of a Function"></a>$\S$Goodness of a Function</h3><p>假设现在有以下的training data:</p>
<script type="math/tex; mode=display">
\begin{align*}
data:&\ x^1 \ \ x^2 \ \ x^3 \dots x^n\\
class:&\ C_1 \ C_1 \ C_2 \dots C_1
\end{align*}</script><p>令<script type="math/tex">f_{w,b}(x)=P(C_1|x)</script>，则由极大似然估计，可定义以下损失函数：</p>
<script type="math/tex; mode=display">
L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\dots f_{w,b}(x^n)</script><p>而目标即为找到最大的<script type="math/tex">L(w,b)</script>，此时该函数的参数即为最佳参数值。</p>
<script type="math/tex; mode=display">
w^*,b^*=arg\ \ max \ L(w,b)</script><p>变形可得：</p>
<script type="math/tex; mode=display">
w^*,b^*=arg\ \ min \ -\ln L(w,b)</script><p>这样做的好处是，可以<strong>利用梯度下降算法求最小值，并且对<script type="math/tex">L(w,b)</script>取对数，可将乘积形式化为求和形式，易于后续计算。</strong></p>
<script type="math/tex; mode=display">
-\ln L(w,b)=-\ln f_{w,b}(x^1)-\ln f_{w,b}(x^2)-\ln[1- f_{w,b}(x^3)]\dots-\ln f_{w,b}(x^n)</script><p><strong>对class1，class2进行符号转换：</strong></p>
<script type="math/tex; mode=display">
class1:\widehat{y}^1=1,\widehat{y}^2=1\ \ class2:\widehat{y}^3=0\ \cdots</script><p>则：</p>
<script type="math/tex; mode=display">
-\ln f_{w,b}(x^1)\implies -[\widehat{y}^1\ln f(x^1)+(1-\widehat{y}^1)\ln (1-f(x^1))]</script><p>这样每个<script type="math/tex">-\ln f_{w,b}(x^i)</script>都可以统一成上式:</p>
<script type="math/tex; mode=display">
-\ln L(w,b)=\sum_i\color{orange}-[\widehat{y}^i\ln f(x^i)+(1-\widehat{y}^i)\ln (1-f(x^i))]</script><p>而橙色部分其实就是<strong>两个伯努利分布的交叉熵（Cross entropy)。</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201121154624.png" style="zoom:67%;"></p>
<p>如图所示，假设有两个分布：<script type="math/tex">p,q</script>如蓝色框所示，那么交叉熵的计算方式即$H(p,q)=-\sum_xp(x)ln(q(x))$。交叉熵代表的含义就是<strong>这两个分布有多接近</strong>。当两个分布相同时，计算的交叉熵就是熵。</p>
<h3 id="S-Find-the-best-function"><a href="#S-Find-the-best-function" class="headerlink" title="$\S$Find the best function"></a>$\S$Find the best function</h3><p>梯度下降求最小，计算步骤如图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201123003816.png" style="zoom:67%;"></p>
<p>对$\ln L(w,b)$求$w_i$的偏微分，只需要计算出$\frac{\ln f_{w,b}(x^n)}{\partial w_i}$与$\frac{\ln (1-f_{w,b}(x^n))}{\partial w_i}$。$f_{w,b}(x)$即为sigmoid 函数，$z=\sum_nw_ix_i+b$。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201123005251.png" style="zoom:67%;"></p>
<p>计算可得到$\frac{\ln (1-f_{w,b}(x^n))}{\partial w_i}$，将两个偏微分计算带入可得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
-\frac{\ln L(w,b)}{\partial w_i}&=\sum_n-[\widehat{y}^n(1-f_{w,b}(x^n_i))x^n_i+(1-\widehat{y}^n)f_{w,b}(x^n_i)x^n_i]\\
&=\sum_n-{\color{purple}(\widehat{y}^n-f_{w,b}(x^n))}x^n_i
\end{align*}</script><p>紫色部分其实直观的表示了真实数据值与函数值之间的差距大小。</p>
<h3 id="S-逻辑回归与线性回归的比较"><a href="#S-逻辑回归与线性回归的比较" class="headerlink" title="$\S$逻辑回归与线性回归的比较"></a>$\S$逻辑回归与线性回归的比较</h3><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201123011427.png" style="zoom:67%;"></p>
<p>可以发现，<strong>逻辑回归其实是在线性回归的基础上，对原线性模型再加上一层sigmoid 函数，从而将函数的输出介于(0,1)之间。而线性回归其输出可为任意值。</strong></p>
<p>在Logistic regression中，<strong>target的值为0或1</strong>，而在Linear regression中，<strong>target的值可为任意值</strong>。</p>
<h3 id="S-Discriminative-V-S-Generative-判别模型VS生成模型"><a href="#S-Discriminative-V-S-Generative-判别模型VS生成模型" class="headerlink" title="$\S$Discriminative V.S. Generative(判别模型VS生成模型)"></a>$\S$Discriminative V.S. Generative(判别模型VS生成模型)</h3><p>在前文中，其实已经介绍了两种不同的方法进行分类。</p>
<ol>
<li>通过假设概率密度函数求解类条件概率，从而通过贝叶斯公式分类，而这种分类方法就叫做生成模型(Generative model)。</li>
<li>通过利用梯度下降算法求解$w,b$。于是可带入逻辑回归模型进行分类，而这种分类方法就叫做判别模型(Discriminative model)。</li>
</ol>
<p>而判别模型和生成模型有什么区别呢?可以用一个例子来说明：</p>
<ul>
<li>判别模型：要确定一只羊是🐐还是🐏，用判别模型的方法是从历史数据中，直接学习到模型，然后通过提取到这只羊的特征带入模型进行预测这只羊是山羊的概率，是绵羊的概率。</li>
<li>生成模型：利用生成模型首先根据山羊的特征学习出山羊的模型，然后根据绵羊的特征学习出绵羊的模型。然后从这只羊中提取特征，放到山羊的模型中，看概率是多少；放到绵羊的模型中，看概率是多少，哪个概率大就是哪个。</li>
</ul>
<p>而对于这两种模型，<strong>它们的函数集都是相同的</strong>：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(wx+b)</script><p>对于判别模型，仅需要利用梯度下降算法，直接求解出参数$w,b$。</p>
<p>对于生成模型，需要通过假设概率分布求解出$\mu_1,\mu_2,B^{-1}$，而在<strong>手推公式</strong>这一节中，有如下代换：</p>
<script type="math/tex; mode=display">
\begin{align*}
w&=(\mu^1-\mu^2)B^{-1}\\
b&=-\frac{1}{2}\mu^1B^{-1}(\mu^1)^{T}+\frac{1}{2}\mu^2B^{-1}(\mu^2)^{T}+\ln\frac{N_1}{N_2}
\end{align*}</script><p>那么这两个不同模型得到的$w,b$相同吗？</p>
<p><strong>其实是不同的</strong>，为什么呢？因为在生成模型中，它是<strong>有假设</strong>的，例如它<strong>假设data满足高斯分布或伯努利分布等等</strong>，但在逻辑回归中，<strong>并没有这些假设</strong>，所以这导致了虽然它们的函数相同却最后计算得到的参数不同。</p>
<p>有些时候生成模型可能会出现一些问题。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201125003150.png" style="zoom:67%;"></p>
<p>例如，在这个例子中，共有13组数据，其中1组为class1，另外12组全部为class2。每组数据均有两个特征。现在当给出一个testing data，问其属于哪个类别。</p>
<p>对于人来说，第一反应它应该就是class1的。那么现在我们看看朴素贝叶斯分类器(naive bayes)是什么结果。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201125003002.png" style="zoom:67%;"></p>
<p>可以发现最后计算结果$P(C_1|x)&lt;0.5$，那么即朴素贝叶斯分类器认为当前tesing data属于class2，那么为什么会造成这个结果呢？<strong>朴素贝叶斯分类器是有假设这种情况存在的（机器脑补这种可能性）。所以结果和人类直观判断的结果不太一样。</strong></p>
<p>对于这两种模型，它们各自有各自的优势：</p>
<ol>
<li>当data数量较少时，<strong>生成模型受数据影响与判别模型相比，影响较小</strong>，因为它有<strong>自己的假设</strong>，甚至无视一些data。而当data较大时，判别模型的优势就较为明显了，因为当数据越多，<strong>它的误差就可以越小</strong>，从而模型越精确。</li>
<li><strong>当遇到一些data有问题时，而生成模型有做假设，有时候就可以把data中有问题的部分忽略掉。</strong></li>
</ol>
<h3 id="S-Multi-class-Classification-多类别分类"><a href="#S-Multi-class-Classification-多类别分类" class="headerlink" title="$\S$Multi-class Classification(多类别分类)"></a>$\S$Multi-class Classification(多类别分类)</h3><p>假定现在有三个类别，它们分别有自己的weight和bias。如下所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201129213423.png" style="zoom:67%;"></p>
<p>把$z_1,z_2,z_3$带入Softmax function​中，最后可得到输出$y_1,y_2,y_3$。可以发现$z_1,z_2,z_3$在Softmax中首先进行指数化，得到$e^{z_1},e^{z_2},e^{z_3}$，后除以$\sum_{j=1}^{3}e^{z_j}$，便可得到输出值。</p>
<p>总结可得Softmax function的函数形式即：</p>
<script type="math/tex; mode=display">
Softmax(z_i)=\frac{e^{z_i}}{\sum_{c=1}^{C}e^{c}}</script><p>那么为什么叫做Softmax呢？从字面意思上看，有soft和max，而max就是最大的意思，而Softmax的核心在于soft。通常情况下，给定一组数，我们要求它的最大值，而这里的最大值就是hardmax，而最大值只有一个，即非黑即白。但在多类别分类中，我们希望找出x的最大概率分类，而这并不能仅仅是一个hardmax。而Softmax的含义在于<strong>不再唯一的确定某个最大值，而是为每个输出分类的结果赋予一个概率值，代表属于该分类的可能性。</strong>它可以对最大的值进行强化，这样会导致大的值和小的值的差距会拉大。</p>
<p>也就是说，<strong>Softmax的输出值可以代表x属于每个类别的概率，即近似表示后验概率。</strong>为什么呢？当假设有三个Class服从高斯分布，当它们公用一个协方差矩阵时，就可以推导得到Softmax。</p>
<h3 id="S-Limitation-of-Logistic-Regression"><a href="#S-Limitation-of-Logistic-Regression" class="headerlink" title="$\S$Limitation of Logistic Regression"></a>$\S$Limitation of Logistic Regression</h3><p>假定现在一组数据有两个feature，则根据Logistic regression则有以下过程：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130191550.png" style="zoom:50%;"></p>
<p>对$x_1,x_2$分别赋予weight和bias，得到$z$带入sigmiod函数便可得到输出的概率值，根据$y$的值从而进行分类。这看似是没有任何问题的，但若现在有下面四组数据：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130192029.png" style="zoom: 67%;"></p>
<p>对两个feature分别赋予0,1，并且已经知道了它们各自的类别。现在我们可以把这四组数据当作四个点，表示在二维平面上。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130192414.png" style="zoom: 50%;"></p>
<p>观察图可以发现，蓝色的点就是属于Class 2的数据，红色的点就是属于Class 1的数据。<strong>如果用逻辑回归处理数据的话，根据逻辑回归的物理意义，它的分界面就是一条直线，使得这两组Class分开，从而达到分类的效果</strong>。</p>
<p>但很明显，观察上图可以发现，不存在这样的直线，能够使红点和蓝点分开。那么如何才能让它们可以用逻辑回归处理呢？</p>
<h3 id="S-Feature-Transformation-特征变换"><a href="#S-Feature-Transformation-特征变换" class="headerlink" title="$\S$Feature Transformation(特征变换)"></a>$\S$Feature Transformation(特征变换)</h3><p>所谓特征变换就是让特征通过某种变换变成新的特征。举个例子，现在如果有定义两个特征：$x_1,x_2$，它们经过某种变换变为新特征：$x_1^{’},x_2^{‘}$。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130195306.png" style="zoom:50%;"></p>
<p>现在假若定义如下变换：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130200034.png" style="zoom:67%;"></p>
<p>那么可以原来二维平面的坐标点可变为新平面下的坐标点：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201130200631.png" style="zoom:67%;"></p>
<p>可以发现，在新平面下，原来不可分的点，现在是可分的了。这是为什么呢？</p>
<p>实际上，在通常情况下，我们的数据不一定是完全线性可分的，很多情况下会存在线性不可分，如同上面所举例子。而我们要将数据处理成线性可分，<strong>则需要进行特征变换，将数据投影到别的空间</strong>，例如上图，数据在$X$空间不可分，但在$X^{’}$空间就是可分的。<strong>而$X^{’}$空间的维度一般是高于$X$空间的维度的。</strong></p>
<p>但有时候很难去寻找有效的特征变换，于是能否有一种通用的方法进行特征变换？</p>
<h3 id="S-Cascading-logistic-regression-models-级联逻辑回归模型"><a href="#S-Cascading-logistic-regression-models-级联逻辑回归模型" class="headerlink" title="$\S$Cascading logistic regression models(级联逻辑回归模型)"></a>$\S$Cascading logistic regression models(级联逻辑回归模型)</h3><p>级联逻辑回归模型从字面意思上就可以看出就是将多个逻辑回归连接起来。而它就是为了解决上述特征变换的问题。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201201133501.png" style="zoom:67%;"></p>
<p>原理过程如图所示，由特征变换和分类两个过程组成。特征$x_1,x_2$作为输入，带入逻辑回归模型，可得到输出$x_1^{’},x_2^{‘}$，而这便是经过逻辑回归得到的两个新的特征。将新特征再作为输入带入逻辑回归模型，可得到分类概率$y$。</p>
<p>下面还是用一个例子来说明。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201201161411.png" style="zoom:67%;"></p>
<p>四组数据，通过调整参数$w_1,w_2,w_3,w_4,b_1,b_2$，可以得到四组数据经过逻辑回归的新特征$x_1^{’},x_2^{‘}$。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201201161240.png" style="zoom:67%;"></p>
<p>将新特征$x_1^{’},x_2^{‘}$带入新逻辑回归模型中，可以发现，新特征新特征$x_1^{’},x_2^{‘}$是线性可分的，由图可知，存在一条直线，将两组Class分开。</p>
<p>至此我们可以引入一个新的概念！</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201201163205.png" style="zoom:67%;"></p>
<p>由以上可知，<strong>一个逻辑回归的输入可以来源于其他逻辑回归的输出，这个逻辑回归的输出也可以是其他逻辑回归的输入，将每一个逻辑回归称为Neuron（神经元），把这些神经元连接起来的网络叫做Neuron Network（神经网络）！</strong></p>
]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch实现回归</title>
    <url>/2021/01/06/Regression/</url>
    <content><![CDATA[<h1 id="实现回归"><a href="#实现回归" class="headerlink" title="实现回归"></a>实现回归</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>首先是一些基础包的导入</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment">#sns.set()可以设置绘图格式</span></span><br><span class="line">sns.set()</span><br><span class="line">sns.set_context(<span class="string">&quot;notebook&quot;</span>, rc=&#123;<span class="string">&quot;font.size&quot;</span>:<span class="number">16</span>,</span><br><span class="line"><span class="string">&quot;axes.titlesize&quot;</span>:<span class="number">20</span>,</span><br><span class="line"><span class="string">&quot;axes.labelsize&quot;</span>:<span class="number">18</span>&#125;)</span><br><span class="line">CB91_Blue = <span class="string">&#x27;#2CBDFE&#x27;</span></span><br><span class="line">CB91_Green = <span class="string">&#x27;#47DBCD&#x27;</span></span><br><span class="line">CB91_Pink = <span class="string">&#x27;#F3A0F2&#x27;</span></span><br><span class="line">CB91_Purple = <span class="string">&#x27;#9D2EC5&#x27;</span></span><br><span class="line">CB91_Violet = <span class="string">&#x27;#661D98&#x27;</span></span><br><span class="line">CB91_Amber = <span class="string">&#x27;#F5B14C&#x27;</span></span><br><span class="line">color_list = [CB91_Blue, CB91_Pink, CB91_Green, CB91_Amber, CB91_Purple, CB91_Violet]</span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.prop_cycle&#x27;</span>] = plt.cycler(color=color_list)</span><br></pre></td></tr></table></figure>
<p>产生随机数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=np.arange(<span class="number">20</span>)</span><br><span class="line">y=np.array([<span class="number">5</span>*x[i]+random.randint(<span class="number">1</span>,<span class="number">20</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x))])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x)</span><br><span class="line">print(y)</span><br><span class="line"><span class="comment">#输出如下</span></span><br><span class="line">[ <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> <span class="number">16</span> <span class="number">17</span> <span class="number">18</span> <span class="number">19</span>]</span><br><span class="line">[  <span class="number">2</span>  <span class="number">20</span>  <span class="number">24</span>  <span class="number">24</span>  <span class="number">30</span>  <span class="number">43</span>  <span class="number">40</span>  <span class="number">55</span>  <span class="number">45</span>  <span class="number">48</span>  <span class="number">66</span>  <span class="number">73</span>  <span class="number">80</span>  <span class="number">76</span>  <span class="number">75</span>  <span class="number">79</span>  <span class="number">88</span> <span class="number">104</span> <span class="number">100</span> <span class="number">107</span>]</span><br></pre></td></tr></table></figure>
<p>作图如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(x,y)</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106211206.png" alt="img"></p>
<p>训练数据，首先要转换为Tensor类型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_train = t.from_numpy(x).float() </span><br><span class="line">y_train = t.from_numpy(y).float()</span><br><span class="line"></span><br><span class="line">print(x_train)</span><br><span class="line">print(y_train)</span><br><span class="line"><span class="comment">#打印输出后的结果</span></span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>,</span><br><span class="line">        <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>])</span><br><span class="line">tensor([  <span class="number">2.</span>,  <span class="number">20.</span>,  <span class="number">24.</span>,  <span class="number">24.</span>,  <span class="number">30.</span>,  <span class="number">43.</span>,  <span class="number">40.</span>,  <span class="number">55.</span>,  <span class="number">45.</span>,  <span class="number">48.</span>,  <span class="number">66.</span>,  <span class="number">73.</span>,</span><br><span class="line">         <span class="number">80.</span>,  <span class="number">76.</span>,  <span class="number">75.</span>,  <span class="number">79.</span>,  <span class="number">88.</span>, <span class="number">104.</span>, <span class="number">100.</span>, <span class="number">107.</span>])</span><br></pre></td></tr></table></figure>
<p>定义训练模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>(<span class="params">t.nn.Module</span>):</span>  <span class="comment">#继承父类:t.nn.Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(LinearRegression,self).__init__() </span><br><span class="line">        self.linear = t.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br></pre></td></tr></table></figure>
<p>super()函数作用：<strong>首先找到父类t.nn.Module,后将类LinearRegression的对象self转换为t.nn.Module的对象,然后“被转换”的类nn.Module对象调用自己的__init__函数.</strong></p>
<p>进行模型的训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = LinearRegression()</span><br><span class="line">criterion = t.nn.MSELoss()  <span class="comment">#定义损失函数</span></span><br><span class="line">optimizer = t.optim.SGD(model.parameters(),<span class="number">0.001</span>)  <span class="comment">#利用SGD优化算法,首先传入需要优化的参数,与学习率</span></span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">1000</span>   <span class="comment">#迭代次数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    input_data = x_train.unsqueeze(<span class="number">1</span>)  <span class="comment">#输入数据进行升维:[20]---&gt;[20,1]</span></span><br><span class="line">    target = y_train.unsqueeze(<span class="number">1</span>)  </span><br><span class="line">    out = model(input_data)   <span class="comment">#模型输出</span></span><br><span class="line">    loss = criterion(out,target) <span class="comment">#将模型输出与target进行比较,计算loss</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment">#梯度信息置为0,作用:将前一个batch的梯度计算结果置为0,因为它没有保留的作用了</span></span><br><span class="line">    loss.backward()   <span class="comment">#利用反向传播计算梯度</span></span><br><span class="line">    optimizer.step()  <span class="comment">#进行单次的优化,参数的更新</span></span><br><span class="line">    <span class="comment">#print(&quot;Epoch:[&#123;&#125;/&#123;&#125;],loss:[&#123;:.4f&#125;]&quot;.format(i+1,num_epochs,loss.item()))  </span></span><br><span class="line">    <span class="comment">#loss.item()可以直接获得loss的值,loss是仅有一个量的张量,故用item()可转换为python标量</span></span><br><span class="line">    <span class="keyword">if</span> ((i+<span class="number">1</span>)%<span class="number">200</span>==<span class="number">0</span>):   </span><br><span class="line">        predict = model(input_data)</span><br><span class="line">        plt.plot(x_train.data.numpy(),predict.squeeze(<span class="number">1</span>).data.numpy())</span><br><span class="line">        loss=criterion(predict,target)</span><br><span class="line">        plt.title(<span class="string">&quot;Loss:&#123;:.4f&#125;&quot;</span>.format(loss.item()))</span><br><span class="line">        plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&quot;Y&quot;</span>)</span><br><span class="line">        plt.scatter(x_train,y_train)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<p>进行1000次迭代，每两百次打印图片</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106213013.png"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106213057.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106213121.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106213202.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210106213241.png" alt></p>
<h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>线性回归虽然可以拟合出来一条直线，但精度较低，可以看到上图的线性回归的Loss的值为<strong>43.2122</strong>，是较高的。我们可以通过多项式回归提高模型精度，多项式回归就是提高特征的次数，如前面的线性回归的$x$是1次，实际上可以采用二次、三次的，增加模型的复杂度，可能会带来overfitting。</p>
<h3 id="Example-Ⅰ"><a href="#Example-Ⅰ" class="headerlink" title="Example Ⅰ"></a>Example Ⅰ</h3><p>下面实现一个简单的多项式回归，用模型拟合一个复杂的多项式方程：</p>
<script type="math/tex; mode=display">
f(x)=-1.13x-2.14x^2+3.15x^3-0.01x^4+0.512</script><p>多项式数据准备：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在[-3,3]之间生成50个点,返回一个一维张量</span></span><br><span class="line">x = t.linspace(<span class="number">-3</span>,<span class="number">3</span>,<span class="number">50</span>)  </span><br><span class="line">y = <span class="number">-1.13</span>*x<span class="number">-2.14</span>*t.pow(x,<span class="number">2</span>)+<span class="number">3.15</span>*t.pow(x,<span class="number">3</span>)<span class="number">-0.01</span>*t.pow(x,<span class="number">4</span>)+<span class="number">0.512</span></span><br><span class="line">plt.scatter(x.data.numpy(),y.data.numpy())</span><br></pre></td></tr></table></figure>
<p>借助公式随机产生50个点，可视化图像如下：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210113232837.png" alt></p>
<p>由于现在的输入不再是一元函数的一维，而是现在的四维，输入变成一个矩阵的形式：</p>
<script type="math/tex; mode=display">
X=\left[\array{x_1^1 & \dots &x_1^4\\
\vdots & \ddots & \vdots\\
x_n^1 & \dots & x_n^4}\right]</script><p>其中$x_n^4$代表第n组样本的第四个feature。</p>
<p>下面将数据拼接成如上所示的矩阵形式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">features</span>(<span class="params">x</span>):</span></span><br><span class="line">    x = x.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> t.cat([x**i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>)],<span class="number">1</span>)   <span class="comment">#将各向量连接在一起,形成矩阵形式</span></span><br></pre></td></tr></table></figure>
<p>现在得到了标准的输入矩阵$X$，还缺少$y$的值，而$y$是通过函数$f(x)$计算出来的，通过以下函数来计算得到$y$值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_weight = t.Tensor([<span class="number">-1.13</span>,<span class="number">-2.14</span>,<span class="number">3.15</span>,<span class="number">-0.01</span>])</span><br><span class="line">x_weight = x_weight.unsqueeze(<span class="number">1</span>)</span><br><span class="line">b = t.Tensor([<span class="number">0.512</span>])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">target</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x.mm(x_weight)+b.item()   <span class="comment">#矩阵相乘,表示x*x_weight,b,item()获取b标量中的数值</span></span><br></pre></td></tr></table></figure>
<p>上面的代码用到了Tensor的mm方法，它是表示矩阵相乘（Matrix Multiplication）。现在通过上面两个方法，批量生成用于训练的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 新建一个随机生成输入数据和输出数据的函数，用于生成train data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span>(<span class="params">batch_size</span>):</span></span><br><span class="line">    <span class="comment"># 生成batch个随机的x</span></span><br><span class="line">    batch_x = t.randn(batch_size)</span><br><span class="line">    feature_x = features(batch_x)</span><br><span class="line">    target_y = target(feature_x)</span><br><span class="line">    <span class="keyword">return</span> feature_x,target_y</span><br></pre></td></tr></table></figure>
<p>下面创建多项式回归模型，使用torch.nn.Linear模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PloynomialRegression</span>(<span class="params">t.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(PloynomialRegression,self).__init__()</span><br><span class="line">        self.ploy = t.nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 输入维度为4维，输出维度为1维</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.ploy(x)</span><br></pre></td></tr></table></figure>
<p>模型新建好后开始训练模型，为了动态的显示模型训练结果，在程序中设置每1000个epoch，就对测试数据进行一次预测，并将预测的误差及预测的输出值可视化显示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 开始训练模型，误差函数使用MSELoss，使用SGD优化参数</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">model = PloynomialRegression()</span><br><span class="line">criterion = t.nn.MSELoss()</span><br><span class="line">optimizer = t.optim.SGD(model.parameters(),<span class="number">0.001</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    batch_x,batch_y = get_data(batch_size)</span><br><span class="line">    out = model(batch_x)</span><br><span class="line">    loss = criterion(out,batch_y)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">if</span>(epoch%<span class="number">200</span>==<span class="number">0</span>):</span><br><span class="line">        predict = model(features(x))</span><br><span class="line">        plt.plot(x.data.numpy(),predict.squeeze(<span class="number">1</span>).data.numpy(),CB91_Pink)</span><br><span class="line">        loss = criterion(predict,y.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        plt.title(<span class="string">&quot;Loss:&#123;:.4f&#125;&quot;</span>.format(loss.item()))</span><br><span class="line">        plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&quot;Y&quot;</span>)</span><br><span class="line">        plt.scatter(x,y)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210114151321.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210114151343.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210114151403.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210114151420.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210114151445.png" alt></p>
<p>经过大量的迭代后，模型能够很好的拟合测试数据。</p>
<h3 id="Example-Ⅱ"><a href="#Example-Ⅱ" class="headerlink" title="Example Ⅱ"></a>Example Ⅱ</h3><p>实现更复杂的曲线拟合，这里我们对心形曲线进行拟合，函数表达式如下：</p>
<script type="math/tex; mode=display">
\begin{cases}
x=16sin^3(t)\\
y=13cos(t)-5cos(2t)-2cos(3t)-cos(4t)
\end{cases}</script><p>首先借助公式随机生成10000个点：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">e = t.linspace(<span class="number">-15</span>,<span class="number">15</span>,<span class="number">10000</span>)  </span><br><span class="line">x1 = <span class="number">16</span>*t.sin(e)**<span class="number">3</span></span><br><span class="line">y1 = <span class="number">13</span>*t.cos(e)<span class="number">-5</span>*t.cos(<span class="number">2</span>*e)<span class="number">-2</span>*t.cos(<span class="number">3</span>*e)-t.cos(<span class="number">4</span>*e)</span><br><span class="line">plt.scatter(x1.data.numpy(),y1.data.numpy())</span><br></pre></td></tr></table></figure>
<p>可视化如下：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115182453.png" alt></p>
<p>与Example Ⅰ不同的是，在这里有两个变量，需要对X,Y都要进行预测。</p>
<p>对X，Y两个变量，分别进行数据处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fea_x</span>(<span class="params">e</span>):</span></span><br><span class="line">    e = e.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> t.sin(e)**<span class="number">3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fea_y</span>(<span class="params">e</span>):</span></span><br><span class="line">    e = e.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> t.cat([t.cos(i*e) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>)],<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>有了这两个方法，便可以批量生成X,Y的训练数据了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data</span>(<span class="params">size</span>):</span></span><br><span class="line">    r = t.randn(size)</span><br><span class="line">    x_fea = fea_x(r)</span><br><span class="line">    y_fea = fea_y(r)</span><br><span class="line">    x_tg = <span class="number">16</span>*x_fea</span><br><span class="line">    y_tg = tg_y(y_fea)</span><br><span class="line">    <span class="keyword">return</span> x_fea,y_fea,x_tg,y_tg</span><br></pre></td></tr></table></figure>
<p>接下来就是定义模型，由于有两个变量，需要分别进行预测，故对X,Y定义两个模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HeartModel_x</span>(<span class="params">t.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(HeartModel_x,self).__init__()</span><br><span class="line">        self.heartx = t.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.heartx(x)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HeartModel_y</span>(<span class="params">t.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(HeartModel_y,self).__init__()</span><br><span class="line">        self.hearty = t.nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span>  <span class="comment">#self必须是任意函数的首个参数</span></span><br><span class="line">        <span class="keyword">return</span> self.hearty(x)</span><br></pre></td></tr></table></figure>
<p>最后，进行模型训练，与Example Ⅰ大致相同，不再赘述，不同的是同时对X，Y进行训练：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epoachs = <span class="number">50000</span></span><br><span class="line">batch_size = <span class="number">10000</span></span><br><span class="line">model_x = HeartModel_x()</span><br><span class="line">model_y = HeartModel_y()</span><br><span class="line">criterion = t.nn.MSELoss()</span><br><span class="line">optimizer1 = t.optim.SGD(model_x.parameters(),<span class="number">0.001</span>)</span><br><span class="line">optimizer2 = t.optim.SGD(model_y.parameters(),<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoach <span class="keyword">in</span> range(epoachs):</span><br><span class="line">    x_train,y_train,x_target,y_target = data(batch_size)</span><br><span class="line">    out_x = model_x(x_train)</span><br><span class="line">    out_y = model_y(y_train)</span><br><span class="line">    </span><br><span class="line">    loss_x = criterion(out_x,x_target)</span><br><span class="line">    loss_y = criterion(out_y,y_target)</span><br><span class="line">    </span><br><span class="line">    optimizer1.zero_grad()</span><br><span class="line">    optimizer2.zero_grad()</span><br><span class="line">    </span><br><span class="line">    loss_x.backward()</span><br><span class="line">    loss_y.backward()</span><br><span class="line">    </span><br><span class="line">    optimizer1.step()</span><br><span class="line">    optimizer2.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(epoach%<span class="number">10000</span>==<span class="number">0</span>):</span><br><span class="line">        predict_x = model_x(fea_x(e))</span><br><span class="line">        predict_y = model_y(fea_y(e))</span><br><span class="line">        plt.plot(predict_x.squeeze(<span class="number">1</span>).data.numpy(),predict_y.squeeze(<span class="number">1</span>).data.numpy(),CB91_Pink)</span><br><span class="line">        loss_x = criterion(predict_x,x1.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        loss_y = criterion(predict_y,y1.unsqueeze(<span class="number">1</span>))</span><br><span class="line">        plt.title(<span class="string">&quot;Loss X:&#123;:.4f&#125;,Loss Y:&#123;:.4f&#125;&quot;</span>.format(loss_x,loss_y))</span><br><span class="line">        plt.xlabel(<span class="string">&quot;X&quot;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&quot;Y&quot;</span>)</span><br><span class="line">        plt.scatter(x1,y1)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
<p>每10000次迭代打印一次训练结果，并将其可视化如下：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115213159.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115213309.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115213335.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115213400.png" alt></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210115213426.png" alt></p>
]]></content>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch问题记录</title>
    <url>/2021/03/22/pytorch%20problem/</url>
    <content><![CDATA[<h1 id="PyTorch-中torch-nn-与torch-nn-function-的区别"><a href="#PyTorch-中torch-nn-与torch-nn-function-的区别" class="headerlink" title="PyTorch 中torch.nn. 与torch.nn.function.的区别"></a>PyTorch 中torch.nn. 与torch.nn.function.的区别</h1><p>今天我在定义一个卷积网络的过程中，发现nn.Conv2d与nn.functional.conv2d看似都是相同的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogCat_Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(Dog_Cat,self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size = <span class="number">7</span>,dilation = <span class="number">2</span>)</span><br><span class="line">        self.conv1 = nn.functional.conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size = <span class="number">7</span>,dilation = <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>类似的还有很多，在卷积，激活，池化等操作中，nn.Conv2d与nn.functional.conv2d均很相似。</p>
<p>通过查看PyTorch的文档发现，例如torch.nn.Conv2d是一个类</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210322202215.png" style="zoom: 50%;"></p>
<p>torch.nn.functional.conv2d则更像一个函数：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210322202712.png" style="zoom:50%;"></p>
<p>简而言之：</p>
<ul>
<li><strong>nn.Conv2d可以理解为一个Class类，需要继承nn.Module类</strong></li>
<li><strong>nn.functional.conv2d可以理解为一个纯函数，由def function(input)定义</strong></li>
</ul>
<p>对于nn中定义的类，可以提取变化的学习参数。而nn.functional中的是函数，是一个固定的运算公式。</p>
<p>由于在深度学习中会有很多权重是不断变化的，所以需要采用类的方式，以确保参数改变后，仍能正确进行运算。</p>
<p><strong>所以一般在建立网络模型时，在__init__函数中，通常使用nn中的类；而在forward函数中，通常使用nn.functional中的函数。</strong></p>
<p>例如以下网络模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog_Cat</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        super(Dog_Cat,self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size = <span class="number">7</span>,dilation = <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span> ,kernel_size = <span class="number">5</span>)</span><br><span class="line">        self.dropout = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">55696</span>,<span class="number">1000</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">1000</span>,<span class="number">50</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">50</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x),<span class="number">2</span>))</span><br><span class="line">        x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)),<span class="number">2</span>))</span><br><span class="line">        x = x.view(<span class="number">-1</span>,<span class="number">55696</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.dropout(x)</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = F.dropout(x)</span><br><span class="line">        x = F.log_softmax(self.fc3(x),dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>当网络模型使用了Dropout层时，其只需要在模型训练的时候进行使用，在模型测试时并不会使用它。</p>
<p><strong>当Dropout层是使用nn.Dropout定义时，在模型训练时，可以通过model.train()开启它，在模型测试时，可以通过model.eval()关闭它。</strong></p>
<p><strong>但当Dropout层是用nn.functional.dropout定义的，那么在使用model.eval()后，也没办法关闭Dropout层。</strong></p>
<p>以上提到了model.train()与model.eval()，那么它们的具体作用是什么呢？为什么要在模型训练与测试的时候加上这两段代码呢？</p>
<h3 id="model-train"><a href="#model-train" class="headerlink" title="model.train()"></a>model.train()</h3><p>如果模型中有BN层(Batch Normalization）和Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的<strong>均值</strong>和<strong>方差</strong>。对于Dropout，<strong>model.train()是随机取一部分网络连接来训练更新参数。</strong></p>
<h3 id="model-eval"><a href="#model-eval" class="headerlink" title="model.eval()"></a>model.eval()</h3><p>如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即<strong>不进行随机舍弃神经元。</strong></p>
<p>训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，<strong>框架会自动把BN和Dropout固定住，不会取平均，而是用训练好的值</strong>，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的的性质。</p>
<p>而在模型测试中，通常还会看见torch.no_grad()，它有什么作用呢？</p>
<ul>
<li>用于停止autograd模块的工作，起到加速和节省显存的作用（具体行为就是停止gradient计算，从而节省了GPU算力和显存）</li>
<li>不会影响dropout和batchnorm层的行为</li>
</ul>
<p>model.eval()与torch.no_grad()同时用，可以更加节省cpu算力，加速计算。</p>
]]></content>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux学习笔记(ubuntu)</title>
    <url>/2021/04/01/Linux/</url>
    <content><![CDATA[<h1 id="Linux笔记"><a href="#Linux笔记" class="headerlink" title="Linux笔记"></a>Linux笔记</h1><h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><p>对于Linux来说，使用统一的目录树结构：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/bin        二进制文件，系统常规命令</span><br><span class="line">/boot       系统启动分区，系统启动时读取的文件</span><br><span class="line">/dev        设备文件</span><br><span class="line">/etc        大多数配置文件</span><br><span class="line">/home       普通用户的家目录</span><br><span class="line">/lib        32位函数库</span><br><span class="line">/lib64      64位库</span><br><span class="line">/media      手动临时挂载点</span><br><span class="line">/mnt        手动临时挂载点</span><br><span class="line">/opt        第三方软件安装位置</span><br><span class="line">/proc       进程信息及硬件信息</span><br><span class="line">/root       临时设备的默认挂载点</span><br><span class="line">/sbin       系统管理命令</span><br><span class="line">/srv        数据</span><br><span class="line">/var        数据</span><br><span class="line">/sys        内核相关信息</span><br><span class="line">/tmp        临时文件</span><br><span class="line">/usr        用户相关设定</span><br></pre></td></tr></table></figure>
<p>Linux下没有C：D：这种盘符的概念</p>
<p>在\home目录下存放着用户目录</p>
<p>超级用户root，其用户目录为/root</p>
<h2 id="Linux命令行"><a href="#Linux命令行" class="headerlink" title="Linux命令行"></a>Linux命令行</h2><h3 id="命令行含义"><a href="#命令行含义" class="headerlink" title="命令行含义"></a>命令行含义</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">示例：root@app00:~<span class="comment"># </span></span><br><span class="line">root    //用户名，root为超级用户</span><br><span class="line">@       //分隔符</span><br><span class="line">app00   //主机名称</span><br><span class="line">~       //当前所在目录，默认用户目录为~，会随着目录切换而变化，例如：（root@app00:/bin<span class="comment"># ，当前位置在bin目录下）</span></span><br><span class="line"><span class="comment">#       //表示当前用户是超级用户，普通用户为$，例如：（&quot;yao@app00:/root$&quot; ，表示使用用户&quot;yao&quot;访问/root文件夹）</span></span><br></pre></td></tr></table></figure>
<h3 id="命令行的组成"><a href="#命令行的组成" class="headerlink" title="命令行的组成"></a>命令行的组成</h3><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">示例: 命令 参数名 参数值</span></span><br></pre></td></tr></table></figure>
<h3 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h3><h4 id="重启系统"><a href="#重启系统" class="headerlink" title="重启系统"></a>重启系统</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)立刻关机</span><br><span class="line">  shutdown -h now 或者 poweroff</span><br><span class="line">(2)两分钟后关机</span><br><span class="line">  shutdown -h 2</span><br></pre></td></tr></table></figure>
<h4 id="关闭系统"><a href="#关闭系统" class="headerlink" title="关闭系统"></a>关闭系统</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)立刻重启</span><br><span class="line">  shutdown -r now 或者 reboot</span><br><span class="line">(2)两分钟后重启</span><br><span class="line">  shutdown -r 2 </span><br></pre></td></tr></table></figure>
<h4 id="帮助命令"><a href="#帮助命令" class="headerlink" title="帮助命令"></a>帮助命令</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ifconfig --<span class="built_in">help</span>   //查看ifconfig命令的用法</span><br></pre></td></tr></table></figure>
<h4 id="命令说明书（man"><a href="#命令说明书（man" class="headerlink" title="命令说明书（man)"></a>命令说明书（man)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">man shutdown         //打开命令说明后，可按<span class="string">&quot;q&quot;</span>键退出</span><br></pre></td></tr></table></figure>
<h4 id="切换用户"><a href="#切换用户" class="headerlink" title="切换用户"></a>切换用户</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Lehrmann Lehrmanncc   //切换为用户<span class="string">&quot;yao&quot;</span>,输入后回车需要输入该用户的密码</span><br><span class="line"><span class="built_in">exit</span>                 //退出当前用户man shutdown         //打开命令说明后，可按<span class="string">&quot;q&quot;</span>键退出</span><br></pre></td></tr></table></figure>
<h3 id="目录操作"><a href="#目录操作" class="headerlink" title="目录操作"></a>目录操作</h3><h4 id="切换目录-cd"><a href="#切换目录-cd" class="headerlink" title="切换目录(cd)"></a>切换目录(cd)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /                 //切换到根目录</span><br><span class="line"><span class="built_in">cd</span> /bin              //切换到根目录下的bin目录</span><br><span class="line"><span class="built_in">cd</span> ../               //切换到上一级目录 或者使用命令：<span class="built_in">cd</span> ..</span><br><span class="line"><span class="built_in">cd</span> ~                 //切换到home目录</span><br><span class="line"><span class="built_in">cd</span> -                 //切换到上次访问的目录</span><br><span class="line"><span class="built_in">cd</span> xx(文件夹名)       //切换到本目录下的名为xx的文件目录，如果目录不存在报错</span><br><span class="line"><span class="built_in">cd</span> /xxx/xx/x         //可以输入完整的路径，直接切换到目标目录，输入过程中可以使用tab键快速补全</span><br></pre></td></tr></table></figure>
<h4 id="查看目录-ls"><a href="#查看目录-ls" class="headerlink" title="查看目录(ls)"></a>查看目录(ls)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls                   //查看当前目录下的所有目录和文件</span><br><span class="line">ls -a                //查看当前目录下的所有目录和文件（包括隐藏的文件）</span><br><span class="line">ls -l                //列表查看当前目录下的所有目录和文件（列表查看，显示更多信息），与命令<span class="string">&quot;ll&quot;</span>效果一样</span><br><span class="line">ls /bin              //查看指定目录下的所有目录和文件 </span><br></pre></td></tr></table></figure>
<h4 id="创建目录-mkdir"><a href="#创建目录-mkdir" class="headerlink" title="创建目录(mkdir)"></a>创建目录(mkdir)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir tools   			//在当前目录下创建一个名为tools的目录</span><br><span class="line">mkdir /bin/tools		//在指定目录下创建一个名为tools的目录</span><br><span class="line">mkdir -p /abc/123/<span class="built_in">test</span>   //使用-p参数，可以将路径的层次目录全部创建</span><br></pre></td></tr></table></figure>
<h4 id="删除目录与文件-rm"><a href="#删除目录与文件-rm" class="headerlink" title="删除目录与文件(rm)"></a>删除目录与文件(rm)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rm 文件名              //删除当前目录下的文件</span><br><span class="line">rm -f 文件名           //删除当前目录的的文件（不询问）</span><br><span class="line">rm -r 目录名         	//递归删除当前目录下此名的目录</span><br><span class="line">rm -rf 目录名        	//递归删除当前目录下此名的目录（不询问）</span><br><span class="line">rm -rf *              //将当前目录下的所有目录和文件全部删除</span><br><span class="line">rm -rf /*             //将根目录下的所有文件全部删除【慎用！相当于格式化系统】</span><br></pre></td></tr></table></figure>
<h4 id="修改目录-mv"><a href="#修改目录-mv" class="headerlink" title="修改目录(mv)"></a>修改目录(mv)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv 当前目录名 新目录名        //修改目录名，同样适用与文件操作</span><br><span class="line">mv /usr/tmp/tool /opt       //将/usr/tmp目录下的tool目录剪切到 /opt目录下面</span><br><span class="line">mv -r /usr/tmp/tool /opt    //递归剪切目录中所有文件和文件夹</span><br></pre></td></tr></table></figure>
<h4 id="拷贝目录-cp"><a href="#拷贝目录-cp" class="headerlink" title="拷贝目录(cp)"></a>拷贝目录(cp)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cp /usr/tmp/tool /opt       //将/usr/tmp目录下的tool目录复制到 /opt目录下面</span><br><span class="line">cp -rf /usr/tmp/tool /opt    //递归强制复制目录中所有文件和文件夹</span><br></pre></td></tr></table></figure>
<h4 id="搜索目录-find"><a href="#搜索目录-find" class="headerlink" title="搜索目录(find)"></a>搜索目录(find)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find /bin -name <span class="string">&#x27;a*&#x27;</span>  		//查找/bin目录下的所有以a开头的文件或者目录</span><br></pre></td></tr></table></figure>
<h4 id="查看当前目录（pwd）"><a href="#查看当前目录（pwd）" class="headerlink" title="查看当前目录（pwd）"></a>查看当前目录（pwd）</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">pwd</span>                         //显示当前位置路径</span><br></pre></td></tr></table></figure>
<h3 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h3><h4 id="新增文件-touch"><a href="#新增文件-touch" class="headerlink" title="新增文件(touch)"></a>新增文件(touch)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">touch example/test.txt    //在example目录下创建test.txt文件</span><br></pre></td></tr></table></figure>
<h3 id="文件权限"><a href="#文件权限" class="headerlink" title="文件权限"></a>文件权限</h3><h4 id="文件权限说明"><a href="#文件权限说明" class="headerlink" title="文件权限说明"></a>文件权限说明</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">文件权限简介：<span class="string">&#x27;r&#x27;</span> 代表可读（4），<span class="string">&#x27;w&#x27;</span> 代表可写（2），<span class="string">&#x27;x&#x27;</span> 代表执行权限（1），括号内代表<span class="string">&quot;8421法&quot;</span></span><br><span class="line"><span class="comment">##文件权限信息示例：-rwxrw-r--</span></span><br><span class="line">-第一位：<span class="string">&#x27;-&#x27;</span>就代表是文件，<span class="string">&#x27;d&#x27;</span>代表是文件夹</span><br><span class="line">-第一组三位：拥有者的权限</span><br><span class="line">-第二组三位：拥有者所在的组，组员的权限</span><br><span class="line">-第三组三位：代表的是其他用户的权限</span><br></pre></td></tr></table></figure>
<h4 id="文件权限-1"><a href="#文件权限-1" class="headerlink" title="文件权限"></a>文件权限</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">普通授权    chmod +x a.txt    </span><br><span class="line">8421法     chmod 777 a.txt     //1+2+4=7，<span class="string">&quot;7&quot;</span>说明授予所有权限</span><br></pre></td></tr></table></figure>
<h3 id="打包与解压"><a href="#打包与解压" class="headerlink" title="打包与解压"></a>打包与解压</h3><h4 id="压缩文件说明"><a href="#压缩文件说明" class="headerlink" title="压缩文件说明"></a>压缩文件说明</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">.zip、.rar        //windows系统中压缩文件的扩展名</span><br><span class="line">.tar              //Linux中打包文件的扩展名</span><br><span class="line">.gz               //Linux中压缩文件的扩展名</span><br><span class="line">.tar.gz           //Linux中打包并压缩文件的扩展名</span><br></pre></td></tr></table></figure>
<h4 id="打包文件"><a href="#打包文件" class="headerlink" title="打包文件"></a>打包文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -cvf 打包压缩后的文件名 要打包的文件</span><br><span class="line">参数说明： c：打包文件; v：显示运行过程; f：指定文件名;</span><br><span class="line">示例：</span><br><span class="line">tar -cvf a.tar file1 file2,...      //多个文件打包</span><br><span class="line">tar -zcvf a.tar.gz file1            //文件打包并压缩</span><br></pre></td></tr></table></figure>
<h4 id="解压文件"><a href="#解压文件" class="headerlink" title="解压文件"></a>解压文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf a.tar                      //解包至当前目录</span><br><span class="line">tar -zxvf a.tar -C /usr------        //‘C’表示切换目录，指定解压的位置</span><br><span class="line">unzip test.zip             //解压*.zip文件 </span><br><span class="line">unzip -l test.zip          //查看*.zip文件的内容 </span><br></pre></td></tr></table></figure>
<h3 id="软连接"><a href="#软连接" class="headerlink" title="软连接"></a>软连接</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用ln命令来创建软连接</span></span><br><span class="line">ln -s example example3    // 其中-s表示soft软连接（默认为硬）</span><br><span class="line"><span class="comment"># example为原始目录或文件名，example2为软连接名</span></span><br></pre></td></tr></table></figure>
<ol>
<li>删除软连接，对原目录没有任何影响</li>
<li>删除原目录，则软连接失效</li>
</ol>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20210402165356.png" style="zoom:67%;"></p>
<p>可以发现根目录下有较多的软连接，例如bin、lib等，bin其实指向目录user/bin。</p>
<h3 id="用户操作"><a href="#用户操作" class="headerlink" title="用户操作"></a>用户操作</h3><h4 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo useradd -m test1</span><br></pre></td></tr></table></figure>
<p>其中，sudo（super use)表示以管理员身份运行；-m参数表示在/home目录下添加用户目录。</p>
<h4 id="添加密码"><a href="#添加密码" class="headerlink" title="添加密码"></a>添加密码</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo passwd test1</span><br></pre></td></tr></table></figure>
<h4 id="删除用户"><a href="#删除用户" class="headerlink" title="删除用户"></a>删除用户</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo userdel test1</span><br></pre></td></tr></table></figure>
<p>但这个时候虽然已经将test1用户删除了，但在\home目录下仍有\test1用户目录，所以需要将其目录删除</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo rm -rf /home/test1/</span><br></pre></td></tr></table></figure>
<ol>
<li>在登陆系统时，Linux默认不能使用root用户进行登陆</li>
<li>只有特殊的用户才能执行sudo，例如刚才创建的新用户test1就不能使用sudo。而用户lehrmann便可以。在Linux下，把能执行sudo命令的用户叫sudoer。</li>
</ol>
<h3 id="root用户"><a href="#root用户" class="headerlink" title="root用户"></a>root用户</h3><p>首次使用root用户，需要给root用户设置密码(只需要设置一次，需要妥善保管root用户密码)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo passwd root</span><br></pre></td></tr></table></figure>
<p>切换root用户，使用su（switch user)命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">su root</span><br></pre></td></tr></table></figure>
<p>su root仅会切换终端(terminal)用户为root用户，桌面环境并不会切换</p>
<p>退出root用户，</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
</search>
