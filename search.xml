<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Love guarantee</title>
    <url>/2020/11/10/Love/</url>
    <content><![CDATA[<p>我曾以为泽泽和恬恬的故事真的就要结束了。那段时间，每当我想起这个结果时，我的心就好疼，因为对于这段感情，我们都付出了所有，爱的很彻底，没有丝毫保留。我想，青春里的爱情最美好的样子也不过如此吧！</p>
<p>那年我18岁，你19岁，一个机缘巧合，我们坐到了一起，开始了真正意义上的熟识吧。那是一个慵懒的下午，来上星期天的晚自习。当我还沉浸在ysc“背叛”我和别人坐的愤怒时，我看了眼，发现旁边是你，那时心里有个感觉：“你会她发生点什么”，我不知道，也没想很多，只是愤怒已经当然无存了。一开始，我们一句话也不说，后来貌似因为我解答了你的问题疑惑而打破僵局，而这时，我与你的故事已经开始。</p>
<p>我加了你QQ，很紧张，那天夜晚，却发现你拒绝了我，一脸懵逼应该是我当时的写照吧，当然最后误会解除了，也是顺利的加上了你，我们开始了日常的聊天，在学校聊不够，回去聊，QQ聊不够，网易云聊。我们聊了真的好多好多，我也是第一次发现，原来我和一个人能这么聊的来，原来你这么有趣，原来我已经深深被你吸引了。</p>
<p>期间发生的一些事情我已经记不起来了，毕竟当时的我们每天就是吃饭，睡觉，做卷子。但我记得，你当时说了一句话，你向ysc说：“你看看lyc，多有耐心，给我讲题”，那天我的心都在飞扬，因为这是被你第一次夸了。我想ysc当时应该笑而不语吧，因为在一个晚上，我曾像他说：我喜欢ppp，他给我说你可能性不是很大，当时算是给我浇了一盆冷水吧。</p>
<p>当我们逐渐熟识后，我们商量着每次小组换座位都坐一起，我还曾想过“入赘”你们组，每次当我想起来，我都忍不住笑，多么美好的事情啊。还记得坐一起的一个晚自习，当时大家在扔粉笔头，jjy向你仍过来一个粉笔头，每次想到这件事情，我总忍不住要拿出来夸耀一番，因为我下意识就用书替你挡住了，一种本能保护你的反应吧。</p>
<p>——2020年11月10日</p>
<hr>
<p>当记忆被打开，往日的思绪就涌了出来。那是个温暖的午后，阳光洒在大地上，我们在这个时间段上起了体育课。与往常一样，解散之后，我去了乒乓球台，打起了乒乓球，只是我貌似观察到了你们在玩跳绳，忘了是谁的邀请，我也加入了你们的跳绳。有很多人，我们玩着跳绳，可惜我跳的并不好，只能在旁边看着，不过xxc貌似看出来我的窘境，于是她开始嚷嚷让你和我一起拉跳绳，紧接着朋友们也跟着吆喝起来，我看见你很羞涩，此处应该说是（少女的羞涩），虽然我也有点不好意思，最后在朋友们的“撮合”下，我们俩一起拉跳绳，几个小朋友也过来跳，我发誓，那是我人生中拉的最紧张的一次跳绳，甚至刚开始都没能摆起绳子。不过在朋友的指导下，我会拉了哈哈哈哈。</p>
<p>每当想起那个画面，我的脑海里充斥了温暖的感觉，绳子的一端是你，我拉着绳子，那根绳子又何尝不是我们俩的一种联系呢，也许在那个时候已经建立起来了。</p>
<p>——2020年11月12日</p>
<hr>
<p>要问我高三最接近你是什么时候，我想也许只有那天了······（未完待续</p>
]]></content>
      <tags>
        <tag>爱情故事</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenCV 基础操作</title>
    <url>/2020/10/14/opencv/</url>
    <content><![CDATA[<h2 id="图像上的算术运算"><a href="#图像上的算术运算" class="headerlink" title="图像上的算术运算"></a>图像上的算术运算</h2><h3 id="图像融合"><a href="#图像融合" class="headerlink" title="图像融合"></a>图像融合</h3><ul>
<li><p><strong>图像融合即也是图像加法，但它是对每个图像乘以相应的权重，使其具有融合或透明的感觉。根据以下等式进行运算。</strong></p>
<script type="math/tex; mode=display">
G(x)=(1-\alpha)f_1(x)+\alpha f_2(x),\quad \alpha\in[0,1]</script></li>
<li><p><strong>f(x)可作为输入的图像，则上式可变为：</strong></p>
<script type="math/tex; mode=display">
dst=\beta\cdot img_1+\alpha\cdot img_2+\gamma\\
\beta+\alpha=1</script></li>
<li><p><strong>在OpenCV中进行图像融合的函数为cv.addWeighted(img1,$\beta$,img2,$\alpha,\gamma$),其有四个参数,img1与img2分别为输入的图像，$\beta,\alpha$分别为各自图像融合时所占的权重，一般情况下$\gamma$为0。</strong></p>
<p>以下为测试代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img1=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\save.png&#x27;</span>)</span><br><span class="line">img2=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\imori.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dst1=cv.addWeighted(img1,<span class="number">0.8</span>,img2,<span class="number">0.2</span>,<span class="number">0</span>)</span><br><span class="line">dst2=cv.addWeighted(img1,<span class="number">0.2</span>,img2,<span class="number">0.8</span>,<span class="number">0</span>)</span><br><span class="line">images=[dst1,dst2]</span><br><span class="line">titles=[<span class="string">&#x27;dst1&#x27;</span>,<span class="string">&#x27;dst2&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    images[i]=cv.cvtColor(images[i],cv.COLOR_BGR2RGB)</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">2</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(images[i],<span class="string">&#x27;jet&#x27;</span>)</span><br><span class="line">    plt.title(titles[i],fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>代码解读：</p>
</li>
<li><p>在这里要注意到，使用Matplotlib显示图片时，要先将图片转换为RGB形式，在OpenCV中，图像是以BGR通道存储的，直接显示会出现问题。</p>
</li>
<li><p>上述测试代码里的img1为img2的灰度图，可以发现当$\beta$取0.8，$\alpha$取0.2时，img1融合时占比较高，当$\beta$取0.2，$\alpha$取0.8时，二者的对比结果如下图所示：</p>
<p><img src="/2020/10/14/opencv/4.png" alt="运行结果图" style="zoom:40%;"></p>
<p>可以发现前者更加偏灰度，后者更加偏向原图。</p>
</li>
</ul>
<h3 id="按位运算"><a href="#按位运算" class="headerlink" title="按位运算"></a>按位运算</h3><ul>
<li><p>有时候，我们不想将图像相加或者融合，例如，下面这张图片：</p>
<p><img src="/2020/10/14/opencv/5.png" style="zoom: 25%;"></p>
<p>它就是由两张图片——（西电LOGO和人物图）进行“掩盖”合成的。</p>
</li>
<li><p><strong>按位操作分为以下四种：AND，OR，NOT，XOR，一一介绍它们的运算：</strong></p>
<p><strong>1. AND：即“与”运算，（1&amp;1=1，1&amp;0=0，0&amp;1=0，0&amp;0=0），其具体意义可理解为，只有两值都大于0，其为真。而在像素中，其取值在0~255之间，0为黑色，255为白色，而这里大于0的像素值都可以看作1。在两个都大于0的像素值之间进行AND运算，其结果取较小的像素值！</strong></p>
<p><strong>2. NOT：即“非”运算，（~1=0，~0=1），其具体意义可理解为，取像素的相反值，则当图像为二值图时，原图的黑色变为白色，白色变为黑色。</strong></p>
<p><strong>3. OR：即“与”运算，（1|1=1，1|0=1，0|1=1，0|0=0），其具体意义可理解为，只有当两值都为0，其为假。而对于两像素值，其先进行二进制转换，后对其进行运算，例如3|5：00000011|00000101=00000111，因此3|5=7。</strong></p>
<p><strong>4. XOR：即“异或”运算，（1^1=0，1^0=1，0^1=1，0^0=0），其具体意义可理解为，只有当两值不相同时，其为真。对于两两像素值，先进行二进制转换，后对其进行运算，例如3^5：00000011^00000101=00000110，因此3^5=6。</strong></p>
</li>
<li><p><strong>OpenCV中，进行按位运算的有以下四个函数：cv.bitwise_not，cv.bitwise_and，cv.bitwise_or，cv.bitwise_xor，这四个函数即对应上面的运算规则。下面将以下两张图片合成为上图：</strong></p>
<p><img src="/2020/10/14/opencv/6.jpg" style="zoom:25%;"></p>
<p><img src="/2020/10/14/opencv/7.jpg" style="zoom:25%;"></p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img1=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\xidian1.jpg&#x27;</span>)</span><br><span class="line">img2=cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\girl.jpg&#x27;</span>)</span><br><span class="line">row,cols,channels=img1.shape</span><br><span class="line">roi=img2[<span class="number">0</span>:row,<span class="number">0</span>:cols]</span><br><span class="line"></span><br><span class="line">img1_gray=cv.cvtColor(img1,cv.COLOR_BGR2GRAY)</span><br><span class="line">ret,dst1=cv.threshold(img1_gray,<span class="number">200</span>,<span class="number">255</span>,cv.THRESH_BINARY_INV)</span><br><span class="line">dst1_inv=cv.bitwise_not(dst1)</span><br><span class="line"></span><br><span class="line">img2_bg=cv.bitwise_and(roi,roi,mask=dst1_inv)</span><br><span class="line">img1_bg=cv.bitwise_and(img1,img1,mask=dst1)</span><br><span class="line"></span><br><span class="line">dst=cv.add(img1_bg,img2_bg)</span><br><span class="line">img2[<span class="number">0</span>:row,<span class="number">0</span>:cols]=dst</span><br><span class="line"></span><br><span class="line">cv.imshow(<span class="string">&#x27;images&#x27;</span>,img2)</span><br><span class="line"></span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>
<p>代码解读:</p>
<ul>
<li><p><strong>首先由shape得到img1（LOGO）图像的大小，即行数和列数。由此，可以在img2图像上，通过numpy直接划分出与img1图像一样大小的 roi图像（为原图的左上角）。</strong></p>
</li>
<li><p><strong>通过threshold函数可将img1图像二值化，而在这之前需要将img1图像转换为灰度图，才能将其传入。使用 THRESH_BINARY_INV方法将其二值化，二值化图像如下：</strong></p>
<p><img src="/2020/10/14/opencv/8.png" style="zoom:33%;"></p>
<p><strong>将得到的二值化图像（dst1）利用“非”运算，得到的图像如下：</strong></p>
<p><img src="/2020/10/14/opencv/9.png" style="zoom:33%;"></p>
<p><strong>可以发现，进行“非”运算后，对二值图来说，其实即进行了颜色反转。</strong></p>
</li>
<li></li>
</ul>
<hr>
</li>
</ul>
<h2 id="阈值分割"><a href="#阈值分割" class="headerlink" title="阈值分割"></a>阈值分割</h2><h3 id="固定阈值分割"><a href="#固定阈值分割" class="headerlink" title="固定阈值分割"></a>固定阈值分割</h3><ul>
<li><p><strong>阈值分割简单来说，即大于阈值的变成一种值，小于阈值的为另一种值</strong>   </p>
</li>
<li><p>在python的cv2库中实现固定阈值分割的为<strong>cv2.threshold()函数</strong>。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = cv.imread(<span class="string">r&#x27;D:\Python\image_processing\exercise\imori.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">ret, th1 = cv.threshold(img, <span class="number">127</span>, <span class="number">255</span>, cv.THRESH_BINARY)</span><br><span class="line">ret,th2=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_BINARY_INV)</span><br><span class="line">ret,th3=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO)</span><br><span class="line">ret,th4=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TOZERO_INV)</span><br><span class="line">ret,th5=cv.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv.THRESH_TRUNC)</span><br><span class="line"></span><br><span class="line">titles=[<span class="string">&#x27;Org&#x27;</span>,<span class="string">&#x27;Binary&#x27;</span>,<span class="string">&#x27;Binary_inv&#x27;</span>,<span class="string">&#x27;Tozero&#x27;</span>,<span class="string">&#x27;Tozero_inv&#x27;</span>,<span class="string">&#x27;Trunc&#x27;</span>]</span><br><span class="line">images=[img,th1,th2,th3,th4,th5]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(images[i],<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.title(titles[i],fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  运行结果如图所示：</p>
<p>  <img src="/2020/10/14/opencv/1.png" alt="运行结果图" style="zoom:40%;"></p>
<ul>
<li><p><strong>cv2.threshold()函数由4个参数组成，img为传入函数的图像，127为设置的阈值大小(threshold)，255为阈值设定方式里的阈值最大值(maxval)，THRESH_BINARY为阈值设定的方式。</strong>      </p>
</li>
<li><p><strong>ret代表当前的阈值。</strong></p>
</li>
<li><p><strong>matplotlib.pyplot中subplot(2,3,i+1)即为将窗口分为2行3列，i+1表示当前的第i+1个子图，imshow()则是对图像进行处理，它不会让图片进行显示，图像显示需要show()</strong></p>
</li>
<li><p><strong>阈值设定有5种方法，分别为:</strong></p>
<ol>
<li><strong>THRESH_BINARY:</strong></li>
</ol>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
maxval &\ if\ src(x,y)>thresh\\
0 &\ otherwise
\end{cases}</script><p>​        当原像素值大于阈值，原像素值变为maxval，除此之外为0。</p>
<p>​    2.<strong>THRESH_BINARY_INV:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
0 &\ if \ src(x,y)>thresh\\
maxval &\ otherwise
\end{cases}</script><p>​        该阈值方法与上述阈值方法相反，从图像也可以观察得到。</p>
<p>​    3.<strong>THRESH_TOZERO:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
src(x,y) &\ if\ src(x,y)>thresh\\
0 &\ oherwise
\end{cases}</script><p>​        当原像素值大于阈值时，原像素值保持不变，除此之外，原像素值变为0。</p>
<p>​    4.<strong>THRESH_TOZERO_INV:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
0 &\ if \ src(x,y)>thresh\\
src(x,y) &\ otherwise
\end{cases}</script><p>​        该阈值方法与上述阈值方法相反。</p>
<p>​     5.<strong>THRESH_TRUNC:</strong></p>
<script type="math/tex; mode=display">
dst(x,y)=\begin{cases}
threshold &\ if \ src(x,y)>thresh\\
src(x,y) &\ otherwise
\end{cases}</script><p>​        放原像素值大于阈值，则原像素值变为阈值，除此之外，原像素值保持不变。</p>
</li>
</ul>
<h3 id="自适应阈值分割"><a href="#自适应阈值分割" class="headerlink" title="自适应阈值分割"></a>自适应阈值分割</h3><ul>
<li></li>
</ul>
]]></content>
      <tags>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine learning</title>
    <url>/2020/10/23/ML/</url>
    <content><![CDATA[<h1 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a><strong>Regression</strong></h1><h2 id="Linear-regression-线性回归"><a href="#Linear-regression-线性回归" class="headerlink" title="Linear regression(线性回归)"></a>Linear regression(线性回归)</h2><h3 id="问题的导入：预测神奇宝贝进化后的战斗力（CP）值"><a href="#问题的导入：预测神奇宝贝进化后的战斗力（CP）值" class="headerlink" title="问题的导入：预测神奇宝贝进化后的战斗力（CP）值"></a>问题的导入：预测神奇宝贝进化后的战斗力（CP）值</h3><ul>
<li><p><strong>输入：进化前神奇宝贝A的CP值，种类，血量（HP），重量（Weight)，高度（Height）</strong></p>
</li>
<li><p><strong>输出：进化后神奇宝贝A的CP值</strong></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233907.png" alt="1" style="zoom:67%;"></p>
</li>
</ul>
<h3 id="机器学习的主要步骤："><a href="#机器学习的主要步骤：" class="headerlink" title="机器学习的主要步骤："></a>机器学习的主要步骤：</h3><p><strong>step 1：Model（确定模型）</strong></p>
<p><strong>step 2：Goodless of function（确定损失函数）</strong></p>
<p><strong>step 3：Best function（找到最好的函数）</strong></p>
<h3 id="S-Model"><a href="#S-Model" class="headerlink" title="$\S $Model"></a>$\S $Model</h3><p>在这里，根据常理推测，神奇宝贝进化后的CP值和进化前的CP值有较大的关系，故我们可以假设进化前的CP值与进化后的CP值呈线性关系。</p>
<p>设神奇宝贝进化前的CP值为$x_{cp}$，进化后的CP值为$y$，则可建立以下线性关系：</p>
<script type="math/tex; mode=display">
y=b+wx_{cp} \tag 1</script><p>在这里我们仅考虑了神奇宝贝进化前的CP值这一特征，如果神奇宝贝影响进化后的CP值的不止这一特征，若有n个特征，那么我们可将$x_{cp}$推广到$x_i$，于是便可得到<strong>Linear model</strong>（线性模型）：</p>
<script type="math/tex; mode=display">
\begin{cases}
y=b+\sum_i^nw_ix_i\\
x_i:feature\ (特征)\\
w_i:weight \ (权重)\\
b:bias \ (偏差) \\
\end{cases} \tag 2</script><p>在这个模型里，一般而言，我们会有<strong>training data</strong>，假设现在我们有10组训练数据，即：</p>
<script type="math/tex; mode=display">
(x^1,\widehat{y}^1)\\
(x^2,\widehat{y}^2)\\
\vdots\\
(x^{10},\widehat{y}^{10}) \tag 3</script><p>对于以上数据来说，它们是已知的，其上标代表了它们的序号，而现在，我们可以将以上数据带入简化模型（式1）中，可得：</p>
<script type="math/tex; mode=display">
\widehat{y}^1=b+wx_{cp}^1\\
\widehat{y}^2=b+wx_{cp}^2\\
\vdots\\
\widehat{y}^{10}=b+wx_{cp}^{10}\\  \tag 4</script><p>对于式4，即就是我们训练数据带入模型后所得，而其只有两个未知数，即$b,w$，而现在我们需要做的就是<strong>找到最适合的$b,w$参数值</strong>。现在就需要损失函数发挥作用了。</p>
<h3 id="S-Goodless-of-function"><a href="#S-Goodless-of-function" class="headerlink" title="$\S $Goodless of  function"></a>$\S $Goodless of  function</h3><ul>
<li><p><strong>损失函数的作用</strong></p>
<script type="math/tex; mode=display">
Loss\ function\ L:
\begin{cases}
input:&\ a\ function\\
output:&\ how\ bad\ it\ is
\end{cases}</script><p>这里要注意评价函数的输入，即为函数$f$，而其输出是它的好坏，但由上式可知，式（4）其<strong>未知量只有待求参数$b,w$，</strong>那么函数评价函数$L(f)$即为：</p>
<script type="math/tex; mode=display">
L(f)=L(w,b) \tag 5</script><p>那么现在，<strong>对于输出所评价函数的好坏即可转化为评价参数$w,b$的好坏。</strong></p>
</li>
<li><p><strong>常见的损失函数</strong></p>
<p>现在我们已有真实数据，那么如何反映其参数的好坏呢，我们可以<strong>通过真实的CP值与预测CP值的差进行衡量</strong>，如下所示：</p>
<script type="math/tex; mode=display">
L(f)=\sum_{i=1}^{10}(\widehat{y}^i-f(x_{cp}^i))^2\Longrightarrow\ L(w,b)=\sum_{i=1}^{10}(\widehat{y}^i-(b+w\cdot x_{cp}^i))^2 \tag 6</script><p>当真实值与误差值较小时，那么此时的$w,b$可认为是最好的，则我们的目标即为：</p>
<script type="math/tex; mode=display">
min\quad L(w,b)</script><p>于是我们现在就需要对其进行求解。</p>
</li>
</ul>
<h3 id="S-Best-function-——-Gradient-Descent（梯度下降法）"><a href="#S-Best-function-——-Gradient-Descent（梯度下降法）" class="headerlink" title="$\S $Best  function ——-Gradient  Descent（梯度下降法）"></a>$\S $Best  function ——-Gradient  Descent（梯度下降法）</h3><p>我们需要通过$Loss\ Function\Longrightarrow”the \ best\ function”$，即达到我们的目标，令：</p>
<script type="math/tex; mode=display">
w*,b*=arg\ \ min_{w,b}\ \ L(w,b)=arg\ \ min_{w,b}\ \ \sum_{i=1}^{10}(\widehat{y}^i-(b+w\cdot x_{cp}^i))^2 \tag 7</script><p>对于以上，我们可以采用穷举法，求得$L(w,b)$的最小值，但下面介绍更加高效的算法。</p>
<ul>
<li><p>梯度下降法</p>
<p>若现在损失函数$L(w,b)$只考虑其$w$单变量情况，有以下函数图像：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233452.png" alt="2" style="zoom:67%;"></p>
<p>当我们现在<strong>随机</strong>在函数上选择一点$w^0$，</p>
<p>1.如果函数在该点的斜率为负，即导数$\frac{\mathrm{d}L}{\mathrm{d}w}&lt;0$，那么此时函数应该为<strong>递减的</strong>，于是可以考虑增大$w^0$。</p>
<p>2.如果函数在该点的斜率为正，即导数$\frac{\mathrm{d}L}{\mathrm{d}w}&gt;0$，那么此时函数应该为<strong>递增的</strong>，于是可以考虑减少$w^0$。</p>
<p>增大多少或减少多少？</p>
<p>定义每次移动的步长为：</p>
<script type="math/tex; mode=display">
step=\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0 \tag 8</script><p>在式（8）中，$\eta$称为”learning rate（学习率）”，<strong>我们可以通过控制$\eta$的大小，从而增加step，以此可以提高求解效率，但若$\eta$过大，则会导致步长过大，那么有可能会错过最优解，故我们需要选取合适的$\eta$。而$\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0$，其微分值越大$\rightarrow$曲线越陡峭$\rightarrow$移动步长越大$\rightarrow$提高求解效率。</strong></p>
<p>以上图为例，其算法如下：</p>
<script type="math/tex; mode=display">
w^0-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^0\Longrightarrow w^1\\
w^1-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^1\Longrightarrow w^2\\
\vdots \ \ 更新循环\\
w^{t-1}-\eta\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{t-1}\Longrightarrow w^t</script><p>在上式中，因为$\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{0}&lt;0$，故需给其添加负号。对点不断更新循环，直到$(\frac{\mathrm{d}L}{\mathrm{d}w}|w=w^{t})=0$停止，此时$w=w^t$即所求参数的最优解。</p>
<p><strong>但从上图我们可以发现$w^t$其实并不为global optimal solution(全局最优解)，其为local optimal solution(局部最优解)，不过由于我们研究的为线性回归，所以不存在局部最优解。故在线性回归中，梯度下降法是有效的。</strong></p>
<p>现在，我们可以将单变量$w$推广到多变量$w,b$，函数如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233543.png" alt="3" style="zoom:67%;"></p>
<p>随机选取点$(w^0,b^0)$，计算函数在该点的偏导$\frac{\partial L}{\partial w}|w=w^0,b=b^0,\frac{\partial L}{\partial b}|w=w^0,b=b^0$。</p>
<p>那么函数在该点的梯度即为：</p>
<script type="math/tex; mode=display">
grad\ L(w^0,b^0)=\nabla L(w^0,b^0)=(\frac{\partial L}{\partial w}|w^0,b^0)\overrightarrow{i}+(\frac{\partial L}{\partial b}|w^0,b^0)\overrightarrow{j}\tag 9</script><p>其中$\nabla$为向量微分算子。</p>
<p><strong>而对于梯度向量，其方向为曲线在等值线上的法线方向（高数中有对其的证明），其通过不断的增加步长以更新点，从而达到最优解，如上图所示。</strong></p>
<p>与单变量相同，多变量算法与其相同：</p>
<script type="math/tex; mode=display">
\begin{cases}
w^0-\eta\frac{\partial L}{\partial w}|w=w^0,b=b^0\Longrightarrow w^1\\
b^0-\eta\frac{\partial L}{\partial b}|w=w^0,b=b^0\Longrightarrow b^1
\end{cases}\\
\begin{cases}
w^1-\eta\frac{\partial L}{\partial w}|w=w^1,b=b^1\Longrightarrow w^2\\
b^1-\eta\frac{\partial L}{\partial b}|w=w^1,b=b^1\Longrightarrow b^2
\end{cases}\\
\vdots\ \ 更新循环\\
\begin{cases}
w^t-\eta\frac{\partial L}{\partial w}|w=w^t,b=b^t\Longrightarrow w^t\\
b^t-\eta\frac{\partial L}{\partial b}|w=w^t,b=b^t\Longrightarrow b^t
\end{cases}\\</script><p>最后直到$\nabla L(w^t,b^t)=0$停止，此时<strong>$w^t,b^t$即为所求得的最优解，由于在线性回归中，所以不同担心会由于初始值的选取，而影响达到全局最优解。</strong></p>
</li>
</ul>
<h3 id="S-Result"><a href="#S-Result" class="headerlink" title="$\S $Result"></a>$\S $Result</h3><p>如果现在通过十组训练数据，得到的结果如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233626.png" alt="4" style="zoom:67%;"></p>
<p>如图所示，其通过梯度下降法，所求得的最优参数$b,w$分别为-188.4，2.7，其中，损失值可用$\frac{1}{10}\sum_{n=1}^{10}\mathrm{e}^n$表示，<strong>$e^n$即为第n组真实值与预测值的差值。</strong></p>
<p>现在将model改用不同函数，观察其结果变化情况，如下图所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233707.png" alt="5" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233746.png" alt="6" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233820.png" alt="7" style="zoom:67%;"></p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233844.png" alt="8" style="zoom:67%;"></p>
<p>可以发现，<strong>随着Model改用函数的阶数的升高，训练集损失值越来越小，但可以发现测试集损失值却并不是越来越少，相反，当阶数大于4后，其测试集损失值增大较多，而这种现象叫做”Overfitting”(过拟合)。</strong></p>
<p>那么我们如何才能防止过拟合的发生呢？</p>
<ul>
<li><p><strong>Regularization(正则化)</strong></p>
<p><strong>由于过拟合是model过度拟合其在训练集的数据，则其会造成曲线波动较大，而如果能让曲线变得smooth平滑，便可防止过拟合。</strong></p>
<p><strong>于是考虑，增加一个与参数$w$（斜率）相关的值，如果loss function 越小的话，那么对应这个值也会越小，即$w$也会越小，于是便会使函数更加平滑。</strong></p>
<p>于是可在线性回归模型中，将loss function变为：</p>
<script type="math/tex; mode=display">
L=\sum_{i=1}^{10}(\widehat{y}^i-(b+\sum_jw_jx_j))^2+\lambda\sum_j(w_j)^2 \tag{10}</script><p>而在式（10）中，$\lambda\sum_j(w_j)^2$称为正则化项，$\lambda$称为正则化参数，它的作用就是平衡loss function这两项，若$\lambda$取得非常大，即对参数$w_j$惩罚的非常大，那么相当于$w_j$趋于0，即相当于原线性回归模型中删除了这些项，那么函数就变成了一条$y=b$的水平直线。</p>
<p>那么为什么减小$w_j$能够使其函数不宜发生overfitting呢？</p>
<p>若现在令$\varDelta x_j$为输入时的干扰项，那么输出所形成的误差即为：</p>
<script type="math/tex; mode=display">
y=b+\sum_jw_j(x_j+\varDelta x_j) \Longrightarrow \varDelta y=w_j\varDelta x_j</script><p>由此我们可以发现，当$w_j$较小时，产生的误差$\varDelta y$也更小，则更加光滑的函数会受更少的影响。</p>
<p>但观察正则化项，我们会发现为什么没有$b$呢？</p>
<p><strong>其实，正则化调整的是函数的平滑程度，但我们调整b对函数的平滑程度不会造成影响，它只能让函数上下移动。</strong></p>
</li>
</ul>
<h1 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a><strong>Classification</strong></h1><p>什么是分类呢?例如在现实生活中，当我们看见一张关于猫与狗的照片时，我们可以很容易区分，照片里的属于猫还是狗，那么如果现在有一些不同类别的大量数据，显然由人进行分类是不现实的，那么我们希望可以借助计算机来帮我们进行数据的分类……</p>
<p>那么我们如何解决分类问题呢？例如下图：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233933.png" alt="9" style="zoom:67%;"></p>
<p>希望能<strong>建立一个函数，输入一个神奇宝贝，就能返回其属性（类别）。</strong></p>
<p>首先我们需要将神奇宝贝数字化，即通过它的特征（Hp，Attack，Defense……）来描述这只神奇宝贝。首先将分类问题简化，只考虑其只有两类。</p>
<h2 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h2><p>我们的目标仍是寻找一个函数，输入x输出其类别，即为：</p>
<ul>
<li><p>Function (Model):</p>
<script type="math/tex; mode=display">
x \rightarrow \ f(x)=\begin{cases}
g(x)>0 \quad &output=class\ 1\\
else &output=class\ 2
\end{cases}</script><p>输入x，则当函数值&gt;0，其属于class 1，否则其属于class 2。而这个函数我们可以利用概率模型进行描述，即<script type="math/tex">P(C_1|x)>0.5</script></p>
</li>
<li><p>Loss Function:</p>
<p>Loss Function的目的是用来评价函数的好坏，那么我们想用模型分类的结果，与正确的模型结果进行对比，统计错误的次数。</p>
<script type="math/tex; mode=display">
L(f)=\sum_n\delta(f(x^n)\neq\widehat{y}^n)</script></li>
<li><p>Find the best function:</p>
<p>Example:perception(感知机算法)、svm(支持向量机)</p>
</li>
</ul>
<h2 id="贝叶斯概率模型"><a href="#贝叶斯概率模型" class="headerlink" title="贝叶斯概率模型"></a>贝叶斯概率模型</h2><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115233955.png" alt="10" style="zoom:67%;"></p>
<p>在Box 1中我们可以计算得到抽出蓝色球的概率为<script type="math/tex">\frac{4}{5}</script>，绿色球的概率为<script type="math/tex">\frac{1}{5}</script>，若已知我们抽取Box 1的概率为<script type="math/tex">\frac{2}{3}</script>，那么我们就可以利用贝叶斯公式计算得到，已知抽取的小球为蓝色，则从Box 1抽出的机率为多少，即<script type="math/tex">P(B_1|Blue)</script>。</p>
<p>而现在我们可以将这些球看作是一个个数据，从而可以利用贝叶斯公式生成概率模型。</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234009.png" alt="11" style="zoom:67%;"></p>
<ul>
<li>先验概率：<script type="math/tex">P(C)</script>，即每种类别发生的概率。</li>
<li>类条件概率：<script type="math/tex">P(x|C)</script>，在某种类别的条件下，某事发生的概率。</li>
<li>后验概率：<script type="math/tex">P(C|x)</script>，某事发生了，它属于某种类别的概率。</li>
</ul>
<p>如果我们可以计算得到<script type="math/tex">P(C_1)</script>，<script type="math/tex">P(C_2)</script>，与<script type="math/tex">P(x|C_1)</script>，<script type="math/tex">P(x|C_2)</script>，那么我们可以通过贝叶斯公式计算得到<script type="math/tex">P(C_1|x)</script>，<script type="math/tex">P(C_2|x)</script>，如果<script type="math/tex">P(C_1|x)>0.5</script>，那么可以认为<script type="math/tex">x</script>是属于Class 1 ，相反可认为<script type="math/tex">x</script>是属于Class 2的。</p>
<p>那么我们的目标就是计算<script type="math/tex">P(C_1)</script>，<script type="math/tex">P(C_2)</script>，与<script type="math/tex">P(x|C_1)</script>，<script type="math/tex">P(x|C_2)</script>。</p>
<ul>
<li><p><strong>先验概率的计算</strong></p>
<p>假设现在两个类别，Class 1：Water(水系)，Class 2：Normal(其他系)。</p>
<p><strong>水系有79只神奇宝贝，其他系有61只神奇宝贝，总样本共有140只神奇宝贝。</strong></p>
<script type="math/tex; mode=display">
\implies P(C_1)=\frac{79}{140}=0.56,\ P(C_2)=\frac{61}{140}=0.44</script><p>即通过所得样本中各自种类的占比得到先验概率，其计算较为简单。</p>
</li>
<li><p><strong>类条件概率的计算</strong></p>
<p>假设水系精灵中有6杰尼龟(水系)，那么我们可以认为在水系精灵中，杰尼龟的概率是<script type="math/tex">P(x|C_1)=\frac{6}{79}=0.076</script>吗？</p>
<p>事实上，这是不对的，<strong>因为该水系精灵样本较少，所以我们无法直接仅由样本中所占比例来推得类条件概率。假设该水系样本精灵中没有水箭龟，但水箭鬼确实是水系精灵，那么我们能说明水箭龟在水系精灵中的概率为0吗？这显然是有问题的。</strong></p>
<p>那么我们如何计算<script type="math/tex">P(x|C)</script>呢？我们利用<strong>极大似然估计法</strong>。</p>
</li>
</ul>
<h3 id="S-极大似然估计"><a href="#S-极大似然估计" class="headerlink" title="$\S$ 极大似然估计"></a>$\S$ 极大似然估计</h3><p>对于<script type="math/tex">P(x|C)</script>的计算，我们很难去寻找到随机变量<script type="math/tex">x</script>所遵循的概率密度函数。<strong>那么如果我们假设已知随机变量<script type="math/tex">x</script>所遵循的概率密度函数，转而计算所假设概率密度函数中的参数值，如果找到最佳的参数值，那么我们就能用该概率密度函数代替类条件概率的密度函数了。</strong></p>
<p>若有样本集<script type="math/tex">X=\{x^1,x^2,x^3,...,x^n\}</script>，每个样本集中的样本都是所谓独立同分布的随机变量。</p>
<p><strong>似然函数定义为：似然函数<script type="math/tex">L(\theta|x)</script>是给定样本x时，关于参数θ的函数，其在数值上等于给定参数<script type="math/tex">\theta</script>后变量X的概率</strong>：</p>
<script type="math/tex; mode=display">
L(\theta|x)=P(X=x|\theta)=P(x^1|\theta)P(x^2|\theta)\dots P(x^n|\theta)</script><p>在当<script type="math/tex">x_i</script>为离散型随机变量时，<script type="math/tex">f(x,\theta)</script>为概率密度函数，则<script type="math/tex">P(x|\theta)=f(x,\theta)</script>。</p>
<script type="math/tex; mode=display">
L(\theta|x)=f(x^1|\theta)f(x^2|\theta)\dots f(x^n|\theta)</script><p><strong>由于<script type="math/tex">X=\{x^1,x^2,x^3,...,x^n\}</script>即为已知数据，那么似然函数即为<script type="math/tex">L(\theta)</script>关于参数<script type="math/tex">\theta</script>的函数，而该似然函数的值越大，说明该参数的估计越佳，那么我们的目标即转变为找到最大的似然函数值，即为</strong></p>
<script type="math/tex; mode=display">
\theta^*=arg \ \ max\ L(\theta)</script><h3 id="S-类条件概率计算"><a href="#S-类条件概率计算" class="headerlink" title="$\S$类条件概率计算"></a>$\S$类条件概率计算</h3><p>回到类条件概率的计算，我们设水系精灵的样本集<script type="math/tex">X=\{x^1,x^2,x^3,...,x^{79}\}</script>，<strong>若每个<script type="math/tex">x^i</script>考虑n个特征</strong>，即<script type="math/tex">x^i=[x_1,x_2,\dots,x_n]</script>，服从<script type="math/tex">Gaussian\ distribution</script>(高斯分布) ，则有：</p>
<script type="math/tex; mode=display">
f_{\mu,B}(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|B|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)B^{-1}(x-\mu)^T}</script><p><strong>现在每个样本<script type="math/tex">x^i</script>仅考虑2个特征</strong>，即<script type="math/tex">x^i=[x_1,x_2]</script>，那么<script type="math/tex">x_i\thicksim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)</script>，即属于二维高斯分布，其概率密度函数为：</p>
<script type="math/tex; mode=display">
f_{\mu,B}(x)=\frac{1}{2\pi|B|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)B^{-1}(x-\mu)^T}</script><p><strong>其中<script type="math/tex">B</script>为协方差矩阵，<script type="math/tex">\mu</script>为均值向量。</strong></p>
<p>根据极大似然估计法，<script type="math/tex">x^i</script>均为独立同分布，且<script type="math/tex">x^i</script>均为离散型数据，则有<script type="math/tex">P(x^i|C)=f_{\mu,B}(x^i)</script>，即：</p>
<script type="math/tex; mode=display">
L(\mu,B)=P(x^1|C_1)P(x^2|C_1)\dots P(x^{79}|C_1)=f_{\mu,B}(x^1)f_{\mu,B}(x^2)\dots f_{\mu,B}(x^{79})=\prod_{i=1}^{79}f_{\mu,B}(x^i)</script><p><strong>那么似然函数<script type="math/tex">L(\mu,B)</script>就是关于参数<script type="math/tex">\mu,B</script>的函数。则最佳参数值即为：</strong></p>
<script type="math/tex; mode=display">
\mu^*,B^*=arg \ \ max_{\mu,B}\ L(\mu,B)</script><p>通过计算，求得最佳参数即为：</p>
<script type="math/tex; mode=display">
\mu^*=\frac{1}{79}\sum_{i=1}^{79}x^i\ \ , \ \ B^*=\frac{1}{79}\sum_{i=1}^{79}(x^i-\mu^*)(x^i-\mu^*)^T</script><p>带入已知数据集，可以分别计算得到两个类别的最佳参数值：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234034.png" alt="12" style="zoom:67%;"></p>
<h3 id="S-Result-1"><a href="#S-Result-1" class="headerlink" title="$\S$Result"></a>$\S$Result</h3><p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234048.png" alt="13" style="zoom:67%;"></p>
<p><strong>观察可以发现，在Testing data上仅有47%的正确率。于是将<script type="math/tex">x^i</script>考虑的特征由2升为7，即<script type="math/tex">x^i</script>变成7维向量，但测试正确率仍只有54%。</strong></p>
<h3 id="S-模型改进"><a href="#S-模型改进" class="headerlink" title="$\S$模型改进"></a>$\S$模型改进</h3><p><strong>在最初的模型里，我们是利用似然函数，分别对Class 1 与Class 2进行参数估计，从而会产生两类参数，现在，我们将这两类公用同一个协方差矩阵<script type="math/tex">B</script>，而Class 1 与Class 2也共同用一个似然函数</strong>，即：</p>
<script type="math/tex; mode=display">
L(\mu_1,\mu_2,B)=f_{\mu_1,B}(x^1)f_{\mu_1,B}(x^2)\dots f_{\mu_1,B}(x^{79})f_{\mu_2,B}(x^{80})\dots f_{\mu_2,B}(x^{140})</script><p><strong>由此计算得到的<script type="math/tex">\mu_1，\mu_2</script>与最初计算结果相同，而<script type="math/tex">B=\frac{79}{140}B_1+\frac{61}{140}B_2</script>。</strong></p>
<p>而现在得到的结果如下所示：</p>
<p><img src="https://gitee.com/Lehrmann/figure/raw/master/image/20201115234104.png" alt="14" style="zoom:67%;"></p>
<p>可以发现，当公用协方差矩阵<script type="math/tex">B</script>后，正确率可以提高，那么为什么会出现这种结果呢？<strong>因为当两个Class公用相同的协方差矩阵<script type="math/tex">B</script>后，可以减小Overfitting。</strong></p>
<h3 id="S-Naive-Bayes-Classifier"><a href="#S-Naive-Bayes-Classifier" class="headerlink" title="$\S$Naive Bayes Classifier"></a>$\S$Naive Bayes Classifier</h3><p>在上文中，我们的特征只有两个，如果特征变成多个应该如何处理呢？<script type="math/tex">x=[x_1,x_2,x_3,\dots,x_k ]</script>，<strong>如果各特征是在相互独立的情况下</strong>，那么有如下关系：</p>
<script type="math/tex; mode=display">
P(x|C_1)=P(x_1|C_1)P(x_2|C_1)\dots P(x_k|C_1)</script><p><strong>而这样做的好处是：<script type="math/tex">P(x_i|C_1)</script>遵循一维高斯分布，相较于二维高斯分布计算量大大降低，而这种方法叫做Naive Bayes Classifier(朴素贝叶斯分类器)。</strong></p>
<h3 id="S-概率密度函数选择"><a href="#S-概率密度函数选择" class="headerlink" title="$\S$概率密度函数选择"></a>$\S$概率密度函数选择</h3><p>在前文中，我们选择了随机变量服从高斯分布，事实上我们可以选择其他分布函数，这是比较随意的，但当随机变量呈现相关性质时，有一些特定的分布效果会更加好。</p>
<ul>
<li>当样本数据x取实数值时，采用高斯分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DN(\mu_{jc},\sigma_{jc})</script></li>
<li>当每种特征<script type="math/tex">x_j\in\{0,1\}</script>时，采用伯努利分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DBer(\mu_{jc},\sigma_{jc})</script></li>
<li>当每种特征取值<script type="math/tex">x_j\in\{1,2,3,\dots,k\}</script>时，采用分类分布：<script type="math/tex">P(x|y=c,\theta)=\prod_{j=1}^DCat(\mu_{jc},\sigma_{jc})</script></li>
</ul>
<h2 id="Logistic-Regression-逻辑回归"><a href="#Logistic-Regression-逻辑回归" class="headerlink" title="Logistic Regression(逻辑回归)"></a>Logistic Regression(逻辑回归)</h2>]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
